<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.0.0-rc2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":true,"nav":{"gitalk":{"order":-1}},"activeClass":"gitalk"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="综述 线性回归介绍 线性回归pmml 介绍 线性回归模型结构 如何手动写一个java类表述线性回归  很多时候，我们在看机器学习的算法的时候，看到的都是一些列的公式推导。那么这些公式推导出来的结果是什么？最后又是如何组织的？作为一个码农，更关心的是，这些公式最后又是如何变为代码的？本系列将借助PMML这一工具，可以用来解析模型的结构，了解各种模型中都有那些元素？这些元素又是通过何种组合方式，计算公">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习不只是调包--通过PMML解析线性回归和逻辑回归">
<meta property="og:url" content="http://yoursite.com/2018/08/06/postpmml4LR/index.html">
<meta property="og:site_name" content="喵十八の小窝">
<meta property="og:description" content="综述 线性回归介绍 线性回归pmml 介绍 线性回归模型结构 如何手动写一个java类表述线性回归  很多时候，我们在看机器学习的算法的时候，看到的都是一些列的公式推导。那么这些公式推导出来的结果是什么？最后又是如何组织的？作为一个码农，更关心的是，这些公式最后又是如何变为代码的？本系列将借助PMML这一工具，可以用来解析模型的结构，了解各种模型中都有那些元素？这些元素又是通过何种组合方式，计算公">
<meta property="og:locale">
<meta property="og:image" content="https://images0.cnblogs.com/blog2015/633472/201503/262037556613399.jpg">
<meta property="og:image" content="https://images0.cnblogs.com/blog2015/633472/201503/262041198028564.jpg">
<meta property="og:image" content="https://images0.cnblogs.com/blog2015/633472/201503/262042295678545.jpg">
<meta property="og:image" content="https://gss3.bdstatic.com/-Po3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D99/sign=a46bd6f1dd33c895a27e9472d01340df/0df3d7ca7bcb0a4659502a5f6f63f6246b60af62.jpg">
<meta property="og:image" content="https://img-blog.csdn.net/20171005175521991?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY3lteTAwMQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="https://img-blog.csdn.net/20141209123917993">
<meta property="article:published_time" content="2018-08-06T13:45:18.000Z">
<meta property="article:modified_time" content="2018-08-06T13:47:03.000Z">
<meta property="article:author" content="喵十八">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="pmml">
<meta property="article:tag" content="逻辑回归">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://images0.cnblogs.com/blog2015/633472/201503/262037556613399.jpg">

<link rel="canonical" href="http://yoursite.com/2018/08/06/postpmml4LR/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-Hans'
  };
</script>

  <title>机器学习不只是调包--通过PMML解析线性回归和逻辑回归 | 喵十八の小窝</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">喵十八の小窝</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-commonweal">

    <a href="/404/" rel="section"><i class="fa fa-heartbeat fa-fw"></i>Commonweal 404</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/06/postpmml4LR/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="喵十八">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="喵十八の小窝">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          机器学习不只是调包--通过PMML解析线性回归和逻辑回归
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2018-08-06 21:45:18 / Modified: 21:47:03" itemprop="dateCreated datePublished" datetime="2018-08-06T21:45:18+08:00">2018-08-06</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/pmml/" itemprop="url" rel="index"><span itemprop="name">pmml</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h1><ul>
<li>线性回归介绍</li>
<li>线性回归pmml 介绍</li>
<li>线性回归模型结构</li>
<li>如何手动写一个java类表述线性回归</li>
</ul>
<p>很多时候，我们在看机器学习的算法的时候，看到的都是一些列的公式推导。那么这些公式推导出来的结果是什么？最后又是如何组织的？作为一个码农，更关心的是，这些公式最后又是如何变为代码的？<br>本系列将借助PMML这一工具，可以用来解析模型的结构，了解各种模型中都有那些元素？这些元素又是通过何种组合方式，计算公式得到最后的结果的？<br>本文针对偏工程人员，不涉及具体的模型优化求解问题，我们关注的是模型实质的结构，以及根据这些信息，如何实现跨平台的使用模型。至于模型的求解过程，前人已经总结完备，不过多赘述，在文中会给出地址，如有兴趣，可以自行查阅推导。</p>
<h1 id="先从简单的线性回归开始"><a href="#先从简单的线性回归开始" class="headerlink" title="先从简单的线性回归开始"></a>先从简单的线性回归开始</h1><h2 id="什么是线性回归"><a href="#什么是线性回归" class="headerlink" title="什么是线性回归"></a>什么是线性回归</h2><p>定义：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在统计学中，线性回归(Linear Regression)是利用称为线性回归方程的最小平方函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。这种函数是一个或多个称为回归系数的模型参数的线性组合。只有一个自变量的情况称为简单回归,大于一个自变量情况的叫做多元回归。</span><br></pre></td></tr></table></figure><br>线性回归是数据挖掘中的基础算法之一，从某种意义上来说，在学习函数的时候已经开始接触线性回归了，只不过那时候并没有涉及到误差项。线性回归的思想其实就是解一组方程，得到回归函数，不过在出现误差项之后，方程的解法就存在了改变，一般使用最小二乘法进行计算。</p>
<h3 id="解决问题流程"><a href="#解决问题流程" class="headerlink" title="解决问题流程"></a>解决问题流程</h3><p>Step1. 选择一个模型函数h<br>Step2. 为h找到适应数据的最优解，即找出最优解下的h的参数。</p>
<h3 id="函数模型"><a href="#函数模型" class="headerlink" title="函数模型"></a>函数模型</h3><p><img src="https://images0.cnblogs.com/blog2015/633472/201503/262037556613399.jpg" alt="线性回归函数模型"><br>其中h()是一个线性函数，所有的自变量构成一个一维向量X，所有参数构成一个一维向量W，就可以将第一行的公式改写为第二行的形式。</p>
<p>假设存在训练数据集<br><img src="https://images0.cnblogs.com/blog2015/633472/201503/262041198028564.jpg" alt=""><br>为了方便，可以改写为矩阵的形式<br><img src="https://images0.cnblogs.com/blog2015/633472/201503/262042295678545.jpg" alt=""></p>
<p>其中x可以看成特征，theater看成是权重。我们的目标就是找出所有的权重，进而出现新的x值时，可以对函数的输出进行估计。那我们如何求得使函数输出最接近样本的值呢？函数输出最接近样本值就意味着二者之差尽可能的小。我们假设输入的特征为，对应的样本值为，我们用模型估计出的值为，估计值与真实值之间的误差表示为<br>成为损失函数，损失函数的自变量为，所以我们需要找到最小时的取值。</p>
<p>在机器学习中我们采用梯度下降算法求解该方程，</p>
<p>一般的求解方法有梯度下降法，牛顿法，共轭梯度法，启发式优化方法等，在这篇<a href="http://www.cnblogs.com/maybe2030/p/4751804.html">博文</a>中介绍的比较详细。再次不多赘述。</p>
<p>OK,通过求解损失函数最小化，我们会得到一组参数，这就是线性回归的解，根据这个解，就有了我们的模型。</p>
<h2 id="操作实例"><a href="#操作实例" class="headerlink" title="操作实例"></a>操作实例</h2><h3 id="sklearn-实现线性回归"><a href="#sklearn-实现线性回归" class="headerlink" title="sklearn 实现线性回归"></a>sklearn 实现线性回归</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from sklearn2pmml import PMMLPipeline</span><br><span class="line">from sklearn2pmml import sklearn2pmml</span><br><span class="line">from sklearn import linear_model</span><br><span class="line">clf = linear_model.LinearRegression()</span><br><span class="line">X = [[0,0],[1,1],[2,2]]</span><br><span class="line">y = [0,1,2]</span><br><span class="line">pipeline = PMMLPipeline([(&quot;classifier&quot;, clf)])</span><br><span class="line">pipeline.fit(X,y)</span><br></pre></td></tr></table></figure>
<p>以上是希望拟合y=0.5 <em> x1 + 0.5 </em> x2。<br>测试下<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pipeline.predict([[1.0,2.0]])</span><br></pre></td></tr></table></figure><br>结果为：1.5 符合预期</p>
<p>将其导出为pmml<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn2pmml(pipeline,&quot;linearregression.pmml&quot;,with_repr = True)  </span><br></pre></td></tr></table></figure></p>
<h3 id="线性回归的PMML及结构"><a href="#线性回归的PMML及结构" class="headerlink" title="线性回归的PMML及结构"></a>线性回归的PMML及结构</h3><p>上一节中的模型，导出PMML文件的主要部分如下：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&lt;DataDictionary&gt;</span><br><span class="line">	&lt;DataField name=&quot;y&quot; optype=&quot;continuous&quot; dataType=&quot;double&quot;/&gt;</span><br><span class="line">	&lt;DataField name=&quot;x1&quot; optype=&quot;continuous&quot; dataType=&quot;double&quot;/&gt;</span><br><span class="line">	&lt;DataField name=&quot;x2&quot; optype=&quot;continuous&quot; dataType=&quot;double&quot;/&gt;</span><br><span class="line">&lt;/DataDictionary&gt;</span><br><span class="line">&lt;RegressionModel functionName=&quot;regression&quot;&gt;</span><br><span class="line">	&lt;MiningSchema&gt;</span><br><span class="line">		&lt;MiningField name=&quot;y&quot; usageType=&quot;target&quot;/&gt;</span><br><span class="line">		&lt;MiningField name=&quot;x1&quot;/&gt;</span><br><span class="line">		&lt;MiningField name=&quot;x2&quot;/&gt;</span><br><span class="line">	&lt;/MiningSchema&gt;</span><br><span class="line">	&lt;RegressionTable intercept=&quot;2.220446049250313E-16&quot;&gt;</span><br><span class="line">		&lt;NumericPredictor name=&quot;x1&quot; coefficient=&quot;0.4999999999999999&quot;/&gt;</span><br><span class="line">		&lt;NumericPredictor name=&quot;x2&quot; coefficient=&quot;0.49999999999999983&quot;/&gt;</span><br><span class="line">	&lt;/RegressionTable&gt;</span><br><span class="line">&lt;/RegressionModel&gt;</span><br></pre></td></tr></table></figure><br>模型一共两个输入 x1 x2,一个目标输出y<br>x1，x2 对应的参数分别为 0.4999999999999999 和 0.49999999999999983 （注意，这里不是0.5，是因为拟合的误差原因）整个的截距是2.220446049250313E-16</p>
<p>只要有了这几个参数，你就有了训练好的模型。针对任意的输入的x1，x2，你都能直接输入一个拟合好的y，掉包 不存在的。</p>
<h3 id="用Scala实现线性回归的预测"><a href="#用Scala实现线性回归的预测" class="headerlink" title="用Scala实现线性回归的预测"></a>用Scala实现线性回归的预测</h3><p>这里，参考了Spark MLlib中的源码，简单写了几个类，主要用于说明线性回归的结构，以及预测的逻辑。没有涉及到模型的训练。<br>从上一节中，可以看出线性回归模型中，其实就两个参数，1个权重向量，1个截距。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">class LinerRegressionModel(</span><br><span class="line">   val weights: Array[Double],</span><br><span class="line">   val intercept: Double</span><br><span class="line">   ) &#123;</span><br><span class="line"></span><br><span class="line">  def predict(testData:Array[Double]): Double =&#123;</span><br><span class="line">    predictPoint(testData,weights,intercept)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def predictPoint(</span><br><span class="line">    dataMatrix: Array[Double],</span><br><span class="line">    weightMatrix: Array[Double],</span><br><span class="line">    intercept: Double): Double = &#123;</span><br><span class="line">    DenseVector(dataMatrix).dot(DenseVector(weightMatrix)) + intercept</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>定义线性回归模型，预测结果是输入向量和权重向量点积加上截距。<br>权重向量就是上边每个入参对应的参数。<br>测试类如下：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def testLinerRegression(): Unit =&#123;</span><br><span class="line">  val weight = Array(0.4999999999999999, 0.49999999999999983)</span><br><span class="line">  val intercept = 2.220446049250313E-16</span><br><span class="line">  val model = new LinerRegressionModel(weight,intercept)</span><br><span class="line">  val x = Array(1.0, 2.0)</span><br><span class="line">  val y = model.predict(x)</span><br><span class="line">  println(f&quot;The Result Of Model is $&#123;y&#125;&quot;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>预测(1.0, 2.0)结果为：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">The Result Of Model is 1.4999999999999998</span><br></pre></td></tr></table></figure><br>四舍五入和python结果一致。</p>
<h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><p>根据上文的叙述，线性回归的模型是求出输出特征向量Y和输入样本矩阵X之间的线性关系系数theater,使其满足Y=theater X，如果的Y是连续的，所以是回归模型。如果我们想要Y是离散的话，怎么办呢？一个可以想到的办法是，我们对于这个Y再做一次函数转换，变为g(Y)。如果我们令g(Y)的值在某个实数区间的时候是类别A，在另一个实数区间的时候是类别B，就可以得到一个分类模型。<br>在逻辑回归中，这个函数就是sigmoid函数<br><img src="https://gss3.bdstatic.com/-Po3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D99/sign=a46bd6f1dd33c895a27e9472d01340df/0df3d7ca7bcb0a4659502a5f6f63f6246b60af62.jpg" alt=""></p>
<p>下图展示了将分布函数变形的过程。<br><img src="https://img-blog.csdn.net/20171005175521991?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY3lteTAwMQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""></p>
<p>具体的求解过程，可以参见这篇文章的叙述，本文不做赘述<a href="http://www.aboutyun.com/thread-10650-1-1.html">http://www.aboutyun.com/thread-10650-1-1.html</a></p>
<h2 id="二元逻辑回归"><a href="#二元逻辑回归" class="headerlink" title="二元逻辑回归"></a>二元逻辑回归</h2><p>如果结果的类别只有两种，那么就是一个二元分类模型了。<br>二元逻辑回归的预测值由下式求得<br><img src="https://img-blog.csdn.net/20141209123917993" alt=""></p>
<p>因此，逻辑回归分类器的解就是一组权值向量，和线性回归是一致的。</p>
<h3 id="二元逻辑回归python实现"><a href="#二元逻辑回归python实现" class="headerlink" title="二元逻辑回归python实现"></a>二元逻辑回归python实现</h3><p>使用自带的iris数据集进行操作，iris中包含3个分类，为了体现二分类的特性，删除了一个分类。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn2pmml <span class="keyword">import</span> PMMLPipeline</span><br><span class="line"><span class="keyword">from</span> sklearn2pmml <span class="keyword">import</span> sklearn2pmml</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">lr = LogisticRegression()</span><br><span class="line"><span class="comment"># 导入数据，为了后续方便，将三类中的一类去除，使之变为二分类问题。</span></span><br><span class="line">iris = load_iris()</span><br><span class="line">df = pd.DataFrame(iris.data)</span><br><span class="line">df[<span class="string">&quot;class&quot;</span>] = iris.target</span><br><span class="line">df.columns=[<span class="string">&#x27;V0&#x27;</span>,<span class="string">&#x27;V1&#x27;</span>,<span class="string">&#x27;V2&#x27;</span>,<span class="string">&#x27;V3&#x27;</span>,<span class="string">&#x27;class&#x27;</span>]</span><br><span class="line">df = df[df[<span class="string">&#x27;class&#x27;</span>] &lt; <span class="number">2</span>]</span><br><span class="line">df.describe()</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(df[df.columns.difference([<span class="string">&#x27;class&#x27;</span>])], df[<span class="string">&#x27;class&#x27;</span>], test_size=<span class="number">0.5</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">pipeline = PMMLPipeline([(<span class="string">&quot;classifier&quot;</span>, lr)])</span><br><span class="line">pipeline.fit(X_train,y_train)</span><br></pre></td></tr></table></figure></p>
<p>然后将训练好的模型导出为PMML<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from sklearn2pmml import sklearn2pmml</span><br><span class="line">sklearn2pmml(pipeline, &quot;LRbin.pmml&quot;, with_repr = True)</span><br></pre></td></tr></table></figure></p>
<h3 id="二元逻辑回归PMML分析"><a href="#二元逻辑回归PMML分析" class="headerlink" title="二元逻辑回归PMML分析"></a>二元逻辑回归PMML分析</h3><p>上一节代码生成的PMML中的主要部分如下：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">&lt;DataDictionary&gt;</span><br><span class="line">		&lt;DataField name=&quot;class&quot; optype=&quot;categorical&quot; dataType=&quot;integer&quot;&gt;</span><br><span class="line">			&lt;Value value=&quot;0&quot;/&gt;</span><br><span class="line">			&lt;Value value=&quot;1&quot;/&gt;</span><br><span class="line">		&lt;/DataField&gt;</span><br><span class="line">		&lt;DataField name=&quot;V0&quot; optype=&quot;continuous&quot; dataType=&quot;double&quot;/&gt;</span><br><span class="line">		&lt;DataField name=&quot;V1&quot; optype=&quot;continuous&quot; dataType=&quot;double&quot;/&gt;</span><br><span class="line">		&lt;DataField name=&quot;V2&quot; optype=&quot;continuous&quot; dataType=&quot;double&quot;/&gt;</span><br><span class="line">		&lt;DataField name=&quot;V3&quot; optype=&quot;continuous&quot; dataType=&quot;double&quot;/&gt;</span><br><span class="line">	&lt;/DataDictionary&gt;</span><br><span class="line">	&lt;RegressionModel functionName=&quot;classification&quot; normalizationMethod=&quot;logit&quot;&gt;</span><br><span class="line">		&lt;MiningSchema&gt;</span><br><span class="line">			&lt;MiningField name=&quot;class&quot; usageType=&quot;target&quot;/&gt;</span><br><span class="line">			&lt;MiningField name=&quot;V0&quot;/&gt;</span><br><span class="line">			&lt;MiningField name=&quot;V1&quot;/&gt;</span><br><span class="line">			&lt;MiningField name=&quot;V2&quot;/&gt;</span><br><span class="line">			&lt;MiningField name=&quot;V3&quot;/&gt;</span><br><span class="line">		&lt;/MiningSchema&gt;</span><br><span class="line">		&lt;Output&gt;</span><br><span class="line">			&lt;OutputField name=&quot;probability(0)&quot; optype=&quot;continuous&quot; dataType=&quot;double&quot; feature=&quot;probability&quot; value=&quot;0&quot;/&gt;</span><br><span class="line">			&lt;OutputField name=&quot;probability(1)&quot; optype=&quot;continuous&quot; dataType=&quot;double&quot; feature=&quot;probability&quot; value=&quot;1&quot;/&gt;</span><br><span class="line">		&lt;/Output&gt;</span><br><span class="line">		&lt;RegressionTable intercept=&quot;-0.24391923532173168&quot; targetCategory=&quot;1&quot;&gt;</span><br><span class="line">			&lt;NumericPredictor name=&quot;V0&quot; coefficient=&quot;-0.31738779611631857&quot;/&gt;</span><br><span class="line">			&lt;NumericPredictor name=&quot;V1&quot; coefficient=&quot;-1.2346299390640323&quot;/&gt;</span><br><span class="line">			&lt;NumericPredictor name=&quot;V2&quot; coefficient=&quot;1.920449906205768&quot;/&gt;</span><br><span class="line">			&lt;NumericPredictor name=&quot;V3&quot; coefficient=&quot;0.8753093733469249&quot;/&gt;</span><br><span class="line">		&lt;/RegressionTable&gt;</span><br><span class="line">		&lt;RegressionTable intercept=&quot;0.0&quot; targetCategory=&quot;0&quot;/&gt;</span><br><span class="line">	&lt;/RegressionModel&gt;</span><br></pre></td></tr></table></figure><br>一共四个入参，V0，V1，V2，V4 都为double类型。目标输出为0，1分类。<br>模型是一个<code>classification</code>，标准化使用的是<code>logit</code>函数，也就是sigmod函数。<br>权重矩阵为(-0.31738779611631857,-1.2346299390640323,1.920449906205768,0.8753093733469249),截距为-0.24391923532173168。<br>这里需要注意，二分类只需要一个RegressionTable就能满足分类的需求，后续的多分类会涉及到多个RegressionTable。<br>这里就能发现，二分类的LR，其实就是对输入求了一次线性回归的值，然后对这个值再求其sigmod解。因而，有了权重矩阵和截距，我们就能求出逻辑回归的预测值。</p>
<h3 id="scala简单实现"><a href="#scala简单实现" class="headerlink" title="scala简单实现"></a>scala简单实现</h3><p>定义一个用于二元逻辑回归计算预测概率值的类。<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">class LRBinModel(</span><br><span class="line">    val weights: Array[Double],</span><br><span class="line">    val intercept: Double) &#123;</span><br><span class="line"></span><br><span class="line">  def predict(testData:Array[Double]): Double =&#123;</span><br><span class="line">    predictPoint(testData,weights,intercept)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def predictPoint(</span><br><span class="line">                    dataMatrix: Array[Double],</span><br><span class="line">                    weightMatrix: Array[Double],</span><br><span class="line">                    intercept: Double): Double = &#123;</span><br><span class="line">    val margin = DenseVector(dataMatrix).dot(DenseVector(weightMatrix)) + intercept</span><br><span class="line">    val score = 1.0 / (1.0 + math.exp(-margin))</span><br><span class="line">    score</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>取测试集的第一条记录，进行测试<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X_t=X_test.head(1)</span><br><span class="line">print(X_t)</span><br><span class="line">y_t=y_test.head(1)</span><br><span class="line">print(y_t)</span><br><span class="line">pipeline.predict_proba(X_t)</span><br></pre></td></tr></table></figure><br>可以看到，第一条记录的数据为<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">6.0  2.7  5.1  1.6</span><br></pre></td></tr></table></figure><br>实际类别为1<br>预测结果为<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([[ 0.00329174,  0.99670826]])</span><br></pre></td></tr></table></figure><br>意为属于类别0的概率为0.0032917，属于类别1的概率为0.99670826，两者的和正好为1。故而，求出为类别1的概率之后，用1减去该值就为类别0的概率。</p>
<p>编写一个简单的测试方法，测试改组数据<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def testLRbin(): Unit =&#123;</span><br><span class="line">  val weight = Array(-0.31738779611631857,-1.2346299390640323,1.920449906205768,0.8753093733469249)</span><br><span class="line">  val intercept = -0.24391923532173168</span><br><span class="line">  val model = new LRBinModel(weight, intercept)</span><br><span class="line">  val x = Array(6.0, 2.7, 5.1, 1.6)</span><br><span class="line">  val y = model.predict(x)</span><br><span class="line">  println(f&quot;The Probability Of Class 1 is $&#123;y&#125;&quot;)</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><br>结果为：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">The Probability Of Class 1 is 0.9967082628300301</span><br></pre></td></tr></table></figure><br>属于类别1的概率为0.9967082628300301 和python 的预测结果基本一致。<br>然后就可以根据设定的阈值，判别属于哪一类了。</p>
<h3 id="模型中的调优参数"><a href="#模型中的调优参数" class="headerlink" title="模型中的调优参数"></a>模型中的调优参数</h3><p>对于逻辑回归模型，会有一些参数需要调节，比如<code>C</code>，<code>max_iter</code>，<code>penalty</code>等，这几个值在模型求解的过程中生效，在已经求解的模型中，并无体现。</p>
<h2 id="多元逻辑回归分析"><a href="#多元逻辑回归分析" class="headerlink" title="多元逻辑回归分析"></a>多元逻辑回归分析</h2><p>多元逻辑回归，和二元类似，分别计算属于每个类别的概率，选取其中的最大值作为预测值。具体叙述参见<a href="https://blog.csdn.net/quiet_girl/article/details/70216899"></a></p>
<h3 id="多元逻辑回归python实现"><a href="#多元逻辑回归python实现" class="headerlink" title="多元逻辑回归python实现"></a>多元逻辑回归python实现</h3><p>我们还是使用iris数据集进行示例，这次不用删除类别了。<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from sklearn2pmml import PMMLPipeline</span><br><span class="line">from sklearn.datasets import load_iris</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line">iris = load_iris()</span><br><span class="line">clf = LogisticRegression()</span><br><span class="line">pipeline = PMMLPipeline([(&quot;classifier&quot;, clf)])</span><br><span class="line">pipeline.fit(iris.data, iris.target)</span><br><span class="line"></span><br><span class="line"># 导出为PMML</span><br><span class="line">from sklearn2pmml import sklearn2pmml</span><br><span class="line">sklearn2pmml(pipeline, &quot;LR.pmml&quot;, with_repr = True)</span><br></pre></td></tr></table></figure></p>
<h3 id="多元逻辑回归PMML分析"><a href="#多元逻辑回归PMML分析" class="headerlink" title="多元逻辑回归PMML分析"></a>多元逻辑回归PMML分析</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">&lt;DataDictionary&gt;</span><br><span class="line">	&lt;DataField name=&quot;y&quot; optype=&quot;categorical&quot; dataType=&quot;integer&quot;&gt;</span><br><span class="line">		&lt;Value value=&quot;0&quot;/&gt;</span><br><span class="line">		&lt;Value value=&quot;1&quot;/&gt;</span><br><span class="line">		&lt;Value value=&quot;2&quot;/&gt;</span><br><span class="line">	&lt;/DataField&gt;</span><br><span class="line">	&lt;DataField name=&quot;x1&quot; optype=&quot;continuous&quot; dataType=&quot;double&quot;/&gt;</span><br><span class="line">	&lt;DataField name=&quot;x2&quot; optype=&quot;continuous&quot; dataType=&quot;double&quot;/&gt;</span><br><span class="line">	&lt;DataField name=&quot;x3&quot; optype=&quot;continuous&quot; dataType=&quot;double&quot;/&gt;</span><br><span class="line">	&lt;DataField name=&quot;x4&quot; optype=&quot;continuous&quot; dataType=&quot;double&quot;/&gt;</span><br><span class="line">&lt;/DataDictionary&gt;</span><br><span class="line">&lt;RegressionModel functionName=&quot;classification&quot; normalizationMethod=&quot;logit&quot;&gt;</span><br><span class="line">	&lt;MiningSchema&gt;</span><br><span class="line">		&lt;MiningField name=&quot;y&quot; usageType=&quot;target&quot;/&gt;</span><br><span class="line">		&lt;MiningField name=&quot;x1&quot;/&gt;</span><br><span class="line">		&lt;MiningField name=&quot;x2&quot;/&gt;</span><br><span class="line">		&lt;MiningField name=&quot;x3&quot;/&gt;</span><br><span class="line">		&lt;MiningField name=&quot;x4&quot;/&gt;</span><br><span class="line">	&lt;/MiningSchema&gt;</span><br><span class="line">	&lt;Output&gt;</span><br><span class="line">		&lt;OutputField name=&quot;probability(0)&quot; optype=&quot;continuous&quot; dataType=&quot;double&quot; feature=&quot;probability&quot; value=&quot;0&quot;/&gt;</span><br><span class="line">		&lt;OutputField name=&quot;probability(1)&quot; optype=&quot;continuous&quot; dataType=&quot;double&quot; feature=&quot;probability&quot; value=&quot;1&quot;/&gt;</span><br><span class="line">		&lt;OutputField name=&quot;probability(2)&quot; optype=&quot;continuous&quot; dataType=&quot;double&quot; feature=&quot;probability&quot; value=&quot;2&quot;/&gt;</span><br><span class="line">	&lt;/Output&gt;</span><br><span class="line">	&lt;RegressionTable intercept=&quot;0.26560616797551695&quot; targetCategory=&quot;0&quot;&gt;</span><br><span class="line">		&lt;NumericPredictor name=&quot;x1&quot; coefficient=&quot;0.4149883282957013&quot;/&gt;</span><br><span class="line">		&lt;NumericPredictor name=&quot;x2&quot; coefficient=&quot;1.4612973885622267&quot;/&gt;</span><br><span class="line">		&lt;NumericPredictor name=&quot;x3&quot; coefficient=&quot;-2.2621411772020728&quot;/&gt;</span><br><span class="line">		&lt;NumericPredictor name=&quot;x4&quot; coefficient=&quot;-1.02909509924489&quot;/&gt;</span><br><span class="line">	&lt;/RegressionTable&gt;</span><br><span class="line">	&lt;RegressionTable intercept=&quot;1.0854237423889572&quot; targetCategory=&quot;1&quot;&gt;</span><br><span class="line">		&lt;NumericPredictor name=&quot;x1&quot; coefficient=&quot;0.41663968559520786&quot;/&gt;</span><br><span class="line">		&lt;NumericPredictor name=&quot;x2&quot; coefficient=&quot;-1.6008331852575897&quot;/&gt;</span><br><span class="line">		&lt;NumericPredictor name=&quot;x3&quot; coefficient=&quot;0.5776576286775582&quot;/&gt;</span><br><span class="line">		&lt;NumericPredictor name=&quot;x4&quot; coefficient=&quot;-1.3855384286634223&quot;/&gt;</span><br><span class="line">	&lt;/RegressionTable&gt;</span><br><span class="line">	&lt;RegressionTable intercept=&quot;-1.2147145780786366&quot; targetCategory=&quot;2&quot;&gt;</span><br><span class="line">		&lt;NumericPredictor name=&quot;x1&quot; coefficient=&quot;-1.7075251538239047&quot;/&gt;</span><br><span class="line">		&lt;NumericPredictor name=&quot;x2&quot; coefficient=&quot;-1.5342683399889876&quot;/&gt;</span><br><span class="line">		&lt;NumericPredictor name=&quot;x3&quot; coefficient=&quot;2.4709716807720206&quot;/&gt;</span><br><span class="line">		&lt;NumericPredictor name=&quot;x4&quot; coefficient=&quot;2.5553821129820884&quot;/&gt;</span><br><span class="line">	&lt;/RegressionTable&gt;</span><br><span class="line">&lt;/RegressionModel&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>基本上和二元逻辑回归类似，只是RegressionTable 有三个，这是因为一共有三个类别，需要三组权重矩阵和截距，分别计算属于当前类别的概率值，因而对于多元逻辑回归而言，有多少元，输出的概率值就有多少个。<br>然后再针对所有输出的概率，求和，然后计算每个概率和求和概率的比值。这是为了保证形式上的统一。</p>
<h3 id="Scala实现"><a href="#Scala实现" class="headerlink" title="Scala实现"></a>Scala实现</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">class LRMultiModel(</span><br><span class="line">   val weights: Array[Array[Double]],</span><br><span class="line">   val intercept: Array[Double]</span><br><span class="line">                          ) &#123;</span><br><span class="line"></span><br><span class="line">  def predict(testData:Array[Double]): Array[Double] =&#123;</span><br><span class="line">    val classNum = weights.size</span><br><span class="line">    val pro = new ArrayBuffer[Double]</span><br><span class="line">    for (i &lt;- 0 until classNum)&#123;</span><br><span class="line">      pro.append(predictPoint(testData,weights(i),intercept(i)))</span><br><span class="line">    &#125;</span><br><span class="line">    val sum = pro.sum</span><br><span class="line">    val result = pro.toArray.map(x=&gt;(x/sum))</span><br><span class="line">    result</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def predictPoint(</span><br><span class="line">                    dataMatrix: Array[Double],</span><br><span class="line">                    weightMatrix: Array[Double],</span><br><span class="line">                    intercept: Double): Double = &#123;</span><br><span class="line">    val margin = DenseVector(dataMatrix).dot(DenseVector(weightMatrix)) + intercept</span><br><span class="line">    val score = 1.0 / (1.0 + math.exp(-margin))</span><br><span class="line">    score</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对于python训练的模型，选取head(1)进行测试<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X_t=X_test.head(1)</span><br><span class="line">print(X_t)</span><br><span class="line">y_t=y_test.head(1)</span><br><span class="line">print(y_t)</span><br><span class="line">pipeline.predict_proba(X_t)</span><br></pre></td></tr></table></figure><br>结果如下：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[ 0.00122453  0.39920192  0.59957355]]</span><br></pre></td></tr></table></figure></p>
<p>编写测试方法测试<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">def testLRMul(): Unit =&#123;</span><br><span class="line">   val weight = Array(</span><br><span class="line">     Array(0.4149883282957013,1.4612973885622267,-2.2621411772020728,-1.02909509924489),</span><br><span class="line">     Array(0.41663968559520786,-1.6008331852575897,0.5776576286775582,-1.3855384286634223),</span><br><span class="line">     Array(-1.7075251538239047,-1.5342683399889876,2.4709716807720206,2.5553821129820884)</span><br><span class="line">   )</span><br><span class="line"></span><br><span class="line">   val intercept = Array(0.26560616797551695, 1.0854237423889572, -1.2147145780786366)</span><br><span class="line">   val model = new LRMultiModel(weight, intercept)</span><br><span class="line">   val x = Array(6.0, 2.7, 5.1, 1.6)</span><br><span class="line">   val y = model.predict(x)</span><br><span class="line">   var mclass = 0</span><br><span class="line">   var max = 0.0</span><br><span class="line">   for (i &lt;- 0 until y.size)&#123;</span><br><span class="line">     println(f&quot;The Probability Of Class $&#123;i&#125; is $&#123;y(i)&#125;&quot;)</span><br><span class="line">     if (y(i) &gt; max)&#123;</span><br><span class="line">       max = y(i)</span><br><span class="line">       mclass = i</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   println(s&quot;The Class May Be $&#123;mclass&#125;&quot;)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><br>结果如下：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">The Probability Of Class 0 is 0.0012245308619260621</span><br><span class="line">The Probability Of Class 1 is 0.39920191920408243</span><br><span class="line">The Probability Of Class 2 is 0.5995735499339914</span><br><span class="line">The Class May Be 2</span><br></pre></td></tr></table></figure></p>
<p>两者完全一致。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本文借助了PMML，解析了简单线性回归和逻辑回归的结构。介绍了这两种模型是如何实现预测的。其实所有看起来，或者听起来“高大上”的模型，在码农的眼里，最终的呈现都是一系列的“参数”而已。<br>通过不同的方式将这些参数组合起来，便可实现一些神奇的功能。</p>
<h1 id="REF"><a href="#REF" class="headerlink" title="REF"></a>REF</h1><p><a href="https://blog.csdn.net/fleurdalis/article/details/54931721">https://blog.csdn.net/fleurdalis/article/details/54931721</a><br>李航 统计学习方法</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
              <a href="/tags/pmml/" rel="tag"># pmml</a>
              <a href="/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" rel="tag"># 逻辑回归</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2018/08/05/postCLT/" rel="prev" title="【翻译活动】面向数据科学的概率论-14.中心极限定律">
      <i class="fa fa-chevron-left"></i> 【翻译活动】面向数据科学的概率论-14.中心极限定律
    </a></div>
      <div class="post-nav-item">
    <a href="/2023/08/20/fedai-overview/" rel="next" title="联邦学习哪家强，中国山东找saber">
      联邦学习哪家强，中国山东找saber <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="gitalk-container"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BB%BC%E8%BF%B0"><span class="nav-number">1.</span> <span class="nav-text">综述</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%88%E4%BB%8E%E7%AE%80%E5%8D%95%E7%9A%84%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%BC%80%E5%A7%8B"><span class="nav-number">2.</span> <span class="nav-text">先从简单的线性回归开始</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-number">2.1.</span> <span class="nav-text">什么是线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E9%97%AE%E9%A2%98%E6%B5%81%E7%A8%8B"><span class="nav-number">2.1.1.</span> <span class="nav-text">解决问题流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%87%BD%E6%95%B0%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.1.2.</span> <span class="nav-text">函数模型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%93%8D%E4%BD%9C%E5%AE%9E%E4%BE%8B"><span class="nav-number">2.2.</span> <span class="nav-text">操作实例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#sklearn-%E5%AE%9E%E7%8E%B0%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-number">2.2.1.</span> <span class="nav-text">sklearn 实现线性回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84PMML%E5%8F%8A%E7%BB%93%E6%9E%84"><span class="nav-number">2.2.2.</span> <span class="nav-text">线性回归的PMML及结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%94%A8Scala%E5%AE%9E%E7%8E%B0%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E9%A2%84%E6%B5%8B"><span class="nav-number">2.2.3.</span> <span class="nav-text">用Scala实现线性回归的预测</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="nav-number">3.</span> <span class="nav-text">逻辑回归</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E5%85%83%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="nav-number">3.1.</span> <span class="nav-text">二元逻辑回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%8C%E5%85%83%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92python%E5%AE%9E%E7%8E%B0"><span class="nav-number">3.1.1.</span> <span class="nav-text">二元逻辑回归python实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%8C%E5%85%83%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92PMML%E5%88%86%E6%9E%90"><span class="nav-number">3.1.2.</span> <span class="nav-text">二元逻辑回归PMML分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#scala%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0"><span class="nav-number">3.1.3.</span> <span class="nav-text">scala简单实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%B0%83%E4%BC%98%E5%8F%82%E6%95%B0"><span class="nav-number">3.1.4.</span> <span class="nav-text">模型中的调优参数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E5%85%83%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90"><span class="nav-number">3.2.</span> <span class="nav-text">多元逻辑回归分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E5%85%83%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92python%E5%AE%9E%E7%8E%B0"><span class="nav-number">3.2.1.</span> <span class="nav-text">多元逻辑回归python实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E5%85%83%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92PMML%E5%88%86%E6%9E%90"><span class="nav-number">3.2.2.</span> <span class="nav-text">多元逻辑回归PMML分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Scala%E5%AE%9E%E7%8E%B0"><span class="nav-number">3.2.3.</span> <span class="nav-text">Scala实现</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">4.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#REF"><span class="nav-number">5.</span> <span class="nav-text">REF</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">喵十八</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">58</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">78</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/Yao544303" title="GitHub → https://github.com/Yao544303" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:yao544303963@gmail.com" title="E-Mail → mailto:yao544303963@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">喵十八</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '2fbc96c0bbe149b498b5',
      clientSecret: 'f28baff71cec48b3d80bc5bc9e5da0c20e946b09',
      repo        : 'Yao544303.github.io',
      owner       : 'Yao544303',
      admin       : ['Yao544303'],
      id          : '7b596d2cb4a448c029c7660639218166',
        language: 'en',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script>

</body>
</html>
