<!doctype html>



  


<html class="theme-next mist use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"/>




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  




<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta name="description">
<meta property="og:type" content="website">
<meta property="og:title" content="喵十八の小窝">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="喵十八の小窝">
<meta property="og:description">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="喵十八の小窝">
<meta name="twitter:description">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Mist',
    sidebar: {"position":"left","display":"always"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: '博主'
    }
  };
</script>

  <title> 喵十八の小窝 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">喵十八の小窝</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-home fa-fw"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-th fa-fw"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-user fa-fw"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-archive fa-fw"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-tags fa-fw"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-commonweal">
          <a href="/404.html" rel="section">
            
              <i class="menu-item-icon fa fa-heartbeat fa-fw"></i> <br />
            
            公益404
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/04/11/spark-discuss201803/" itemprop="url">
                  2018_3月讨论整理
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-04-11T22:28:09+08:00" content="2018-04-11">
              2018-04-11
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index">
                    <span itemprop="name">bigdata</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h1><p>一下内容来自群中出现的问题，大家讨论的结果</p>
<h1 id="2018-03-29-01"><a href="#2018-03-29-01" class="headerlink" title="2018.03.29_01"></a>2018.03.29_01</h1><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>如何成为技术大牛</p>
<h2 id="根据阿里的分享"><a href="#根据阿里的分享" class="headerlink" title="根据阿里的分享"></a>根据阿里的分享</h2><p>do more<br>do better<br>do exercise<br><a href="http://mp.weixin.qq.com/s/t1P0mw9Hf4y27EiZB2biXw" target="_blank" rel="external">如何成为技术大牛</a></p>
<h1 id="2018-03-29-02"><a href="#2018-03-29-02" class="headerlink" title="2018.03.29_02"></a>2018.03.29_02</h1><h2 id="问题描述-1"><a href="#问题描述-1" class="headerlink" title="问题描述"></a>问题描述</h2><p>淘宝如何保持宝贝数量的一致性</p>
<h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>淘宝宝贝数量先减去购卖数，不为0就可以同步处理，处理失败再加回来。如果宝贝数量减到一，则竞争。</p>
<h1 id="2018-03-28-01"><a href="#2018-03-28-01" class="headerlink" title="2018.03.28_01"></a>2018.03.28_01</h1><h2 id="问题描述-2"><a href="#问题描述-2" class="headerlink" title="问题描述"></a>问题描述</h2><p>worker 与 executor 线程和进程的相关理解，以及引申为spark 分区等概念</p>
<h2 id="相关思路讨论"><a href="#相关思路讨论" class="headerlink" title="相关思路讨论"></a>相关思路讨论</h2><ol>
<li><p>executor 是进程，在其中执行的task 是线程，spark 所谓的多线程，是一个executor 中可以多个task 执行。<br>1.1 MR 采用了多进程模型，Spark采用了多线程模式，这里指的是同一个节点上多个任务的运行模式。因为无论MR 和 Spark 整体上看，都是多进程：MR是由多个独立的Task 进程组成，Spark 应用程序的运行环境是由多个独立的Executor 进程构建的临时资源池构成的。<br>多进程模型便于细粒度控制每个任务占用的资源，但会消耗较多的启动时间，不适合运行低延迟类型的作业，这是MapReduce广为诟病的原因之一。而多线程模型则相反，该模型使得Spark很适合运行低延迟类型的作业。总之，Spark同节点上的任务以多线程的方式运行在一个JVM进程中，可带来以下好处：<br>1）任务启动速度快，与之相反的是MapReduce Task进程的慢启动速度，通常需要1s左右；<br>2）同节点上所有任务运行在一个进程中，有利于共享内存。这非常适合内存密集型任务，尤其对于那些需要加载大量词典的应用程序，可大大节省内存。<br>3）同节点上所有任务可运行在一个JVM进程(Executor)中，且Executor所占资源可连续被多批任务使用，不会在运行部分任务后释放掉，这避免了每个任务重复申请资源带来的时间开销，对于任务数目非常多的应用，可大大降低运行时间。与之对比的是MapReduce中的Task：每个Task单独申请资源，用完后马上释放，不能被其他任务重用，尽管1.0支持JVM重用在一定程度上弥补了该问题，但2.0尚未支持该功能。<br>尽管Spark的过线程模型带来了很多好处，但同样存在不足，主要有：<br>1）由于同节点上所有任务运行在一个进程中，因此，会出现严重的资源争用，难以细粒度控制每个任务占用资源。与之相反的是MapReduce，它允许用户单独为Map Task和Reduce Task设置不同的资源，进而细粒度控制任务占用资源量，有利于大作业的正常平稳运行<br>ref: <a href="http://dongxicheng.org/framework-on-yarn/apache-spark-multi-threads-model/" target="_blank" rel="external">董西城的解释</a></p>
</li>
<li><p>spark分区数,task数目,core数,worker节点个数,excutor数量梳理<br>输入可能以多个文件的形式存储在HDFS上，每个File都包含了很多块，称为Block。<br>当Spark读取这些文件作为输入时，会根据具体数据格式对应的InputFormat进行解析，一般是将若干个Block合并成一个输入分片，称为InputSplit，注意InputSplit不能跨越文件。<br>随后将为这些输入分片生成具体的Task。InputSplit与Task是一一对应的关系。<br>随后这些具体的Task每个都会被分配到集群上的某个节点的某个Executor去执行。</p>
</li>
</ol>
<ul>
<li>每个节点可以起一个或多个Executor。</li>
<li>每个Executor由若干core组成，每个Executor的每个core一次只能执行一个Task。</li>
<li>每个Task执行的结果就是生成了目标RDD的一个partiton。<br>这里的core是虚拟的core而不是机器的物理CPU核，可以理解为就是Executor的一个工作线程。</li>
</ul>
<p>至于partition的数目：</p>
<ul>
<li>对于数据读入阶段，例如sc.textFile，输入文件被划分为多少InputSplit就会需要多少初始Task。</li>
<li>在Map阶段partition数目保持不变。</li>
<li>在Reduce阶段，RDD的聚合会触发shuffle操作，聚合后的RDD的partition数目跟具体操作有关，例如repartition操作会聚合成指定分区数，还有一些算子是可配置的。</li>
</ul>
<p>ref <a href="https://www.cnblogs.com/hadoop-dev/p/6669232.html" target="_blank" rel="external">梳理</a></p>
<p>2.1 一种理解：<br>executor 不是越多越好，假设你给spark 分配总的资源 为48 个vcore<br>(这个不是物理的CPU核，一个物理CPU核可以划分成多个vcore，是为了更细粒度的资源控制，<br>这样对应小任务较多情况，能提升资源利用率，打个比方，我一个CPU能分成4个vcore 和 1 个vcore，<br> 我任务1,2,3,4 其实都只要使用1个vcore 即1/4 个物理CPU，在分成4个vcore 的时候，<br> 一个CPU能并行处理4个任务，在分1个vcore时，只能串行)<br>然后96G 的内存<br>及 48c 96g<br>假设你的参数如下：<br>num-executor 8<br>executor-cores 8</p>
<p>8*8 =64 &gt; 48 了，这个时候，只会给你 48/8 =6 个executor</p>
<p>同理对于<br>num-executor    8<br>executor-menory 18<br>8*18=144 &gt; 96 了， 这个时候只会给你 96/8 =12 个executor</p>
<p>当 cores 和 menory 都超的时候，取小的，总之资源不能超总的。</p>
<p>在每个stage的时候，会根据partition的数据，划分出task的数量，一个vcore 同一时间只能处理一个task<br>假设有4个executor ，每个executor 有4个vcore，即同时处理的task数量为16 （这是你集群的处理能力）<br>那么当你的分区，只有4个的时候，即只有4个task，就意味着你的集群资源是空着的，没有利用满。（最好reparation）</p>
<p>当你的分区有32个的时候， 意味着需要 32/16 2轮能处理完。这时候，你再调大并行度，也没有用，集群资源就那么多。</p>
<p>还有关于并行度的提高，也并不是越大越好。<br>1是小文件的问题<br>2是任务太小的话，启动任务的时间占比 相对任务的处理时间占比也会很高，这样得不偿失。</p>
<p>2.2 另一种思路<br>一般每个partition对应一个task。在我的测试过程中，如果没有设置spark.default.parallelism参数，spark计算出来的partition非常巨大，与我的cores非常不搭。我在两台机器上（8cores <em>2 +6g </em> 2）上，spark计算出来的partition达到2.8万个，也就是2.9万个tasks，每个task完成时间都是几毫秒或者零点几毫秒，执行起来非常缓慢。在我尝试设置了 spark.default.parallelism 后，任务数减少到10，执行一次计算过程从minute降到20second</p>
<p>spark.default.parallelism 的说明见 <a href="https://blog.csdn.net/bbaiggey/article/details/51984753" target="_blank" rel="external">说明</a><br>一句话，就是触发shuffle 操作之后的默认分区数，相当于手动reparation</p>
<h1 id="2018-03-28-02"><a href="#2018-03-28-02" class="headerlink" title="2018.03.28_02"></a>2018.03.28_02</h1><h2 id="问题描述-3"><a href="#问题描述-3" class="headerlink" title="问题描述"></a>问题描述</h2><p>两个巨大的表,默认都要用reduce join. 且其中一个表中join依赖的相同key的数据大量重复.考虑到数据倾斜,这个partitioner怎么实现?</p>
<h2 id="思路-1"><a href="#思路-1" class="headerlink" title="思路"></a>思路</h2><p>这是一个半连接的问题</p>
<ol>
<li>先把重复比较厉害的key 过滤了，（合并了） 再做map侧的join</li>
</ol>
<p>ref：<br><a href="https://www.zhihu.com/question/39680151" target="_blank" rel="external">如何利用spark快速计算笛卡尔积？</a><br><a href="https://www.zhihu.com/question/31979689?utm_medium=social&amp;utm_source=qq" target="_blank" rel="external">spark千万数据join问题?</a><br><a href="https://blog.csdn.net/lzm1340458776/article/details/43017425" target="_blank" rel="external">MapReduce表连接之半连接SemiJoin</a></p>
<h1 id="2018-03-28-03"><a href="#2018-03-28-03" class="headerlink" title="2018.03.28_03"></a>2018.03.28_03</h1><h2 id="问题描述-4"><a href="#问题描述-4" class="headerlink" title="问题描述"></a>问题描述</h2><p>spark sql 的partition 数目</p>
<h2 id="思路-2"><a href="#思路-2" class="headerlink" title="思路"></a>思路</h2><ol>
<li>我是这么理解Spark 200G 内存处理2T数据的。<br>打个比方，内存为100G 有2000G 的数据，一共10000个partition，每个partition就是200M （方便起见按1G=1000M 计算）<br>假设触发shuffle的stage1，每个partition处理的约为200M数据。<br>默认的配置，一个executor 1C2G。 2G内存处理200M数据，还是没问题的。<br>这样就有100/2=50 个executor，处理10000个task 一共需要10000/50 = 200轮。</li>
</ol>
<p>stage1的数据处理的结果，你可以选择cache 到内存，也可以选择到disk。这样再进行stage2的操作。<br>（假设处理结果为1000G 还是10000个partition， 这样只能到disk 了）<br>那stage2处理方式一样，从disk读数据，然后进行10000/50 = 200轮 的操作。每个executor 有2G，处理100M 还是可以的。</p>
<ol>
<li>hive的分区数目和Spark 的partition 数无关<br>你可以去hdfs上查看，hive的分区目录下也有分片的文件如：(假设分区字段为时间)<br>table/20180101/part-00000<br>~<br>table/20180101/part-00010<br>那这个分区下的文件，对应Spark 中10个partition<br>然后通过spark.sql.shuffle.partitions 参数来调节执行sql中shuffle 时的task数量</li>
</ol>
<h1 id="2017-03-27-01"><a href="#2017-03-27-01" class="headerlink" title="2017.03.27_01"></a>2017.03.27_01</h1><h2 id="问题描述："><a href="#问题描述：" class="headerlink" title="问题描述："></a>问题描述：</h2><p>如这样的 rdd(k,v):RDD[(String, String)]<br>需要对第一个字段 k进行归并，然后统计v去重之后出现次数最多的字符（分组 top1）。</p>
<p>例如<br>a,A<br>a,B<br>a,A<br>b,C<br>b,D<br>c,D<br>c,D</p>
<p>结果为 (a,A) (b,C) (c,D)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">rdd.map(x=&gt;(x._1 + &quot;|&quot; +  x._2, 1))</span><br><span class="line">  .reduceByKey(_ + _)</span><br><span class="line">  .map(x=&gt;(x._1.split(&quot;\\|&quot;, -1)(0), (x._1.split(&quot;\\|&quot;, -1)(1),x._2)))</span><br><span class="line">  .reduceByKey((x,y) =&gt;&#123;</span><br><span class="line">    val re = if (x._2 &gt; y._2)&#123;</span><br><span class="line">      x</span><br><span class="line">    &#125;else &#123;</span><br><span class="line">      y</span><br><span class="line">    &#125;</span><br><span class="line">    re</span><br><span class="line">  &#125;)</span><br><span class="line">  .map(x=&gt;(x._1,x._2._1))</span><br><span class="line">```  </span><br><span class="line"></span><br><span class="line">## 扩展</span><br><span class="line">1. topN 的问题该如何处理？ 除了groupBy 还有别的思路么？ 效率？</span><br><span class="line">2. 次数相同该如何处理？</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 2018.03.27_02</span><br><span class="line">## 问题描述：</span><br><span class="line">如这样的 rdd(k,v):RDD[(String, String)]</span><br><span class="line">需要对第一个字段 k进行归并，然后统计v去重之后出现的次数。</span><br><span class="line"></span><br><span class="line">例如</span><br><span class="line">a,A</span><br><span class="line">a,B</span><br><span class="line">a,A</span><br><span class="line">b,C</span><br><span class="line">b,D</span><br><span class="line">c,D</span><br><span class="line">c,D</span><br><span class="line"></span><br><span class="line">结果为 (a,2) (b,2) (c,1)</span><br><span class="line"></span><br><span class="line">## 目前整理的几种思路是：</span><br><span class="line">1. 先进行reduceBy,将v合并为字符串，之后再拆分、去重，统计出现次数</span><br></pre></td></tr></table></figure>
<p>map(a,b).reduceByKey((x1,x2)=&gt;{<br>val sum =x1+”\t”+x2<br>sum}).map{x._2.split(“\t”).distinct.size}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">2. 将k,v 拼接为一个字符串，去重，之后再差分，然后再进行reduceBy</span><br></pre></td></tr></table></figure></p>
<p>rdd.map(x=&gt;(x._1 +”|” x.<em>2)).distinct().map((</em>.split(“|”)(0),1)).reduceByKey(<em>+</em>)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">3. 将v 改为Set，reduce 之后再统计size</span><br></pre></td></tr></table></figure></p>
<p>rdd.map(x=&gt;(x._1, Set(x.<em>2)).reduceByKey(</em> ++ _).map(x=&gt;(x._1, x._2.size))<br>```</p>
<ol>
<li>使用dropDuplicates()<br>需要spark 2.x ，将rdd转为DataSet 之后再进行操作。<h2 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h2></li>
<li>四种思路比较？ 是否有更好的解决方案？</li>
<li>当v 也是多个字段该如何处理？</li>
<li>Set 和 String的序列化效率差异？即 String 合并之后再拆分的开销，和使用Set 序列化增长的开销 哪个比较大？</li>
</ol>

          
        
      
    </div>
    
    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/04/11/dttry1/" itemprop="url">
                  DT近期合作爬坑记录
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-04-11T22:24:52+08:00" content="2018-04-11">
              2018-04-11
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index">
                    <span itemprop="name">bigdata</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="DT代码中的坑"><a href="#DT代码中的坑" class="headerlink" title="DT代码中的坑"></a>DT代码中的坑</h1><p>连续两周时间都在支持DT以及相关的label的开发<br>能够明显的发现DT提供的代码质量非常之差。列举出来，前事不忘后事之师。</p>
<h2 id="hard-core"><a href="#hard-core" class="headerlink" title="hard core"></a>hard core</h2><p>在spark的代码中，将master 以及入参全部hard core，入参不必说他，将master 设置之后，我spark submit可是会报错的啊。</p>
<h2 id="不转为String-直接saveAsTextFile"><a href="#不转为String-直接saveAsTextFile" class="headerlink" title="不转为String 直接saveAsTextFile"></a>不转为String 直接saveAsTextFile</h2><p>常常出现<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(ABACD,ADF,1)</span><br><span class="line"></span><br><span class="line">[Ljava.long.String;@76abcd405]</span><br></pre></td></tr></table></figure></p>
<p>前者是元组直接输出，后者输出的是地址，写代码的时候一定需要注意</p>
<h2 id="集群600G-内存，输入200G-全部cache"><a href="#集群600G-内存，输入200G-全部cache" class="headerlink" title="集群600G 内存，输入200G 全部cache"></a>集群600G 内存，输入200G 全部cache</h2><p>恩 cache 的确可以提高效率，但是你这个样子做，确定不会oom？</p>
<h2 id="多次join"><a href="#多次join" class="headerlink" title="多次join"></a>多次join</h2><p>输入为 （A,B,C）<br>希望得到的输出 （A,B/sum(B),B,D）<br>做了多次join,开销非常之大<br>在实践之前，可以先进行采样，加入对A进行reduce 之后 ，按key分布的数据量不大，倾斜不严重的情况下，<br>可以将join 转变<br>map 处理为 RDD[String,Map]<br>再reduce， 得到RDD[String,Map]之后，在map内部进行相似逻辑的操作，这样能提高效率。</p>
<h2 id="不做异常检测"><a href="#不做异常检测" class="headerlink" title="不做异常检测"></a>不做异常检测</h2><p>维表可能存在空值，不做异常检测，直接进行string =》 int 的转化，必然异常。</p>
<h2 id="过滤数据"><a href="#过滤数据" class="headerlink" title="过滤数据"></a>过滤数据</h2><p>接上，对空值的过滤，需要谨慎再谨慎，每条数据都是很宝贵的，需要非常认真的对待，建议在filter之前，先sample一下，看看数据是什么样子，看看要filter的数据是什么样子，再做决断。</p>
<h2 id="sample的重要性"><a href="#sample的重要性" class="headerlink" title="sample的重要性"></a>sample的重要性</h2><p>既然用到了spark 处理的数据量级自然不会小，在大数据量测试之前务必使用小数据量进行逻辑的验证，直接用大数据量跑的话，耗时耗资源不去说，万一错了，代价也很大。</p>
<h1 id="其他非代码的坑"><a href="#其他非代码的坑" class="headerlink" title="其他非代码的坑"></a>其他非代码的坑</h1><h2 id="维表过多"><a href="#维表过多" class="headerlink" title="维表过多"></a>维表过多</h2><p>维表过多，导致管理起来非常困难，一定要协商好一个更新机制</p>
<h2 id="Spark-submit-脚本"><a href="#Spark-submit-脚本" class="headerlink" title="Spark-submit 脚本"></a>Spark-submit 脚本</h2><p>这个必须有，整理的晚了，每次提交都要重新编写，虽然时间不多，但多几次，很容易让人狂躁<br>整理了如下一个模板<br><a href="https://github.com/Yao544303/shell/blob/master/java/spark_submit.sh" target="_blank" rel="external">spark-submit模板</a></p>
<h2 id="信息沟通必须及时"><a href="#信息沟通必须及时" class="headerlink" title="信息沟通必须及时"></a>信息沟通必须及时</h2>
          
        
      
    </div>
    
    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/04/11/lookalike/" itemprop="url">
                  Lookalike 技术调研
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-04-11T22:20:08+08:00" content="2018-04-11">
              2018-04-11
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/DM/" itemprop="url" rel="index">
                    <span itemprop="name">DM</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="What"><a href="#What" class="headerlink" title="What"></a>What</h1><p>基本上所有的互联网公司都有其广告投放平台，这是给广告主投放广告的一个页面。广告主可以通过广告提交页面提交自己的广告需求，后台会给广告主圈定一部分潜在用户，这个就是我们称为Lookalike的模块。<br><strong>lookalike 不是某一种特定的算法，而是一类方法的统称，这类方法综合运用多种技术，最终达到目的。</strong></p>
<h1 id="How"><a href="#How" class="headerlink" title="How"></a>How</h1><p><strong>第一种就是显性的定位，广告主根据用户的标签直接定位</strong><br>比如说通过年龄、性别、地域这样的标签来直接圈定一部分用户进行投放。这个时候我们的技术支持就是后台的用户画像的挖掘。</p>
<p><strong>第二种做法，通过一个机器学习的模型，来定位广告主的潜在用户</strong><br>广告主提交一批客户名单，我们称之为种子用户，它作为机器学习的正样本。负样本我们会从非种子用户，或者是说平台会积累历史的一些相似的广告作为负样本，这个问题就转化为一个二分类的模型，正负样本组成学习的样本，训练模型之后，利用模型结构对活跃用户进行打分，最后得到广告主需要的目标人群。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3698622-811ff11444c330b7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="通过机器学习方法定位目标人群"></p>
<h1 id="各大厂的做法"><a href="#各大厂的做法" class="headerlink" title="各大厂的做法"></a>各大厂的做法</h1><p>对于特征和模型算法，不同的公司各有差异：特征取决于公司有哪些数据；在模型算法上，Facebook 和Google对外公布的说法就是一个预测模型，Yahoo发表过几篇论文，详细介绍过它的算法，比如LR，Linear SVM，GBDT都有尝试，论文里面提到的是GBDT的效果比较好。下图列出了不同公司的做法，供大家参考</p>
<p><strong><em>吐槽  google 和 非死不可的 predict model 怕不是用了深度学习？   私以为 两家公司 节操没这么高。</em></strong></p>
<p><img src="https://upload-images.jianshu.io/upload_images/3698622-b51da7acbcb0f90c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="各大厂做法"></p>
<h1 id="需要哪些特征"><a href="#需要哪些特征" class="headerlink" title="需要哪些特征"></a>需要哪些特征</h1><ol>
<li>行为结果数据<br>所谓行为结果数据是已经采取了具体行动的数据，例如购买数据，入资数据等。</li>
<li>行为意向数据<br>所谓行为意向数据是倾向于采取某种行为的人群数据，最典型的是搜索引擎的数据，一般来说消费者在做最终的购买决策之前，往往会通过搜索引擎了解产品周边的一些相关信息，相关搜索关键字数据可以定位到一个有强购买倾向的人。这也是很多广告主投入较多预算在SEM上的原因。但这种数据一般很难从搜索引擎侧获取，购买关键字的成本也越来越高。一般来说，通过行为意向数据来寻找人群，转化率会比较高，因为行为意向人群往往已经达到了转化前的最后一步的关键时刻，此时对意向人群进行营销，效果往往很明显。但同时广告主也面临一定的风险，因为这时客户可能已被别的竞品在更早的环节进行了影响，转化成本也相应提高。</li>
<li>行为偏好数据<br>对于大多数第三方DMP平台来说，主要还是通过这一类数据来帮助广告主找到潜在的人群，从业务逻辑来说，具有某种偏好或者属于某种类型的人群往往会更倾向于购买某款产品，对于这部分数据的学习也能促成最终的转化。而且行为偏好数据会保证广告主在潜在客群覆盖规模和精准度之间达到一个很好的平衡，因此也是广告主普遍选用的一种数据。</li>
<li>行为模式数据<br>所谓行为模式是指通过分析消费者的行为与时间、空间的关系，以及一系列行为之间的时间和空间序列关系，总结出的具有一定一致性意义的行为表现，通过这些一致性模式预测相关行为。行为模式数据往往应用于场景营销，但是由于加工行为模式的数据计算复杂度较高，同时对分析的实时性要求也很高，因此目前还处在探索和优化阶段，实际的应用落地不多。</li>
</ol>
<h1 id="应用tips"><a href="#应用tips" class="headerlink" title="应用tips"></a>应用tips</h1><ol>
<li>结合聚类算法一起使用<br>有时候客户提供过来的种子人群成分是非常复杂的，往往是参杂了大量子类人群的总和，如果直接拿这些种子人群进行lookaLike，则相当于把人群的特征进行了弱化，最终找出来的相似人群特征会变得不明显。例如某奢侈品牌，他们的一方种子人群中包含2类，一类是真正有钱的人群，平时开豪车住别墅的，另外一类是普通的城市小白领，他们往往攒好几个月的工资进行一次消费。这2种人群必须先通过聚类算法区分出来，然后再输入lookaLike算法去扩大。</li>
<li>在什么媒体上用<br>LookaLike算法选出的人群最终是在媒体的流量人群中实现触达，因此媒体自身流量对最终lookaLike算法落地的效果影响非常大，例如我们做过的某次营销案例，选取某DSP做为精准营销的落地媒体，在整个4周的营销过程中，最终选取的精准人群只有2%曝光成功。（一方面由于该DSP媒体流量均为长尾流量，而我们选取的目标人群为金融类目标人群，该DSP对目标人群覆盖率低，另外由于低价策略，竞价成功率低也导致了最终触达的精准人群规模比较小。）最终我们分析了这2%成功曝光的人群，发现他们也是Lookalike算法相似度相对较低的，也就是说最相似的那部分目标人群在该媒体上并没有出现和竞得。<br>因此为了保证lookaLike算法落地的效果，选取与广告主自身产品相对匹配的目标媒体以及合适的出价都非常重要。</li>
<li>根据效果数据优化lookaLike算法<br>一旦精准营销活动开始后，就可以回收消费者对营销的反馈数据做为正样本来对lookaLike算法进行优化。通过TalkingData对大量历史投放数据的分析，动态优化lookaike算法可以极大的提升算法的转化效果：在同样选取相似度TOP100w样本进行精准投放的情况下，每日优化样本库组相比较不优化组在一周的投放周期内，可提升激活率180％以上。样本库优化的周期可以根据效果数据回收的量级、媒体的技术支持能力、以及DMP平台自身的数据更新周期综合决定，建议每1-2日更新目标用户群。</li>
</ol>
<h1 id="一些实际例子"><a href="#一些实际例子" class="headerlink" title="一些实际例子"></a>一些实际例子</h1><h2 id="利用用户画像，给用户打标签，利用相同标签找到目标人群"><a href="#利用用户画像，给用户打标签，利用相同标签找到目标人群" class="headerlink" title="利用用户画像，给用户打标签，利用相同标签找到目标人群"></a>利用用户画像，给用户打标签，利用相同标签找到目标人群</h2><p>实例：美的豆浆机通过Youmi DSP进行了Look-alike人群扩展投放<br>有米广告取得美的家电第一方消费者数据，涵盖浏览、购买行为等ID信息。通过导入Youmi DMP进行全库记录匹配，找到个体的在线历史大数据。经由人群分析模型，有米洞察到美的用户的个性倾向特征，通过标签算法挖掘，将数据库中拥有高相似画像的人群列为一类精准用户。根据标签模型，得出这些用户具有较多且重合的“健康”“时尚”“亲子”“女性”“中高收入”“一二线城市”等细分人群画像。</p>
<p>分析：利用用户画像给用户打上各类标签。根据种子人群分析大部分种子用户具有的标签特征 例如：家庭女性、30-40岁、已婚，未生小孩，健康。那么对于一个标签为：上班族，30-40岁、已婚，未生小孩，健康 女性就是其目标人群。</p>
<h2 id="利用分类算法来实现的：种子人群为正样本，候选对象为负样本，训练分类模型，然后用模型对所有候选对象进行筛选。"><a href="#利用分类算法来实现的：种子人群为正样本，候选对象为负样本，训练分类模型，然后用模型对所有候选对象进行筛选。" class="headerlink" title="利用分类算法来实现的：种子人群为正样本，候选对象为负样本，训练分类模型，然后用模型对所有候选对象进行筛选。"></a>利用分类算法来实现的：种子人群为正样本，候选对象为负样本，训练分类模型，然后用模型对所有候选对象进行筛选。</h2><p>显然候选样本并发所有的样本都是负样本，所有这是一个典型的PU learning问题<br>PU learning：Positive and unlabeled learning</p>
<h2 id="利用社交网络进行人群扩散：利用好友关系，将种子人群标签传给社区中的好友，从而实现人群扩散"><a href="#利用社交网络进行人群扩散：利用好友关系，将种子人群标签传给社区中的好友，从而实现人群扩散" class="headerlink" title="利用社交网络进行人群扩散：利用好友关系，将种子人群标签传给社区中的好友，从而实现人群扩散"></a>利用社交网络进行人群扩散：利用好友关系，将种子人群标签传给社区中的好友，从而实现人群扩散</h2><h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><ol>
<li><p>数据准备<br>　　① 获得用户的属性（User Profile），如性别、年龄、学历、职业、地域、能力标签等；<br>　　② 根据项目内容和活动内容制定一套受众标签（Audience Label）；<br>　　③ 提取用户之间的关注关系，微博之间的转发关系；<br>　　④ 获取微博message 中的文本内容；<br>　　⑤ 获得微博message 中的图片内容。</p>
</li>
<li><p>用户标签特征处理<br>　　① 根据步骤1 中用户属性信息和已有的部分受众标签系统。利用GBDT 算法（可以直接用xgboost）将没有标签的受众全部打上标签。这个分类问题中请注意处理连续值变量以及归一化。<br>　　② 将标签进行向量化处理，这个问题转化成对中文单词进行向量化，这里用word2vec 处理后得到用户标签的向量化信息Label2vec。这一步也可以使用word2vec在中文的大数据样本下进行预训练，再用该模型对标签加以提取，对特征的提取有一定的提高，大约在0.5%左右。</p>
</li>
<li><p>文本特征处理<br>　　清洗整理步骤1 中提取到的所有微博message 文本内容，训练doc2vec 模型，得到单个文本的向量化表示，对所得的文本作聚类（KMeans，在30 万的微博用户的message 上测试，K 取128 对文本的区分度较强），最后提取每个cluster 的中心向量，并根据每个用户所占有的cluster 获得用户所发微博的文本信息的向量表示Content2vec。</p>
</li>
<li><p>图像特征<br>　　将步骤1 中提取到的所有的message 图片信息进行整理分类，使用预训练卷积网络模型（这里为了平衡效率选取VGG16 作为卷积网络）提取图像信息，对每个用户message 中的图片做向量化处理，形成Image2vec，如果有多张图片则将多张图片分别提取特征值再接一层Max Pooling 提取重要信息后输出。</p>
</li>
<li><p>社交关系建立（node2vec 向量化）<br>　　将步骤1 数据准备中获得的用户之间的关系和微博之间的转发评论关系转化成图结构，并提取用户关系sub-graph，最后使用node2vec 算法得到每个用户的社交网络图向量化表示。下图为社交关系化后的部分图示。</p>
</li>
</ol>
<p><img src="https://upload-images.jianshu.io/upload_images/3698622-fae87cac852a4e89.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="社交关系向量化"></p>
<p><strong>fully connected layers,FC</strong><br>        将步骤2345 得到的向量做拼接，经过两层FC，得到表示每个用户的多特征向量集（User Vector Set, UVS）。这里取的输出单元个数时可以根据性能和准确度做平衡，目前英特实现的是输出512 个单元，最后的特征输出表达了用户的社交关系、用户属性、发出的内容、感兴趣的内容等的混合特征向量，这些特征向量将作为下一步比对相似性的输入值。<br>　　分别计算种子用户和潜在目标用户的向量集，并比对相似性。英特使用的是余弦相似度计算相似性，将步骤6 得到的用户特征向量集作为输入x 和y，代入下面公式计算相似性。<br><img src="https://upload-images.jianshu.io/upload_images/3698622-6081cbd4c2554129.gif?imageMogr2/auto-orient/strip" alt=""><br>注意：余弦相似度更多是从方向上区分差异，而对绝对的数值不敏感，因此没法衡量每个维度值的差异。这里要在每个维度上减去一个均值或者乘以一个系数，或者在之前做好归一化。</p>
<ol>
<li>受众扩展<br>　　① 获取种子受众名单，以及目标受众的数量N；<br>　　② 检查种子用户是否存在于UVS 中，将存在的用户向量化；<br>　　③ 计算受众名单中用户和UVS 中用户的相似度，提取最相似的前N 个用户作为目标受众。</li>
</ol>
<p>　　最后将以上步骤串联起来，形成流程图。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3698622-947d0c0c9ded25e4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Lookalike 算法流程图
"></p>
<p>在以上步骤提取完特征后，英特使用一个两层的神经网络做最后的特征归并提取，算法结构示意图如下</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3698622-c59e57ca74f9f2a3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Lookalike 算法结构示意图"><br>其中FC1 层也可以替换成Max Pooling，Max Pooling 层具有强解释性，也就是把用户特征群上提取的最重要的特征点作为下一层的输入，读者可以自行尝试，这里限于篇幅问题就不做展开了。</p>
<h1 id="来自Youtube-amp-google"><a href="#来自Youtube-amp-google" class="headerlink" title="来自Youtube &amp;  google"></a>来自Youtube &amp;  google</h1><p>深度候选人生成模型 + 分布式打分模型<br>使用的是分类方式，将客户分成可能的N类，选取打分最高的类<br>引入DNN 的好处在于大多数类型的连续特征和离散特征可以直接添加到模型当中。</p>
<h1 id="ref"><a href="#ref" class="headerlink" title="ref:"></a>ref:</h1><p><a href="">Deep Neural Networks for YouTube Recommendations</a><br><a href="">Audience Expansion for Online Social Network Advertising</a><br><a href="https://zhuanlan.zhihu.com/p/25509178" target="_blank" rel="external">微信广告推广的</a><br><a href="http://blog.csdn.net/kwame211/article/details/77568021" target="_blank" rel="external">综述类型的</a></p>

          
        
      
    </div>
    
    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/12/27/percentrank/" itemprop="url">
                  Spark 爬坑记录之percent_rank()
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-12-27T23:14:20+08:00" content="2017-12-27">
              2017-12-27
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index">
                    <span itemprop="name">bigdata</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>在Kmeans 预处理之前的数据预处理部分，需要做一次类似percent_rank() 操作。<br>举个例子，输入一条记录为user_1,key1,value_1<br>输入多条记录<br>按照key1 聚合之后，结果为key1,list[value1,value2…..]<br>将list 排序，并获取count  结果为    key1，count，list<br>然后计算user1 这条记录的rank  =  index（value1）/count</p>
<h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">    val ah_freqlist_dic_old = user_ah_freq_dic.map(x=&gt;(x._1._2,Nil:+x._2))</span><br><span class="line">      .reduceByKey(_:::_)</span><br><span class="line">      .map(x=&gt;(x._1,(x._2.length).toString +: x._2.sorted))</span><br><span class="line">      .collectAsMap()</span><br><span class="line">val ah_freqlist_dic = sc.broadcast(ah_freqlist_dic_old)</span><br><span class="line">val user_ah_percent_dic = user_ah_freq_dic.map(t=&gt;((t._1._1,t._1._2),ah_freqlist_dic.value.getOrElse(t._1._2,Nil).indexOf(t._2)/((ah_freqlist_dic.value.getOrElse(t._1._2,Nil)(0).toInt).toFloat)))</span><br></pre></td></tr></table></figure>
<p>上述代码在实现的时候效率极低。</p>
<p>当把这段代码换成 percent_rank() 之后，奇迹发生了<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val df = sqlContext.createDataFrame(user_ah_freq_dic).toDF(&quot;user&quot;,&quot;ah&quot;,&quot;freq&quot;)</span><br><span class="line">   df.registerTempTable(&quot;uahf&quot;)</span><br><span class="line">   val sql=&quot;select user,ah,percent_rank() over (partition by ah order by freq) as rank from uahf&quot;</span><br><span class="line"></span><br><span class="line">   val user_ah_percent_dic = sqlContext.sql(sql).rdd.map(x=&gt;((x(0).toString,x(1).toString),x(2).toString.toFloat))</span><br></pre></td></tr></table></figure></p>
<p>果然spark sql 的是做了优化的。阅读spark sql 的源码，又有计划了</p>

          
        
      
    </div>
    
    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/12/27/scala版本冲突/" itemprop="url">
                  Spark 爬坑记录之开发Scala版本冲突
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-12-27T23:14:09+08:00" content="2017-12-27">
              2017-12-27
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index">
                    <span itemprop="name">bigdata</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>若在maven 中配置了<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&lt;plugin&gt;</span><br><span class="line"> &lt;groupId&gt;org.scala-tools&lt;/groupId&gt;</span><br><span class="line"> &lt;artifactId&gt;maven-scala-plugin&lt;/artifactId&gt;</span><br><span class="line"> &lt;executions&gt;</span><br><span class="line"> &lt;execution&gt;</span><br><span class="line"> &lt;goals&gt;</span><br><span class="line"> &lt;goal&gt;compile&lt;/goal&gt;</span><br><span class="line"> &lt;goal&gt;testCompile&lt;/goal&gt;</span><br><span class="line"> &lt;/goals&gt;</span><br><span class="line"> &lt;/execution&gt;</span><br><span class="line"> &lt;/executions&gt;</span><br><span class="line"> &lt;configuration&gt;</span><br><span class="line"> &lt;scalaVersion&gt;$&#123;scala.version&#125;.6&lt;/scalaVersion&gt;</span><br><span class="line"> &lt;args&gt;</span><br><span class="line"> &lt;arg&gt;-target:jvm-1.7&lt;/arg&gt;</span><br><span class="line"> &lt;/args&gt;</span><br><span class="line"> &lt;/configuration&gt;</span><br><span class="line">&lt;/plugin&gt;</span><br></pre></td></tr></table></figure></p>
<p>无需再在IDEA中配置 scala 的sdk 版本不一致会冲突</p>
<p>注意这一行，最好标注小版本 2.10.6</p>
<scalaversion>2.10.6</scalaversion>

          
        
      
    </div>
    
    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/12/27/spark2-x迁移至1-6/" itemprop="url">
                  将基于Spark 2.x 开发的LDA 程序 迁移至Spark 1.6 的环境
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-12-27T23:10:50+08:00" content="2017-12-27">
              2017-12-27
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index">
                    <span itemprop="name">bigdata</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>对方提供的LDA 聚类程序，是基于Spark 2.x 的，但是，我们的生产环境是Spark 1.6。 恩那么问题就来了，怎么让基于Spark 2.x 的代码在Spark 1.6 上跑起来。第一个想法是，只要把SparkSession 改为SparkContext 和 SQLContext 就行了。然后发现自己真的图样图森破。对方非常高端的使用了ml库，当然这不是问题。对方是用python 写的，当然这也不是问题<del>毕竟小学生都要学python了，作为一个程序员不会python 就说不过去了</del>对方的python 版本是2.7，当然这更不是问题。但是当你遇到需要将python 2.7 写的基于Spark 2.x  ml 写的代码迁移到spark 1.6 上的时候，就有问题了。<br><del>他喵的，老子都要精分了，一直在python 2.7 python 3.0  spark 1.6  spark2.2 版本间切换，在java，scala ，python 语言间切换</del></p>
<h1 id="改造经过"><a href="#改造经过" class="headerlink" title="改造经过"></a>改造经过</h1><p>任务到手，谋定后动，当然先开始分析。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">spark = SparkSession.builder.master(&apos;yarn&apos;) \</span><br><span class="line">	.appName(&apos;marketing_data_lda_clustering&apos;) \</span><br><span class="line">	.enableHiveSupport() \</span><br><span class="line">	.getOrCreate()</span><br></pre></td></tr></table></figure></p>
<p>这个，当然小case啦。改成sparkContext 就行了<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">conf = SparkConf().setMaster(&apos;yarn&apos;) \</span><br><span class="line">	.setAppName(&apos;marketing_data_lda_clustering&apos;) </span><br><span class="line">sc = SparkContext(conf = conf)	</span><br><span class="line">sqlContext = SQLContext(sc)</span><br></pre></td></tr></table></figure></p>
<p>然后，以为就搞定了，开开心心的spark-submit。不出意外的报错，提示下面代码缺少方法。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lda = LDA(k=30, seed=long(time.time()), optimizer=&quot;em&quot;)</span><br><span class="line">model = lda.fit(feature_dataframe)</span><br></pre></td></tr></table></figure></p>
<p>怎么可能！！！ ml 怎么可能没有fit？？？<br>带着满腔疑惑，打开了api，然后发现，不是ml 没有fit，是1.6 的ml 库中压根就没有LDA！！！<br>哦对，顺便看了眼隔壁Scala 的API 两个都有。。。<br><del>心情复杂</del><br>看来只能将ml 的程序改成mllib了<del>毕竟我当年也改过，只不过是java 的而已</del><br>说干就干，首先将fit 改为train，然后将输入从dataframe 改为RDD 即可。因为构建features的时候，对方已经将df 转为了rdd，只需要将对方从rdd转为df 的那一步去掉就行了。<del>简直完美</del><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">features = data.rdd.map(lambda x: (x[0], x[1:])).groupByKey().mapValues(list).map(lambda row: build_sparse_vector(row, tag_index_map)).cache()</span><br><span class="line">model = LDA.train(rdd, k=10, seed=1)</span><br></pre></td></tr></table></figure></p>
<p>然后，不出意外的报错了，类型不匹配。<br>怎么会类型不匹配呢？一定是我对LDA 的算法理解不够深入。干脆找个简单的例子来测试下吧。<br>于是照着example 撸了下面的代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">conf = SparkConf().setMaster(&apos;local&apos;).setAppName(&apos;marketing_data_lda_clustering&apos;) </span><br><span class="line">sc = SparkContext(conf = conf)	</span><br><span class="line">sqlContext = SQLContext(sc)</span><br><span class="line">data = [</span><br><span class="line">     [1, Vectors.dense([0.0, 1.0])],</span><br><span class="line">     [2, SparseVector(2, &#123;0: 1.0&#125;)],</span><br><span class="line">]</span><br><span class="line">rdd =  sc.parallelize(data)</span><br><span class="line">model = LDA.train(rdd, k=2, seed=1)</span><br><span class="line">print(model.vocabSize())</span><br><span class="line"></span><br><span class="line">print(model.describeTopics())</span><br><span class="line">print(model.describeTopics(1))</span><br></pre></td></tr></table></figure></p>
<p>结果一次通过，正确无比。那么问题究竟出在哪里呢？ 算法的各项参数差异不大，应该不会出问题。有问题的就只能是rdd了。python 新学，不会debug，只能祭出print 大法了<del>顺便吐槽下，能让服务器版本2.7  driver python 版本2.6 的 也是醉了</del><br>将测试用的rdd collect 然后打印出来的结果如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[1, DenseVector([0.0, 1.0])], [2, SparseVector(2, &#123;0: 1.0&#125;)]]</span><br></pre></td></tr></table></figure></p>
<p>有问题的代码打出来是<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(u&apos;D5A55D4E4E8F484FB4ACFCCAE1E37BFE&apos;, SparseVector(10, &#123;1: 1.0, 7: 1.0&#125;)), (u&apos;41AF98FE050658F50F0DD55F823CE954&apos;, SparseVector(10, &#123;2: 1.0, 4: 1.0&#125;)), (u&apos;4CD09883D04D35697666115ADE9393DC&apos;, SparseVector(10, &#123;5: 1.0, 6: 1.0, 8: 1.0, 9: 1.0&#125;)), (u&apos;CF5F565193E6FE9B5D7D50F3DADCB85F&apos;, SparseVector(10, &#123;3: 1.0&#125;)), (u&apos;E776CC8BAF4900C092D9789077EF1E56&apos;, SparseVector(10, &#123;0: 1.0&#125;))]</span><br></pre></td></tr></table></figure></p>
<p>这他妈坑爹呢！！！ 对方的构造方法返回居然是这个格式，无奈在features 加上如下的map 方法<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.map(lambda x: [x[0], x[1]])</span><br></pre></td></tr></table></figure></p>
<p>然而报错依赖。绝望之际，只能去读源码了。以前一直读的是scala，不知道python的源码会怎们样？<br>怀着激动的心情，我打开了spark源码。<br>机智如我，一下子就找到了python\pyspark\mllib 路径下的clustering.py文件<del>这不是废话么</del><br>找到了LDA的实现<br>关键的实现，就这么一句<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = callMLlibFunc(&quot;trainLDAModel&quot;, rdd, k, maxIterations,</span><br><span class="line">                      docConcentration, topicConcentration, seed,</span><br><span class="line">                      checkpointInterval, optimizer)</span><br></pre></td></tr></table></figure></p>
<p>这个callMLlibFunc 方法好厉害，看了好几个方法都是调用这个。于是就追到这个方法里看了下。<br>结果<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def callJavaFunc(sc, func, *args):</span><br><span class="line">    &quot;&quot;&quot; Call Java Function &quot;&quot;&quot;</span><br><span class="line">    args = [_py2java(sc, a) for a in args]</span><br><span class="line">    return _java2py(sc, func(*args))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def callMLlibFunc(name, *args):</span><br><span class="line">    &quot;&quot;&quot; Call API in PythonMLLibAPI &quot;&quot;&quot;</span><br><span class="line">    sc = SparkContext.getOrCreate()</span><br><span class="line">    api = getattr(sc._jvm.PythonMLLibAPI(), name)</span><br><span class="line">    return callJavaFunc(sc, api, *args)</span><br></pre></td></tr></table></figure></p>
<p>他喵的，原来pyspark mllib 的底层实现是调了jar 。真是简洁的设计呢。</p>
<p>原来python并不是实现。。正当绝望之时，想起当时调试LR，被labeledpoint 支配的恐惧。难道id 输入不能是string？和graphx 一样都必须转化为long？python既然依赖于scala 的源码，那查scala 的源码岂不是一样？<br>于是查啊查，查到LDA 的scala 执行<br>在org.apache.spark.mllib.clustering.LDA.scala  中查到有个run<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def run(documents: JavaPairRDD[java.lang.Long, Vector]): LDAModel = &#123;</span><br><span class="line">  run(documents.rdd.asInstanceOf[RDD[(Long, Vector)]])</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>好了很明确了，就是要转成long，那df 为什么不用转呢？</p>
<p>又开始查ml的源码</p>
<p> 在fit 方法中，有这么一段<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val oldData = LDA.getOldDataset(dataset, $(featuresCol))</span><br></pre></td></tr></table></figure></p>
<p>我能吐槽么。。。 老版本的能用的，直接加个前缀。。。。。。<br>然后再跟到vgetOldDataset<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">/** Get dataset for spark.mllib LDA */</span><br><span class="line">private[clustering] def getOldDataset(</span><br><span class="line">     dataset: Dataset[_],</span><br><span class="line">     featuresCol: String): RDD[(Long, OldVector)] = &#123;</span><br><span class="line">  dataset</span><br><span class="line">    .withColumn(&quot;docId&quot;, monotonically_increasing_id())</span><br><span class="line">    .select(&quot;docId&quot;, featuresCol)</span><br><span class="line">    .rdd</span><br><span class="line">    .map &#123; case Row(docId: Long, features: Vector) =&gt;</span><br><span class="line">      (docId, OldVectors.fromML(features))</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>连带着注释一起贴了，说好的ml 是对df 有了优化的呢，他喵的，这不是 还是转成mllib 实现了么！！！<br>所以，问题其实已经解决了，就是这一句<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">withColumn(&quot;docId&quot;, monotonically_increasing_id())</span><br></pre></td></tr></table></figure></p>
<p>生成了一个long 类型的id。<br>ok 完美落幕，再一次愉快的submit。<br>然后 WTF！！！！！  又他娘的报错。<br>从describeTopics的结果生成df 的时候发错，然后发现。<br>RDD  describeTopics的结果是这样的<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[([7, 8, 1, 0, 4, 3, 2, 6, 9, 5], [0.10368743342260674, 0.10207234154290727, 0.10138905514695376, 0.10061123338885637, 0.09970383057872025, 0.09948034467474759, 0.09945100732346705, 0.09850959571098065, 0.09837913555700037, 0.09671602265375995]), ([5, 9, 6, 2, 3, 4, 0, 1, 8, 7], [0.10328397706229461, 0.10162086430285339, 0.1014904041601532, 0.10054899262906491, 0.100519655280321, 0.10029616939567182, 0.09938876666399324, 0.09861094497314936, 0.09792765863627545, 0.09631256689622307])]</span><br></pre></td></tr></table></figure></p>
<p>DF describeTopics 的结果在前面有个编号。是ML 在调用时有入参，做了一次累加。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">def describeTopics(maxTermsPerTopic: Int)</span><br></pre></td></tr></table></figure></p>
<p>so 略作调整，通过row number 加了个字段搞定。至此 告一段路</p>

          
        
      
    </div>
    
    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/12/27/数据挖掘/" itemprop="url">
                  数据挖掘 & Spark MLlib 经验记录
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-12-27T23:06:33+08:00" content="2017-12-27">
              2017-12-27
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index">
                    <span itemprop="name">bigdata</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <pre><code>之前断断续续，用Spark MLlib 做了将近两个月的数据挖掘，记录点东西。
</code></pre><p>1.数据挖掘是有目的的，Spark只是工具<br>在数据挖掘操作前，需要明确，通过这些计算，你希望从这一堆数据中获取到什么。不然只是每个算法跑一遍，也只能证明你调用Spark API 的能力合格了。</p>
<p> 在最开始的时候，进入了一个误区，以为社区炒的火热的Spark是全知全能的。数据挖掘什么的，Spark就能搞定了。然而，Spark毕竟只是一个工具，使用工具归根结底还是人。最开始，只是想掌握Spark这种技术而已，从而忽视了数据挖掘的目的（当然，这也和团队构成有关，一个产品经理加我一个研发，总觉得和业务相关的事由产品经理去实现就行。。。 图样图森破啊 ）</p>
<p>将Spark MLlib 中的算法能跑的都跑了一遍，得到了许多自己根本不知道意义的数据，才发现，其实做了无用功。</p>
<p>2.数据可视化很重要<br>看到这个需求的时候，第一反应是找相应的工具。的确有很多数据可视化的工具，画出来的图非常炫。但是，这些工具大部分真的只是画图的工具，需要你将数据整理好输入进去，然后展现出来。然而我需要的则是能够做一些简单汇总计算的操作。于是试过了echarts 、plotyly.js 、R 等等，居然发现是Excel 用的最顺手。然而Excel 却难以处理如此庞大的数据只能进行切割。下一步是该好好学学python了，考虑直接用python on Spark 进行计算，然后对结果画图，这样应该会简单很多。</p>
<p>3.不要随便造轮子<br>用Spark用了几天，然后被zb推荐用了weka，方才恍然，TMD世界上怎么有真么方便的工具啊，还要Spark MLlib 干蛋啊。各种算法都整合好，数据处理又方便，还能出简单的图，简直完美。</p>
<p>4.知己知彼<br>接上，既然weka这么完美，那为何还要用Spark呢？归根结底，还是数据量太大，只能使用Spark。这也是Spark的优势所在。即便这个玩意，对机器配置要求高，容易OOM，GC一团乱码，还不稳定。然而做离线的机器学习处理，以上这些劣势who care？只要它够快，处理的数据够大，能出结果就行了。没有工具是万能的，只要他能满足你的需求他就是个好工具。</p>
<p>5.MLlib VS ML<br>诚然，一开始想上ML的。封装程度更高的API，简洁的Pipeline，一切都是那么美好。然而数据却不是那么美好。很多源数据的清洗没法用SparkSQL 或者 Hive进行。（好吧 我承认我很水，SQL 只是刚会用而已）于是采用了MLlib。</p>
<p>因为数据是存在Hive表里的，思路就如下：</p>
<p>SparkSQL 取数据 -&gt;RDD  （进行清洗）-&gt;MLlib （进行挖掘） -&gt;python (可视化）</p>
<p>6.讲故事很重要<br>说实话，最后计算出来的数据，其实我是看不懂的，还好zb厉害，能够给出各种合理的解释，然后按照相应的方向进行深挖。</p>

          
        
      
    </div>
    
    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/12/27/spark开发环境搭建/" itemprop="url">
                  spark开发环境搭建
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-12-27T23:03:36+08:00" content="2017-12-27">
              2017-12-27
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index">
                    <span itemprop="name">bigdata</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Spark本地安装"><a href="#Spark本地安装" class="headerlink" title="Spark本地安装"></a>Spark本地安装</h1><ul>
<li>Java 安装</li>
<li>Spark 安装</li>
<li>PySpark 安装</li>
</ul>
<h2 id="Java安装"><a href="#Java安装" class="headerlink" title="Java安装"></a>Java安装</h2><p>这一部分不多赘述，配置好Java 环境变量即可。</p>
<h2 id="Spark-安装"><a href="#Spark-安装" class="headerlink" title="Spark 安装"></a>Spark 安装</h2><p>在<a href="http://spark.apache.org/downloads.html" target="_blank" rel="external">官网</a>下载所需版本的Spark 压缩包</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3698622-0b4a4a91b763512a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>解压至对应目录，如 C:\dev\spark1.6.3<br>配置环境变量<br><img src="http://upload-images.jianshu.io/upload_images/3698622-fe96697c1ec0e859.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这时，进入cmd 命令行，可以启动。<br><img src="http://upload-images.jianshu.io/upload_images/3698622-9b047f516b942438.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="Pyspark-安装"><a href="#Pyspark-安装" class="headerlink" title="Pyspark 安装"></a>Pyspark 安装</h2><p>要求在本机已经安装好Spark。此外python 3.6 版本不兼容Spark 1.6，使用时需要注意。<br>新增环境变量：PYTHONPATH<br>值为：%SPARK_HOME%\Python;%SPARK_HOME%\python\lib\py4j-0.9-src.zip</p>
<p>同时，在python 的配置的Lib\site-packages 中新增pyspark.pth 文件，内容为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">C:\dev\spark1.6.3\python</span><br></pre></td></tr></table></figure></p>
<p>重启CMD ，输入pyspark 即可<br><img src="http://upload-images.jianshu.io/upload_images/3698622-791496f526c654d9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h1 id="开发环境搭建"><a href="#开发环境搭建" class="headerlink" title="开发环境搭建"></a>开发环境搭建</h1><h2 id="Scala"><a href="#Scala" class="headerlink" title="Scala"></a>Scala</h2><p>搭建一个maven 工程即可pom.xml 如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><span class="line">&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&quot;&gt;</span><br><span class="line">  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;</span><br><span class="line">  &lt;groupId&gt;com.ych&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;ychTestSpark4S&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;</span><br><span class="line">  &lt;inceptionYear&gt;2008&lt;/inceptionYear&gt;</span><br><span class="line">    &lt;properties&gt;</span><br><span class="line">        &lt;spark.version&gt;1.6.2&lt;/spark.version&gt;</span><br><span class="line">        &lt;scala.version&gt;2.10&lt;/scala.version&gt;</span><br><span class="line">    &lt;/properties&gt;</span><br><span class="line"></span><br><span class="line">  &lt;repositories&gt;</span><br><span class="line">    &lt;repository&gt;</span><br><span class="line">      &lt;id&gt;scala-tools.org&lt;/id&gt;</span><br><span class="line">      &lt;name&gt;Scala-Tools Maven2 Repository&lt;/name&gt;</span><br><span class="line">      &lt;url&gt;http://scala-tools.org/repo-releases&lt;/url&gt;</span><br><span class="line">    &lt;/repository&gt;</span><br><span class="line">  &lt;/repositories&gt;</span><br><span class="line"></span><br><span class="line">  &lt;pluginRepositories&gt;</span><br><span class="line">    &lt;pluginRepository&gt;</span><br><span class="line">      &lt;id&gt;scala-tools.org&lt;/id&gt;</span><br><span class="line">      &lt;name&gt;Scala-Tools Maven2 Repository&lt;/name&gt;</span><br><span class="line">      &lt;url&gt;http://scala-tools.org/repo-releases&lt;/url&gt;</span><br><span class="line">    &lt;/pluginRepository&gt;</span><br><span class="line">  &lt;/pluginRepositories&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  &lt;dependencies&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;spark-core_$&#123;scala.version&#125;&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;spark-streaming_$&#123;scala.version&#125;&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;spark-sql_$&#123;scala.version&#125;&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;spark-hive_$&#123;scala.version&#125;&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;spark-mllib_$&#123;scala.version&#125;&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.avro&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;avro&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;1.7.7&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;junit&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;junit&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;4.4&lt;/version&gt;</span><br><span class="line">      &lt;scope&gt;test&lt;/scope&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.specs&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;specs&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;1.2.5&lt;/version&gt;</span><br><span class="line">      &lt;scope&gt;test&lt;/scope&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;com.databricks&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;spark-csv_2.10&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;1.0.3&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">  &lt;/dependencies&gt;</span><br><span class="line"></span><br><span class="line">  &lt;build&gt;</span><br><span class="line">    &lt;plugins&gt;</span><br><span class="line">      &lt;plugin&gt;</span><br><span class="line">        &lt;groupId&gt;org.scala-tools&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;maven-scala-plugin&lt;/artifactId&gt;</span><br><span class="line">        &lt;executions&gt;</span><br><span class="line">          &lt;execution&gt;</span><br><span class="line">            &lt;goals&gt;</span><br><span class="line">              &lt;goal&gt;compile&lt;/goal&gt;</span><br><span class="line">              &lt;goal&gt;testCompile&lt;/goal&gt;</span><br><span class="line">            &lt;/goals&gt;</span><br><span class="line">          &lt;/execution&gt;</span><br><span class="line">        &lt;/executions&gt;</span><br><span class="line">        &lt;configuration&gt;</span><br><span class="line">          &lt;scalaVersion&gt;$&#123;scala.version&#125;.6&lt;/scalaVersion&gt;</span><br><span class="line">          &lt;args&gt;</span><br><span class="line">            &lt;arg&gt;-target:jvm-1.5&lt;/arg&gt;</span><br><span class="line">          &lt;/args&gt;</span><br><span class="line">        &lt;/configuration&gt;</span><br><span class="line">      &lt;/plugin&gt;</span><br><span class="line">      &lt;plugin&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;maven-eclipse-plugin&lt;/artifactId&gt;</span><br><span class="line">        &lt;configuration&gt;</span><br><span class="line">          &lt;downloadSources&gt;true&lt;/downloadSources&gt;</span><br><span class="line">          &lt;buildcommands&gt;</span><br><span class="line">            &lt;buildcommand&gt;ch.epfl.lamp.sdt.core.scalabuilder&lt;/buildcommand&gt;</span><br><span class="line">          &lt;/buildcommands&gt;</span><br><span class="line">          &lt;additionalProjectnatures&gt;</span><br><span class="line">            &lt;projectnature&gt;ch.epfl.lamp.sdt.core.scalanature&lt;/projectnature&gt;</span><br><span class="line">          &lt;/additionalProjectnatures&gt;</span><br><span class="line">          &lt;classpathContainers&gt;</span><br><span class="line">            &lt;classpathContainer&gt;org.eclipse.jdt.launching.JRE_CONTAINER&lt;/classpathContainer&gt;</span><br><span class="line">            &lt;classpathContainer&gt;ch.epfl.lamp.sdt.launching.SCALA_CONTAINER&lt;/classpathContainer&gt;</span><br><span class="line">          &lt;/classpathContainers&gt;</span><br><span class="line">        &lt;/configuration&gt;</span><br><span class="line">      &lt;/plugin&gt;</span><br><span class="line">    &lt;/plugins&gt;</span><br><span class="line">  &lt;/build&gt;</span><br><span class="line">  &lt;reporting&gt;</span><br><span class="line">    &lt;plugins&gt;</span><br><span class="line">      &lt;plugin&gt;</span><br><span class="line">        &lt;groupId&gt;org.scala-tools&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;maven-scala-plugin&lt;/artifactId&gt;</span><br><span class="line">        &lt;configuration&gt;</span><br><span class="line">          &lt;scalaVersion&gt;$&#123;scala.version&#125;&lt;/scalaVersion&gt;</span><br><span class="line">        &lt;/configuration&gt;</span><br><span class="line">      &lt;/plugin&gt;</span><br><span class="line">    &lt;/plugins&gt;</span><br><span class="line">  &lt;/reporting&gt;</span><br><span class="line">&lt;/project&gt;</span><br></pre></td></tr></table></figure></p>
<h2 id="Java-开发环境"><a href="#Java-开发环境" class="headerlink" title="Java 开发环境"></a>Java 开发环境</h2><p>同Scala</p>
<h2 id="python"><a href="#python" class="headerlink" title="python"></a>python</h2><p>设定好，需要使用的python 环境即可。<br>spyder 根据anaconda 设定的python 环境，选择对应的spyder 启动即可。<br>pycharm 如下配置：<br><img src="http://upload-images.jianshu.io/upload_images/3698622-8b4c2e09d180aee1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>

          
        
      
    </div>
    
    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/12/07/note4learnspark/" itemprop="url">
                  Spark学习笔记汇总
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-12-07T23:50:59+08:00" content="2017-12-07">
              2017-12-07
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index">
                    <span itemprop="name">bigdata</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>Spark 作为目前最火的技术栈<del>或许 大概 应该 maybe 没有之一了吧</del>，看上去很厉害，实际上也很厉害。。。<br><del>去年，有个东西还准备自己造轮子解决，后来耽搁了，上周一搜，已经有大神造好了轮子</del><br>本篇，作为自己学习Spark 的一个记录，不涉及Spark 的具体介绍，主要是一些学习思路和学习资料的整理，资源都来自网络及社区，侵删。</p>
<h1 id="前置条件"><a href="#前置条件" class="headerlink" title="前置条件"></a>前置条件</h1><h2 id="编程语言"><a href="#编程语言" class="headerlink" title="编程语言"></a>编程语言</h2><p>学习使用Spark，需要有一定的基础知识，在编程语言的方面，目前支持了Scala、Java、Python、R。个人建议是Scala 或 Java。</p>
<ul>
<li><p>Scala<br>Scala 作为Spark 的开发语言，简洁、优雅、语法丰富、支持lamba表达式，能够明显的提高开发效率，唯一的问题就是代码的可读性稍差<del>刚开始用Scala 写Spark 的时候，都用笔在纸上写出各个RDD之间的转化</del> 。此外，会java的程序员一抓一大把，会Scala的略少，可能有一个学习的过程。</p>
<p>   <strong>快学Scala</strong><br>  很不错的书   <del>恩，买了到现在还跟新的一样</del><br>  PDF <a href="https://pan.baidu.com/s/1dEQiSEX" target="_blank" rel="external">下载链接</a> 密码：l9ef，请支持正版</p>
<p>   <strong>为Java程序员编写的Scala的入门教程</strong><br>非常实用的教程<del>1个小时从入门到精通</del><br>该教程仅在对语言和其编译器进行简要介绍。目的读者是那些已经具有一定编程经验，而想尝试一下Scala语言的人们。要阅读该教程，应当具有基础的面向对象编程的概念，尤其是Java语言的。<br><a href="https://www.iteblog.com/archives/1325.html" target="_blank" rel="external">传送门</a></p>
<p><strong>Scala语言规范</strong><br>Scala语言定义和一些核心库模块的参考手册，可以当工具书查。<br><a href="http://www.scala-lang.org/docu/files/Scala%E8%AF%AD%E8%A8%80%E8%A7%84%E8%8C%83.pdf" target="_blank" rel="external">传送门</a></p>
<p> <strong>菜鸟教程</strong><br><a href="http://www.runoob.com/scala/scala-tutorial.html" target="_blank" rel="external">http://www.runoob.com/scala/scala-tutorial.html</a></p>
</li>
</ul>
<ul>
<li><p>Java<br><del>这个没啥好说的</del><br>真是初学者的话，网上随便搜个<a href="http://www.runoob.com/java/java-tutorial.html" target="_blank" rel="external">教程</a>，装好JDK和IDE，先写个hello world，再了解下面向对象的知识，然后看看多线程，差不多可以边看API边用着了。剩下就是左手<a href="https://www.baidu.com/" target="_blank" rel="external">Google</a> 右手<a href="https://stackoverflow.com/" target="_blank" rel="external">Stack Overflow</a>了。<br>参考书的话，推荐<a href="https://pan.baidu.com/s/1eSIpBpC" target="_blank" rel="external">Java 核心技术</a> 和 <a href="https://pan.baidu.com/s/1i5BxQoH" target="_blank" rel="external">Thinking In Java</a>  <del>这么经典的书，你不买套正版的，好意思说自己是程序员么</del></p>
</li>
<li><p>Python<br>胶水型语言，即用即贴。各种神奇的库，机器学习、数据分析方便的一比。工业用就差那么点了。</p>
<p>  <strong>廖雪峰的python教程</strong><br>廖雪峰老师的这个<a href="https://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000" target="_blank" rel="external">教程</a>作为入门足够了。剩下的，看着<a href="http://spark.apache.org/docs/latest/api/python/index.html" target="_blank" rel="external">Spark 的官方API</a>，自己搜吧。</p>
</li>
<li><p>R<br>如果你本身就是搞数据分析出身，用R习惯了，那也不用学什么了。<br>如果你本身不会R，上面三个还不够你学么！！！ </p>
</li>
</ul>
<h2 id="算法知识"><a href="#算法知识" class="headerlink" title="算法知识"></a>算法知识</h2><ul>
<li>机器学习<br>如果你要使用 Spark MLlib 的话，你需要一定的机器学习基础，至少得知道你用的算法是什么吧。<br>周志华老师的<a href="https://pan.baidu.com/s/1mhG1oeO" target="_blank" rel="external">机器学习</a><br>斯坦福大学公开课 <a href="http://open.163.com/special/opencourse/machinelearning.html" target="_blank" rel="external">机器学习课程</a><br>Mitchell 的<a href="https://pan.baidu.com/s/1dF94n4H" target="_blank" rel="external">机器学习</a></li>
</ul>
<ul>
<li>图论<br>如果要使用Graph X 的话，需要一些图论 和 图计算的基本知识。<br><a href="https://pan.baidu.com/s/1dEFtAz3" target="_blank" rel="external">图论及其应用</a></li>
</ul>
<h2 id="SQL"><a href="#SQL" class="headerlink" title="SQL"></a>SQL</h2><p>  使用Spark SQL 的话，你需要一些SQL 的基本知识。</p>
<h1 id="入门教程"><a href="#入门教程" class="headerlink" title="入门教程"></a>入门教程</h1><p>Spark 的教程方法很多，最实用的是去官网，看Quick Start。<br>其他的，网上也有很多，整理了几个个人觉得很棒的教程。</p>
<ul>
<li><p>Spark 入门实战<br><a href="http://blog.csdn.net/yirenboy/article/details/47291765" target="_blank" rel="external">Spark 入门实战系列</a> 这套教程从Spark 的生态圈介绍开始，涉及了基础的开发环境搭建，基本概念的介绍，以及Spark 组件的使用。理论结合实践，demo 代码（基于Scala）一应俱全。可谓入门的不二之选。</p>
</li>
<li><p>Spark快速大数据分析<br><a href="http://www.ituring.com.cn/book/1558" target="_blank" rel="external">Spark快速大数据分析</a>，这本书200页左右，介绍了Spark RDD 的基本操作，以及其余模块的基本使用，非常适合初学者。</p>
</li>
<li><p>IDE推荐<br><strong>首选</strong>是IDEA<br><strong>次选</strong>是Eclipse<br><del>当然，你可以用vim，没人拦着你</del><br><strong>此外</strong>还有一个方式，在已经搭建好Spark 环境的集群上，直接进入<a href="http://blog.csdn.net/yeruby/article/details/41043039" target="_blank" rel="external">Spark shell</a>，都不用编译工程，可谓方便快捷，在实验一些逻辑的时候，能提高不少效率<br><strong>还有</strong>Spark 的Master 设置为local 的时候，可以直接运行IDE看到结果，无需打包上传至服务器，使用spark-submit 的方式提交。验证代码逻辑可用。</p>
</li>
</ul>
<h1 id="进阶教程"><a href="#进阶教程" class="headerlink" title="进阶教程"></a>进阶教程</h1><ul>
<li><p>Spark 高级大数据分析<br><a href="http://www.ituring.com.cn/book/1668" target="_blank" rel="external">Spark 高级大数据分析</a>介绍了如何利用Spark进行大规模数据分析的若干模式，通过实例向讲述了怎样解决分析型问题。</p>
</li>
<li><p>Apache Spark 源码剖析<br><a href="https://www.amazon.cn/Apache-Spark%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90-%E8%AE%B8%E9%B9%8F/dp/B00U0A9L3C/ref=sr_1_6?ie=UTF8&amp;qid=1426825361&amp;sr=8-6&amp;keywords=spark" target="_blank" rel="external">Apache Spark 源码剖析</a> 以Spark 1.02版本源码为切入点，着力于探寻Spark所要解决的主要问题及其解决办法，通过一系列精心设计的小实验来分析每一步背后的处理逻辑。</p>
</li>
</ul>
<ul>
<li><p>Spark 源码走读<br>该<a href="http://www.cnblogs.com/hseagle/p/3664933.html" target="_blank" rel="external">系列</a> 从Spark Job 的提交开始，进行Spark的源码走读。<br><del>恩 他就是上面那本书的作者</del></p>
</li>
<li><p>Spark SQL 源码分析<br>盛利大神 源码阅读的<a href="http://blog.csdn.net/oopsoom/article/details/38257749" target="_blank" rel="external">笔记</a></p>
</li>
<li><p>Spark Streaming 源码解析系列<br>来自 腾讯 广点通 技术团队出品的<a href="https://github.com/lw-lin/CoolplaySpark/tree/master/Spark%20Streaming%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E7%B3%BB%E5%88%97" target="_blank" rel="external">源码解析</a></p>
</li>
<li><p>Spark MLlib 源码阅读<br><a href="http://blog.csdn.net/column/details/14894.html" target="_blank" rel="external">这个</a> 按照MLlib 中的次序，进行介绍。</p>
</li>
<li><p>RDD paper<br>Spark 诞生地 UCB 的论文<a href="http://www.ece.eng.wayne.edu/~sjiang/ECE7650-winter-16/12-spark-questions.pdf" target="_blank" rel="external">Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing</a> 系统的阐述了RDD的设计初衷和基本架构，对深入理解Spark 有很大的帮助。<br>这是一篇<a href="http://blog.sciencenet.cn/blog-425672-520947.html" target="_blank" rel="external">中文翻译</a></p>
</li>
</ul>
<ul>
<li><p>API<br>在开发的过程中，没有什么比官方文档更好的老师了。自己在<a href="http://spark.apache.org/docs/latest/index.html" target="_blank" rel="external">官网</a>找对应版本的API即可。</p>
</li>
<li><p>源码<br>在github 上有Spark 的源码，个人感觉目前用的比较多的是1.6.3 和 2.1.0 两个版本。Spark 1.6 和 2.x 设计上确有一些地方有了改动，但是总体的实现思路，还是类似的，看源码的话，根据自己当下使用的版本，开始看吧。一些设计理念可以借助上文的源码走读加深理解。</p>
</li>
</ul>
<h1 id="一些站点"><a href="#一些站点" class="headerlink" title="一些站点"></a>一些站点</h1><ul>
<li><p><a href="http://spark.csdn.net/" target="_blank" rel="external">CSDN 专栏</a></p>
</li>
<li><p><a href="https://www.iteblog.com/archives/category/spark/" target="_blank" rel="external">过往记忆-Spark专栏</a><br><del>依稀记得，当时年少，懵懂无知，网还被墙，一遇问题，只能度娘，结果搜到的解决方法，大多来自过往记忆</del></p>
</li>
</ul>
<ul>
<li><p><a href="http://jerryshao.me/" target="_blank" rel="external">邵塞塞</a><br>早期Spark contributor之一</p>
</li>
<li><p><a href="https://amplab.cs.berkeley.edu/author/mzaharia" target="_blank" rel="external">Matei Zaharia</a><br>UCB 的大神</p>
</li>
<li><p><a href="https://databricks.com/blog" target="_blank" rel="external">databricks</a></p>
</li>
</ul>

          
        
      
    </div>
    
    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/02/12/体道/" itemprop="url">
                  体道
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-02-12T21:39:16+08:00" content="2017-02-12">
              2017-02-12
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/一语/" itemprop="url" rel="index">
                    <span itemprop="name">一语</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>半年时间，断断续续，一本《老子·庄子》 跟着我从南京、南京来来回回了七八次。再加上surface中的一本《易经》、一本《唐诗选》。算来，阅读量竟如此匮乏。（好吧，如果把Spark 、Scala 这些都算上的话，其实也不少 科科）<br>然后，这次出差回来，忽然就打了鸡血，又又又又 列了个清单。乱七八糟定了些目标。找了些书，慢慢看。顺便提醒自己做个记录。</p>
<p>PS：借用河上公 的题，来给自己开个好头吧。</p>
<p>书单Re：<br>各学科领域入门书籍推荐<br><a href="http://www.guokr.com/blog/21940/" target="_blank" rel="external">http://www.guokr.com/blog/21940/</a><br>豆瓣9分以上小说，文学，历史，哲学，心理学，法学，经济学图书<br><a href="https://www.douban.com/doulist/1739529/?start=75&amp;sort=seq&amp;sub_type=" target="_blank" rel="external"> https://www.douban.com/doulist/1739529/?start=75&amp;sort=seq&amp;sub_type=</a></p>

          
        
      
    </div>
    
    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/Ruby.png"
               alt="喵十八" />
          <p class="site-author-name" itemprop="name">喵十八</p>
          <p class="site-description motion-element" itemprop="description"></p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">25</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">8</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">58</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/u/1525632730" target="_blank" title="Weibo">
                  
                    <i class="fa fa-weibo"></i>
                  
                  Weibo
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://github.com/Yao544303" target="_blank" title="GitHub">
                  
                    <i class="fa fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="" target="_blank" title="Twitter">
                  
                    <i class="fa fa-twitter"></i>
                  
                  Twitter
                </a>
              </span>
            
          
        </div>

        
        

        
        

      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">喵十八</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  



  



  
  
  

  

  

</body>
</html>
