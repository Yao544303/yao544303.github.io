<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.0.0-rc2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":true,"nav":{"gitalk":{"order":-2}},"activeClass":"gitalk"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="喵十八の小窝">
<meta property="og:url" content="http://yoursite.com/page/4/index.html">
<meta property="og:site_name" content="喵十八の小窝">
<meta property="og:locale">
<meta property="article:author" content="喵十八">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/page/4/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-Hans'
  };
</script>

  <title>喵十八の小窝</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">喵十八の小窝</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-commonweal">

    <a href="/404/" rel="section"><i class="fa fa-heartbeat fa-fw"></i>Commonweal 404</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/02/spark-discuss201806/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="喵十八">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="喵十八の小窝">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/07/02/spark-discuss201806/" class="post-title-link" itemprop="url">群内2018_6月讨论整理</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2018-07-02 23:22:01 / Modified: 23:34:48" itemprop="dateCreated datePublished" datetime="2018-07-02T23:22:01+08:00">2018-07-02</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h1><p>以下内容来自群中出现的问题，大家讨论的结果<br>Q群：432600958<br>微信群：加微信w3aboutyun,附上about云铁粉<br>部分内容整理时，已经注明出处，但很多内容，较为零碎，也无暇整理，如有不妥，请联系我，谢谢。</p>
<h1 id="两个集群间迁徙hive数据有什么方案-—by-阿黄生"><a href="#两个集群间迁徙hive数据有什么方案-—by-阿黄生" class="headerlink" title="两个集群间迁徙hive数据有什么方案 —by 阿黄生"></a>两个集群间迁徙hive数据有什么方案 —by 阿黄生</h1><h2 id="时间"><a href="#时间" class="headerlink" title="时间"></a>时间</h2><p>2018.06.01</p>
<h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><p><strong>Step1</strong>：<br>迁移hdfs数据至新集群,通过distcp实现</p>
<p><strong>Step2</strong>：<br>源集群metastore数据备份导出(mysql导出)</p>
<p><strong>Step3</strong>：<br>新的集群导入metastore数据(mysql导入)</p>
<p><strong>Step4</strong>：<br>升级hive内容库(如果hive版本需要升级操作，同版本不需要操作)</p>
<p><strong>Step5</strong>：<br>修改 metastore 内容库的集群信息（重要）</p>
<h2 id="REF"><a href="#REF" class="headerlink" title="REF"></a>REF</h2><p><a href="https://blog.csdn.net/yinansheng1/article/details/78402459">Distcp</a><br><a href="https://blog.csdn.net/levy_cui/article/details/70156682">hadoop跨集群之间迁移hive数据</a></p>
<h1 id="Scala优雅的实现"><a href="#Scala优雅的实现" class="headerlink" title="Scala优雅的实现"></a>Scala优雅的实现</h1><h2 id="时间-1"><a href="#时间-1" class="headerlink" title="时间"></a>时间</h2><p>2018.06.01</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>存在一个Array[String]  例如 Array(“A”,”B”,”C”,”D”,…..”Z”)<br>现在希望替换B为 b1,b2,b3,b4,b5，然后转为String<br>即生成如下结果</p>
<blockquote>
<p>Ab1CDEF…Z<br>Ab2CDEF…Z<br>Ab3CDEF…Z<br>Ab4CDEF…Z<br>Ab5CDEF…Z  </p>
</blockquote>
<p>怎么能写的优雅点？ </p>
<h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>生成一个可变的Array，替换array(2) 然后mkString</p>
<h1 id="创建SparkContext-两种写法"><a href="#创建SparkContext-两种写法" class="headerlink" title="创建SparkContext 两种写法"></a>创建SparkContext 两种写法</h1><h2 id="时间-2"><a href="#时间-2" class="headerlink" title="时间"></a>时间</h2><p>2018.06.05</p>
<h2 id="内容-1"><a href="#内容-1" class="headerlink" title="内容"></a>内容</h2><p>第一种写法<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val sc = new SparkContext(sparkConf)</span><br></pre></td></tr></table></figure><br>第二种写法<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val sc = SparkContext,getOrCreate(conf)</span><br></pre></td></tr></table></figure><br>两者的区别在于，第一种写法是根据SparkConf 新建一个sparkContext,这里注意一旦设置完成SparkConf，就不可被使用者修改。<br>第二种写法，是先使用现用的SparkContext,没有再创建一个。主要用于多applications共享SparkContext。<br>api解释：</p>
<blockquote>
<p>This function may be used to get or instantiate a SparkContext and register it as a singleton object. Because we can only have one active SparkContext per JVM, this is useful when applications may wish to share a SparkContext.<br>This method allows not passing a SparkConf (useful if just retrieving).<br>Note: This function cannot be used to create multiple SparkContext instances even if multiple contexts are allowed.</p>
</blockquote>
<p>在实际应用中，如果没有多applications 共享sparkContext的业务需求，两者无差别。(不过getOrCreate这种写法看着好像厉害点)</p>
<h1 id="编写应用的jar包-和-spark自带的jar包冲突"><a href="#编写应用的jar包-和-spark自带的jar包冲突" class="headerlink" title="编写应用的jar包 和 spark自带的jar包冲突"></a>编写应用的jar包 和 spark自带的jar包冲突</h1><h2 id="时间-3"><a href="#时间-3" class="headerlink" title="时间"></a>时间</h2><p>2018.06.10</p>
<h2 id="内容-2"><a href="#内容-2" class="headerlink" title="内容"></a>内容</h2><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p>使用jpmml 报错如下：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">java.lang.NoSuchMethodError: org.jpmml.model.JAXBUtil.createFilteredSource(Lorg/xml/sax/InputSource;[Lorg/xml/sax/XMLFilter;)Ljavax/xml/transform/sax/SAXSource;</span><br><span class="line">        at org.jpmml.model.filters.ImportFilter.apply(ImportFilter.java:94)</span><br><span class="line">        at org.jpmml.model.PMMLUtil.unmarshal(PMMLUtil.java:33)</span><br></pre></td></tr></table></figure></p>
<p>查看jar包，能够发现有这个类。</p>
<p>查看依赖是否有冲突<br>mvn -Dverbose dependency:tree —&gt; tree.txt</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[INFO] +- org.jpmml:pmml-evaluator:jar:1.4.1:compile</span><br><span class="line">[INFO] |  +- (org.jpmml:pmml-model:jar:1.4.1:compile - omitted for conflict with 1.2.15)</span><br><span class="line">[INFO] |  +- com.google.guava:guava:jar:24.0-jre:compile</span><br></pre></td></tr></table></figure>
<p>可以发现，使用jpmml时，使用的pmml的版本为1.4.1<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.jpmml&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;pmml-evaluator&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.4.1&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><br>但是spark mllib 中自带的jpmml的版本为1.2.15，直接使用的时候，会因为jar包依赖冲突报错</p>
<h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><p>具体思路是使用shade。</p>
<p><strong>Step1</strong><br>新建一个空的项目，将jpmml以及它的相关依赖包以shade的打包成一个独立的jar包，对应jpmml相关类的使用均从此jar包引用。<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependencies&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.jpmml&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;pmml-evaluator&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;1.4.1&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.jpmml&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;pmml-evaluator-extension&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;1.4.1&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">&lt;/dependencies&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;build&gt;</span><br><span class="line">    &lt;plugins&gt;</span><br><span class="line">        &lt;plugin&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;2.4.2&lt;/version&gt;</span><br><span class="line">            &lt;configuration&gt;</span><br><span class="line">                &lt;createDependencyReducedPom&gt;false&lt;/createDependencyReducedPom&gt;</span><br><span class="line">            &lt;/configuration&gt;</span><br><span class="line">            &lt;executions&gt;</span><br><span class="line">                &lt;execution&gt;</span><br><span class="line">                    &lt;phase&gt;package&lt;/phase&gt;</span><br><span class="line">                    &lt;goals&gt;</span><br><span class="line">                        &lt;goal&gt;shade&lt;/goal&gt;</span><br><span class="line">                    &lt;/goals&gt;</span><br><span class="line">                    &lt;configuration&gt;</span><br><span class="line">                        &lt;relocations&gt;</span><br><span class="line">                            &lt;relocation&gt;</span><br><span class="line">                                &lt;pattern&gt;org.jpmml&lt;/pattern&gt;</span><br><span class="line">                                &lt;shadedPattern&gt;my.pmml.jpmml&lt;/shadedPattern&gt;</span><br><span class="line">                            &lt;/relocation&gt;</span><br><span class="line">                            &lt;relocation&gt;</span><br><span class="line">                                &lt;pattern&gt;org.dmg&lt;/pattern&gt;</span><br><span class="line">                                &lt;shadedPattern&gt;my.pmml.dmg&lt;/shadedPattern&gt;</span><br><span class="line">                            &lt;/relocation&gt;</span><br><span class="line"></span><br><span class="line">                        &lt;/relocations&gt;</span><br><span class="line">                    &lt;/configuration&gt;</span><br><span class="line">                &lt;/execution&gt;</span><br><span class="line">            &lt;/executions&gt;</span><br><span class="line">        &lt;/plugin&gt;</span><br><span class="line">    &lt;/plugins&gt;</span><br><span class="line">&lt;/build&gt;</span><br></pre></td></tr></table></figure></p>
<ul>
<li>将org.jpmml映射为my.pmml.jpmml</li>
<li>将org.dmg映射为my.pmml.dmg<br>然后利用mvn clean install命令进行打包得到jpmml-base-1.0-SNAPSHOT.jar，创建一个属于你自己版本的jpmml包。之后将该包上传到私服maven镜像。</li>
</ul>
<p><strong>Step2</strong><br>在工程中使用自己的jpmml包<br>原始的maven 依赖<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.jpmml&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;pmml-evaluator&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.4.1&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure></p>
<p>新的maven 依赖<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;my.pmml.jpmml&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;pmml-evaluator&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure></p>
<p>原来的import<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import org.dmg.pmml.FieldName;</span><br><span class="line">import org.dmg.pmml.PMML;</span><br><span class="line">import org.jpmml.evaluator.ModelEvaluator;</span><br><span class="line">import org.jpmml.evaluator.ModelEvaluatorFactory;</span><br><span class="line">import org.jpmml.model.PMMLUtil;</span><br></pre></td></tr></table></figure></p>
<p>新的import<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import my.pmml.dmg.pmml.FieldName;</span><br><span class="line">import my.pmml.dmg.pmml.PMML;</span><br><span class="line">import my.pmml.jpmml.evaluator.*;</span><br><span class="line">import my.pmml.jpmml.model.PMMLUtil;</span><br></pre></td></tr></table></figure></p>
<h3 id="Ref"><a href="#Ref" class="headerlink" title="Ref"></a>Ref</h3><p><a href="https://github.com/jpmml/jpmml-evaluator">jpmml</a><br><a href="https://blog.csdn.net/lkforce/article/details/62429998">用dependency:tree查看maven引入jar包的传递依赖</a><br><a href="https://www.jianshu.com/p/d9fb7afa634d">java 依赖包冲突，使用maven的Shade方式解决</a></p>
<h1 id="关于图计算"><a href="#关于图计算" class="headerlink" title="关于图计算"></a>关于图计算</h1><h2 id="时间-4"><a href="#时间-4" class="headerlink" title="时间"></a>时间</h2><p>2018.06.26</p>
<h2 id="内容-3"><a href="#内容-3" class="headerlink" title="内容"></a>内容</h2><ul>
<li>spark graphx实现的算法都比较简单，复杂的一般都得自己再写</li>
<li>如果，数据量不大，java之类的都有一些开源的图计算jar包，能够支持</li>
<li>我们对关系计算，引入了neo4j数据库，大数据量就在库中查询实现，小数据量引用开源图算法包</li>
</ul>
<h1 id="一个奇怪的问题：-A-master-URL-must-be-set-in-your-configuration"><a href="#一个奇怪的问题：-A-master-URL-must-be-set-in-your-configuration" class="headerlink" title="一个奇怪的问题： A master URL must be set in your configuration"></a>一个奇怪的问题： A master URL must be set in your configuration</h1><h2 id="时间-5"><a href="#时间-5" class="headerlink" title="时间"></a>时间</h2><p>2018.06.28</p>
<h2 id="内容-4"><a href="#内容-4" class="headerlink" title="内容"></a>内容</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val sparkConf = new SparkConf().setAppName(&quot;XXXX&quot;)</span><br><span class="line">val sc = new SparkContext(sparkConf)</span><br></pre></td></tr></table></figure>
<p>这个放在main方法外面就会报异常。非常诡异。</p>
<h2 id="REF-1"><a href="#REF-1" class="headerlink" title="REF"></a>REF</h2><p><a href="https://blog.csdn.net/sinat_33761963/article/details/51723175">异常解决：A master URL must be set in your configuration</a></p>
<h1 id="资料分享"><a href="#资料分享" class="headerlink" title="资料分享"></a>资料分享</h1><h2 id="Spark-Summits介绍及如何下载相关视频资料"><a href="#Spark-Summits介绍及如何下载相关视频资料" class="headerlink" title="Spark Summits介绍及如何下载相关视频资料"></a>Spark Summits介绍及如何下载相关视频资料</h2><p><a href="https://pan.baidu.com/s/15ScF3brxRQGszmo-Jv1I6A">链接</a> 密码：kuxh</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/05/readnotes4may/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="喵十八">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="喵十八の小窝">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/06/05/readnotes4may/" class="post-title-link" itemprop="url">201805读书笔记</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2018-06-05 23:11:52 / Modified: 23:15:41" itemprop="dateCreated datePublished" datetime="2018-06-05T23:11:52+08:00">2018-06-05</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%B8%80%E8%AF%AD/" itemprop="url" rel="index"><span itemprop="name">一语</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="阿里有一帮吃货工程师，在食堂玩起了“黑科技”"><a href="#阿里有一帮吃货工程师，在食堂玩起了“黑科技”" class="headerlink" title="阿里有一帮吃货工程师，在食堂玩起了“黑科技”"></a>阿里有一帮吃货工程师，在食堂玩起了“黑科技”</h1><h2 id="类别"><a href="#类别" class="headerlink" title="类别"></a>类别</h2><p>创意</p>
<h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><p>吃饭，算价格麻烦<br>通过图像识别辨明菜品<br>计算价格（荤菜肉少算素菜价格）<br>计算热量发给钉钉 每日告警<br><a href="https://mp.weixin.qq.com/s/2UAOjlnzR5ROc27f6unasw">ref</a></p>
<h1 id="技术与商业到底啥关系？我们从业务角度聊一聊"><a href="#技术与商业到底啥关系？我们从业务角度聊一聊" class="headerlink" title="技术与商业到底啥关系？我们从业务角度聊一聊"></a>技术与商业到底啥关系？我们从业务角度聊一聊</h1><h2 id="类别-1"><a href="#类别-1" class="headerlink" title="类别"></a>类别</h2><p>视野</p>
<h2 id="内容-1"><a href="#内容-1" class="headerlink" title="内容"></a>内容</h2><p>一、技术促进繁荣生态</p>
<ul>
<li>就中国的消费领域来讲，用户众多且多元化</li>
<li>中国被称为世界工厂，我们的设计生产和制造能力有着完整、丰富、敏捷的特征。</li>
<li>除了电商生态外，整个新零售的体系是一个更大的生态，我们的用户非常多样，用户的需求也逐渐升级</li>
</ul>
<p>二、技术提升商业效率</p>
<ul>
<li>流量集中化</li>
<li>产品化</li>
<li>大数据赋能</li>
</ul>
<p>三、技术洞察商业本质</p>
<ul>
<li>用户是非常复杂的</li>
<li>面向的商家也是非常复杂的</li>
<li>我们面对数十亿计的商品，形形色色非常复杂。</li>
</ul>
<p>四、技术引领商业创新</p>
<ul>
<li>创新的智能商业体系。</li>
<li>线上线下商业打通升级。</li>
<li>创新跨界市场的发现和培育。</li>
</ul>
<p><a href="https://mp.weixin.qq.com/s/ph6xg6oxw76uezp6c6PmpQ">ref</a></p>
<h1 id="年轻人，你为什么要来阿里搞技术？"><a href="#年轻人，你为什么要来阿里搞技术？" class="headerlink" title="年轻人，你为什么要来阿里搞技术？"></a>年轻人，你为什么要来阿里搞技术？</h1><h2 id="类别-2"><a href="#类别-2" class="headerlink" title="类别"></a>类别</h2><p>鸡汤</p>
<h2 id="内容-2"><a href="#内容-2" class="headerlink" title="内容"></a>内容</h2><p>在阿里有句话，在成就别人中成就自己。<br>Stay hungry,stay foolish,but be crazy.<br>大丈夫抱经世之才，岂可空老于林泉之下    你是想卖一辈子糖水，还是想和我们一起改变世界。<br>非凡人、平凡心、非凡事  </p>
<p><a href="https://mp.weixin.qq.com/s/Kw0cTZErXEV8G0QxbE90rg">ref</a></p>
<h1 id="当我们在做数据异常分析时，我们在分析什么"><a href="#当我们在做数据异常分析时，我们在分析什么" class="headerlink" title="当我们在做数据异常分析时，我们在分析什么"></a>当我们在做数据异常分析时，我们在分析什么</h1><h2 id="类别-3"><a href="#类别-3" class="headerlink" title="类别"></a>类别</h2><p>数据挖掘</p>
<h2 id="内容-3"><a href="#内容-3" class="headerlink" title="内容"></a>内容</h2><p><a href="http://www.woshipm.com/data-analysis/999413.html">ref</a></p>
<h1 id="中国没有哲学家"><a href="#中国没有哲学家" class="headerlink" title="中国没有哲学家"></a>中国没有哲学家</h1><h2 id="类别-4"><a href="#类别-4" class="headerlink" title="类别"></a>类别</h2><p>思考</p>
<h2 id="内容-4"><a href="#内容-4" class="headerlink" title="内容"></a>内容</h2><p>按照西方对哲学家的定义，中国没有哲学家<br>中国有哲学家，也有哲学，只是，没有将这种哲学提升到一个科学的角度，更多的从伦理等经验进行总结<br>名实之辩  白马非马<br>通过修改定义的标准，你可以论证任何荒谬的命题，这是一种常见的诡辩术。 逻辑学上叫做“诉诸纯洁”，有个案例就是“没有真正的苏格兰人”<br>“你可以知道所有的语言是怎么叫这种鸟的，可是结果还是一点也不懂得它。你仅仅是知道了世界不同地区的人怎么称呼这只鸟罢了。我们还是来仔细瞧瞧它在做什么吧——那才是真正重要的”<br><a href="http://www.sohu.com/a/232129357_669860">ref</a></p>
<h1 id="诺，你的30岁"><a href="#诺，你的30岁" class="headerlink" title="诺，你的30岁"></a>诺，你的30岁</h1><h2 id="类别-5"><a href="#类别-5" class="headerlink" title="类别"></a>类别</h2><p>鸡汤</p>
<h2 id="内容-5"><a href="#内容-5" class="headerlink" title="内容"></a>内容</h2><p>多读书<br>既然无所事事亦难逃一死，何不奋斗终生  </p>
<p><a href="https://mp.weixin.qq.com/s/v2Itu0StSsD1Kya6nOCokQ">ref</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/30/ml-online/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="喵十八">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="喵十八の小窝">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/05/30/ml-online/" class="post-title-link" itemprop="url">机器学习业务与工程的区别以及模型上线方案学习</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2018-05-30 23:24:11 / Modified: 23:30:05" itemprop="dateCreated datePublished" datetime="2018-05-30T23:24:11+08:00">2018-05-30</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>机器学习业务与工程的区别以及模型上线方案学习</p>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>先声明，我是数据驱动和计算驱动派的</p>
<p>数据，即特征工程 决定了模型的上限，所有模型都是去拟合这个上限</p>
<p>计算能力，决定了模型能够拟合这个上限到什么程度</p>
<h1 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h1><p>在真实的数据科学世界里，我们会有两个极端，一个是业务，一个是工程。</p>
<p>偏向业务的数据科学被称为数据分析（Data Analysis），也就是A型数据科学。</p>
<p>偏向工程的数据科学被称为数据构建(Data Building)，也就是B型数据科学。</p>
<p>一般称之为数据科学家 和 数据工程师</p>
<h2 id="软件工程师，数据工程师，数据科学家-数据分析师的区别"><a href="#软件工程师，数据工程师，数据科学家-数据分析师的区别" class="headerlink" title="软件工程师，数据工程师，数据科学家 数据分析师的区别"></a>软件工程师，数据工程师，数据科学家 数据分析师的区别</h2><p>国外 ETL 服务商 Stitch 的 CEO Jake Stein，近日对这个话题进行了总结。他还绘制了一张工具图，来呈现他们在日常工具使用上的不同。</p>
<p><strong>软件工程师</strong></p>
<p>软件工程师干的活儿是开发应用和系统。这过程中的每一个环节，从设计、写代码、测试到检查，开发者都要参与。生成数据的产品都是他们开发的。软件工程是三个角色中最古老的一个，并且有相当成熟的方法体系和工具库。</p>
<p>工作内容包括：<br>前端、后端开发  网页应用 移动应用 操作系统开发 软件设计 </p>
<p><strong>数据工程师</strong></p>
<p>数据工程师需要开发能对数据进行整合、存储和提取的系统，并从软件工程师开发的应用和系统中获取数据。数据工程的诞生，是作为软件工程大类下的一个更细分的技能类别。据雷锋网了解，根据国外统计，40% 的数据工程师原本是软件工程师。</p>
<p>工作内容包括：<br>高级数据结构 分布式计算 并发程序设计 使用 Hadoop, Spark, Kafka, Hive 等新工具 开发ETL/数据流水线（data pipelines）</p>
<p><strong>数据科学家</strong></p>
<p>数据科学家的职责是基于数据作分析。优秀的数据科学家应该全面理解商业运作和数据之于实现高级目标的价值。</p>
<p>工作内容包括：<br>数据建模 机器学习 算法 商业智能的 dashboards</p>
<p><strong>数据分析师</strong></p>
<p>数据科学家和数据分析师的区别在于，后者不负责编程，统计建模，机器学习等。两者所使用的工具也有很大的区别。数据分析师使用的商业智能工具包括： Microsoft Excel , Tableau, SAS, SAP, Qlik 等。 就数据挖掘和数据建模而言，数据分析师大多使用BM SPSS Modeler, Rapid Miner, SAS, KNIME.；数据科学家倾向于使用R和Python。</p>
<p><strong>工具链</strong></p>
<p>从工具上来看，按由业务到工程的顺序，这个链条是：EXCEL &gt;&gt; R &gt;&gt; Python &gt;&gt; Scala</p>
<h2 id="Data-Building-的重要性"><a href="#Data-Building-的重要性" class="headerlink" title="Data Building 的重要性"></a>Data Building 的重要性</h2><p>在大数据时代，数据工程师的角色愈发地重要。也许，数据架构师的称谓更准确。和数据分析师不同，他们不太关注统计、分析技能、建模等。他们的工作重点在于数据架构、计算、数据存储、数据流等。 因此，数据工程师必须具备相当强的编程能力—包括编写数据查询程序的能力。也就是说，他们的能力必须达到开发高手的级别。 </p>
<p>数据工程师还负责数据库设计，数据仓储，建立数据湖。 这就意味着，他们必须十分熟悉现有的数据库技术和数据管理系统，比如和大数据有关的Hadoop与HBase 等。</p>
<p>同时，对模型的实现原理，也必须有所涉及。</p>
<p>此外，非功能性的基础设施问题，如数据的可扩展性、可靠性、韧性、有效性，备份等也由数据工程师来负责。</p>
<p>具体到dpi数据现在的工作，就是从特征的选择，到提取转化，到模型上线。</p>
<p>此外我们希望能够设计出一套更快速的迭代出特征的方法，方便公司其他同事，有选取一些好的特征的想法时，就能快速的实现，并进行建模测试。</p>
<h2 id="实际的工作"><a href="#实际的工作" class="headerlink" title="实际的工作"></a>实际的工作</h2><p>实际中，训练集的大小，正样本的大小也就1万多条，算上负样本，也就3万，现在单点计算能力，基本可以hold住，随便玩。但是面对预测集合呢？<br>需要注意的是，你对训练集进行的一些特征处理的手段，是要应用到训练集上的一旦使用了一些异常值处理，比如计算分位数，这些，sortby 之后再操作，开销是非常可观。</p>
<p>同时，因为分布式计算的特性，所以一些算法，在spark 训练时，效率和性能 都不如 R python 单机训练，这也需要对Spark本身进行优化</p>
<h1 id="模型上线实现方案-及-遇到的坑"><a href="#模型上线实现方案-及-遇到的坑" class="headerlink" title="模型上线实现方案 及 遇到的坑"></a>模型上线实现方案 及 遇到的坑</h1><h2 id="R-server"><a href="#R-server" class="headerlink" title="R server"></a>R server</h2><p>因为做数据分析的，大部分是使用R的，但是R这个语言，是搞统计的写的，并不能很好的工程化。</p>
<p>微软买了个R server,类似于提供了一个接口，去调用R训练好的模型。<br>R Server其实是对开源R从研究角度向工业生产角度的一种努力。微软的R其实包括了微软R Open和R Server。 ROpen是基于开源R的一个改进实现，对开源R从底层基于（英特尔数学核心函数库）提供了多线程的支持。RServer其实主要有两个最重要的功能，第一个是分布式，第二是DeployR，也就是部署。RServer能够支持分析任务运行的时候，数据能够从磁盘读取，不需要在任务开始时就把所有数据加载到内存。在这个基础之上，R Server就可以通过多台机器协同工作进行分布式的R分析任务。这个支持是通过ScaleR完成，并不是所有的开源R包都能实现分布式。DeployR主要是指开发完R脚本之后可以通过简单的Publish Service就把R发布成web服务提供服务，不需要额外的开发和运维成本。</p>
<p><strong>通过三种模式提供R Server服务</strong></p>
<p>第一种是在物理机或者虚拟机上安装R Server， 这种R Server可以提供Web结点和Compute 结点，其中Web结点用来发布模型提供Web 服务，Compute Node则主要用于计算。这两种Node是逻辑概念，可以部署在同一物理机上。多台机器可以组成R Server的服务集群，应对大量请求和Fail Over，通过DeployR发布web服务主要推荐这种方式。</p>
<p>第二种是在Azure上将HDInsight集群和R Server结合, 在Spark的基础之上提供R的分析能力，在这种模式，HDInsight提供了Edge Node作为R的开发环境，开发完成的Rscript可以直接通过rxSetComputeContext切换到 Spark Context在Spark的大数据集上分布式运行。运行结果可以通过DeployR发布在Edge Node或者其他R Server集群上。 Edge Node的发布不具备fail over的能力。</p>
<p>第三种就是SQL Server +R 的模式，这种模式可以把完成开发之后的Rscript嵌入到Sql Server的存储过程中，通过存储过程调用的方式对SQL server数据进行分析，简化了数据移动和处理的过程。在Sql Server上运行R也能支持ScaleR, 但是目前ScaleR的高性能和分布式只有企业版支持，其他版本相对较低。</p>
<p><strong>优势</strong><br>可以直接使用R模型</p>
<p><strong>缺点</strong><br>无法处理大规模数据量 R本身的局限性， 环境要求特殊</p>
<h2 id="python-sklearn"><a href="#python-sklearn" class="headerlink" title="python sklearn"></a>python sklearn</h2><p>sklearn是机器学习中一个常用的python第三方模块，里面对一些常用的机器学习方法进行了封装，在进行机器学习任务时，并不需要每个人都实现所有的算法，只需要简单的调用sklearn里的模块就可以实现大多数机器学习任务。</p>
<p><strong>优势</strong><br>包很多，资源很丰富，发展快</p>
<p><strong>缺点</strong><br>无法处理大规模的数据</p>
<h2 id="模型转化pmml"><a href="#模型转化pmml" class="headerlink" title="模型转化pmml"></a>模型转化pmml</h2><p>全称（Predictive Model Markup Language），利用XML描述和存储数据挖掘模型，是一个已经被W3C所接受的标准。MML是一种基于XML的语言，用来定义预测模型。</p>
<p>它为各个公司定义预测模型和在不同的应用程序之间共享模型提供了一种快速并且简单的方式。通过使用标准的XML解析器对PMML进行解析，应用程序能够决定模型输入和输出的数据类型，模型详细的格式，并且按照标准的数据挖掘术语来解释模型的结果。　</p>
<p>PMML提供了一个灵活机制来定义预测模型的模式，同时支持涉及多个预测模型的模型选择和模型平衡（model averaging）。对于那些需要全部学习（ensemble learning）、部分学习（partitioned learning）和分布式学习（distributed learning）的应用程序，这种语言被证明是非常有用的。另外，它使得在不同的应用程序和系统之间移动预测模型变得容易、方便。</p>
<p><strong>优势</strong><br>跨语言，跨平台</p>
<p><strong>缺点</strong></p>
<ul>
<li>当模型表述比较复杂的时候，如 随机森林，PMML会过大，加载报错</li>
<li>使用PMML 需要字段名、字段类型完全匹配</li>
<li>当特征名过多时，创建DataFrame 不可行 （通过hive 建表，然后从hive 读取的方式，但是编写schema 太过冗长，且整个过程过于冗长）</li>
<li>PMML 版本问题，spark 1.6.3 只支持4.2</li>
<li>模型必须是单纯的模型，不能包含任何特征处理，如果有特征处理，必须归入pipeline</li>
<li>不同环境下，模型的支持情况不同</li>
</ul>
<p><strong>Tips</strong></p>
<ul>
<li>想要使用PMML进行模型跨环境使用，需要满足下列条件</li>
<li>表述模型的兼容性</li>
<li>表述模型的复杂性不可过高</li>
<li>表述模型的字段名，字段类型必须保持一致，这个涉及到各环境模型训练的细节，必须在训练前先设计好</li>
</ul>
<h2 id="pyspark-amp-Rspark"><a href="#pyspark-amp-Rspark" class="headerlink" title="pyspark  &amp;  Rspark"></a>pyspark  &amp;  Rspark</h2><ul>
<li>版本的坑</li>
<li>API的坑</li>
<li>环境的坑</li>
</ul>
<h2 id="Scala-写-Spark"><a href="#Scala-写-Spark" class="headerlink" title="Scala 写 Spark"></a>Scala 写 Spark</h2><p><strong>优势</strong></p>
<ul>
<li>无缝衔接</li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li>效率低，准确度低，慢，难以收敛</li>
<li>API 中坑，没有办法存文件</li>
<li>版本的坑，新版的api 支持的功能更多。比如 去除outlier 之后的 MaxAbsScaler，多项式展开 PolynomialExpansion</li>
</ul>
<h2 id="直接写一个java类"><a href="#直接写一个java类" class="headerlink" title="直接写一个java类"></a>直接写一个java类</h2><ul>
<li>需要对模型有深入了解</li>
<li>需要编程能力较强</li>
</ul>
<h1 id="ref"><a href="#ref" class="headerlink" title="ref"></a>ref</h1><p><a href="http://tech.sina.com.cn/roll/2017-03-21/doc-ifycnpvh5201784.shtml">一张图看懂数据科学家、数据工程师和软件工程师之间的区别</a><br><a href="https://segmentfault.com/a/1190000004879349">[原]深入对比数据科学工具箱：Python和R之争[2016版]</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/30/spark-discuss201805/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="喵十八">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="喵十八の小窝">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/05/30/spark-discuss201805/" class="post-title-link" itemprop="url">群内2018_5月讨论整理</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2018-05-30 23:02:09 / Modified: 23:27:38" itemprop="dateCreated datePublished" datetime="2018-05-30T23:02:09+08:00">2018-05-30</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h1><p>以下内容来自群中出现的问题，大家讨论的结果</p>
<p>Q群：432600958</p>
<p>微信群：加微信w3aboutyun,附上about云铁粉</p>
<p>部分内容整理时，已经注明出处，但很多内容，较为零碎，也无暇整理，如有不妥，请联系我，谢谢。<br><del>五月一直在加班，整理的东西有限</del></p>
<h1 id="如何从小白快速蜕变为大佬"><a href="#如何从小白快速蜕变为大佬" class="headerlink" title="如何从小白快速蜕变为大佬"></a>如何从小白快速蜕变为大佬</h1><h2 id="时间"><a href="#时间" class="headerlink" title="时间"></a>时间</h2><p>2018.05.03</p>
<h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><p>这是一些经验，主要有两点<br>1、 培养自己的阅读习惯<br>2、 提高对新技术的敏感力<br>文章末附了很多公众号</p>
<p><a href="https://mp.weixin.qq.com/s/glPjf3S7j7uCX2_zgDr2jg">ref</a></p>
<h1 id="Spark-读取本地文件"><a href="#Spark-读取本地文件" class="headerlink" title="Spark 读取本地文件"></a>Spark 读取本地文件</h1><h2 id="时间-1"><a href="#时间-1" class="headerlink" title="时间"></a>时间</h2><p>2018.05.03</p>
<h2 id="讨论内容"><a href="#讨论内容" class="headerlink" title="讨论内容"></a>讨论内容</h2><p>Spark 作为一个分布式系统，如果读取本地文件，则要求该文件在所有节点都存在，并且目录都要求一致。</p>
<ul>
<li>解决方法1：分发到所有节点（麻烦）</li>
<li>解决方法2：将文件传到hdfs，读取hdfs路径</li>
<li>解决方法3：使用addfile方法<h2 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h2><a href="https://www.iteblog.com/archives/1704.html">Spark中函数addFile 和 addJar函数介绍</a></li>
</ul>
<h1 id="机器学习优化算法讨论-—-by-木东居士-ref"><a href="#机器学习优化算法讨论-—-by-木东居士-ref" class="headerlink" title="机器学习优化算法讨论  — by 木东居士 ref"></a>机器学习优化算法讨论  — by 木东居士 <a href="https://wx.zsxq.com/mweb/views/topicdetail/topicdetail.html?topic_id=15454141825552&amp;user_id=48512221855458">ref</a></h1><h2 id="时间-2"><a href="#时间-2" class="headerlink" title="时间"></a>时间</h2><p>2018.05.03</p>
<h2 id="讨论内容-1"><a href="#讨论内容-1" class="headerlink" title="讨论内容"></a>讨论内容</h2><p>机器学习的算法其实比运筹算法要简单，更偏重应用场景，更多的时间在做特征工程</p>
<p>机器学习算法里基本都有用到二阶特性（牛顿法要求hessian矩阵），都是一阶的（如梯度下降）</p>
<p>大规模学习来说，障碍往往在于算法的计算能力不足，而不是数据不够，所以也可以说传统额统计学习方法都不适合大规模数据处理</p>
<p>在样本量比较多的时候，线性分类方法的劣势小很多，例如可以通过手工拆分/离散化特征来模拟非线性关系。而且有个经验是，在数据量大的时候，一些看起来粗暴无脑的方法反而有令人惊奇的效果。</p>
<p>SVM 计算复杂度O(n^2)，存在两个超参数，只能通过穷举实验来求，计算时间要高于不少非线性分类器<br>随机森林 计算复杂度O(nlogn)</p>
<h1 id="资源分享"><a href="#资源分享" class="headerlink" title="资源分享"></a>资源分享</h1><h2 id="时间-3"><a href="#时间-3" class="headerlink" title="时间"></a>时间</h2><p>2018.05.08</p>
<h2 id="内容-1"><a href="#内容-1" class="headerlink" title="内容"></a>内容</h2><p><a href="https://pan.baidu.com/s/11TeYq09tY9JRz2R1pILx9g">经典算法大全</a> 密码 ce85</p>
<p><a href="https://pan.baidu.com/share/init?surl=sjI1HVJ">算法导论中文版.pdf</a> 密码 2ygr</p>
<p><a href="https://pan.baidu.com/s/1LztUJ56WU9JjqZWNejvaoQ">python资源分享</a>  密码：ll7q</p>
<p>内容包括：《Python 2.7 Tutorial 中文版》《Python3程序开发指南(第二版)》《Python高级编程》《python核心笔记》《python核心编程第二版笔记》《Python技术手册（第2版）》《Python源码剖析》《quantsp研究计划书》《笨办法学Python》<br>by 小青年 </p>
<h1 id="工作感悟"><a href="#工作感悟" class="headerlink" title="工作感悟"></a>工作感悟</h1><h2 id="时间-4"><a href="#时间-4" class="headerlink" title="时间"></a>时间</h2><p>2018.05.12</p>
<h2 id="内容-2"><a href="#内容-2" class="headerlink" title="内容"></a>内容</h2><h3 id="工作的意义"><a href="#工作的意义" class="headerlink" title="工作的意义"></a>工作的意义</h3><p>最近在思考一个问题，工作的意义，一份工作最终目的究竟不该是一份简单的工资，而是这个公司能赋予你的资源，另一个是这份资源的成果转换对于市场的冲击力，对人，对社会的贡献度，人终究是需要处理社会活动的。</p>
<p>职业背景和职业活动的限制，换句话说，工作本身是用来解决社会问题的，个人可能觉得只是天天码代码，做工作量，但它的成效简直太小了，一眼可以看到他的发展和未来。但如果说直面社会问题，去为解决问题而去选择走向，我个人理解人是会不去换不同的工作和行业，最终形成一套解决方案的，另一个角度，如果是为了盈利，这个解决方案的出发点一开始就是针对购买力的问题，你做的是服务和解决方案的路子。触手伸的越广，收益越大。</p>
<p>即使是办企业也一样，仔细去看每个企业的基本描述类似，但核心针对点不一样，不管是个人也好，企业也罢，需要求同存异，找出自己的突破点，这个时代不是靠努力，而是靠挖掘力，执行力。</p>
<p> by 道友 枫柚master </p>
<h1 id="Spark-persist-DISK-ONLY-产生的问题"><a href="#Spark-persist-DISK-ONLY-产生的问题" class="headerlink" title="Spark persist(DISK_ONLY)产生的问题"></a>Spark persist(DISK_ONLY)产生的问题</h1><h2 id="时间-5"><a href="#时间-5" class="headerlink" title="时间"></a>时间</h2><p>2018.05.30</p>
<h2 id="内容-3"><a href="#内容-3" class="headerlink" title="内容"></a>内容</h2><p>在Spark 的代码中增加persist(DISK_ONLY) 之后，会出现多个task failed,错误原因为container memoryOverhead</p>
<p>去掉persist(DISK_ONLY) 就不再报这个错误，没有task failed, 并且运行速度提升。<br>此外，对比每个stage的IO,发现DISK_ONLY的有部分stage的IO是去掉persist的两倍。</p>
<p><strong>猜想原因</strong></p>
<p>DISK_ONLY 是将不做序列化的对象直接存入DISK,这部分产生大量IO,会占用内存。</p>
<p><a href="https://blog.csdn.net/qq_20641565/article/details/76216417">Spark中cache和persist的作用以及存储级别</a></p>
<h1 id="Spark-代码优化"><a href="#Spark-代码优化" class="headerlink" title="Spark 代码优化"></a>Spark 代码优化</h1><ul>
<li>使用更高效的数据结构  BitSet、OpenHashSet、OpenHashMap</li>
<li>inline</li>
<li>一些汉字字符串，做维表映射之后，变成数字编号</li>
</ul>
<p><a href="https://www.jianshu.com/p/de8bb509b6f5">Spark 高效数据结构</a></p>
<h1 id="人工智能主要三块-大数据、NLP、CV"><a href="#人工智能主要三块-大数据、NLP、CV" class="headerlink" title="人工智能主要三块  大数据、NLP、CV"></a>人工智能主要三块  大数据、NLP、CV</h1><h1 id="分布式发号器选择方案"><a href="#分布式发号器选择方案" class="headerlink" title="分布式发号器选择方案"></a>分布式发号器选择方案</h1><ul>
<li>UUID</li>
<li>DB</li>
<li>SnowFlake</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/01/spark-discuss20180402/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="喵十八">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="喵十八の小窝">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/05/01/spark-discuss20180402/" class="post-title-link" itemprop="url">群内2018_４月讨论整理2</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2018-05-01 23:04:27 / Modified: 23:12:48" itemprop="dateCreated datePublished" datetime="2018-05-01T23:04:27+08:00">2018-05-01</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h1><p>以下内容来自群中出现的问题，大家讨论的结果<br>Q群：432600958<br>微信群：加微信w3aboutyun,附上about云铁粉</p>
<ul>
<li>部分内容整理时，已经注明出处，但很多内容，较为零碎，也无暇整理，如有不妥，请联系我，谢谢。</li>
<li>这次整理开始，按照问题进行分类</li>
</ul>
<h1 id="银行存储金额-使用什么类型"><a href="#银行存储金额-使用什么类型" class="headerlink" title="银行存储金额,使用什么类型"></a>银行存储金额,使用什么类型</h1><h2 id="时间"><a href="#时间" class="headerlink" title="时间"></a>时间</h2><p>2018.04.27</p>
<h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>用Int，直接存分</p>
<h1 id="今日头条用户定向预估的实现方案-讨论"><a href="#今日头条用户定向预估的实现方案-讨论" class="headerlink" title="今日头条用户定向预估的实现方案　讨论"></a>今日头条用户定向预估的实现方案　讨论</h1><h2 id="时间-1"><a href="#时间-1" class="headerlink" title="时间"></a>时间</h2><p>2018.04.27</p>
<h2 id="说明-1"><a href="#说明-1" class="headerlink" title="说明"></a>说明</h2><p><a href="http://wsq.discuz.com/?siteid=264104844&amp;c=index&amp;a=viewthread&amp;tid=24415&amp;source=pcscan">ref</a><br>看到今日头条的广告投放平台，可以选择各种维度的用户定向条件，并根据不同的条件组合实时给出预估覆盖用户量，感觉把各种维度组合存储下来这个数据量太大了，尤其App行为定向几乎涵盖了市场上所有的App，还有商圈定向，所有维度组合得上亿条吧？有没有人知道这块的技术是如何实现的呢？1.如何存储？2.如何快速查询？ </p>
<h2 id="思路-1"><a href="#思路-1" class="headerlink" title="思路"></a>思路</h2><ul>
<li>类似kylin那样玩 </li>
<li>计算广告领域的 adx 受众定向，有些维度需要存 有些不需要  有的维度只是为了分流 给adx或dsp等</li>
<li>不是实时计算 实时计算的一般是ctr 和 反作弊 其它的都是以前计算  推荐的也是多数提前计算 kylin满足不了  单纯的dsp的qps可以到5000+ 不超 100ms访给头条  头条的qps会更大</li>
<li>应该也是给用户打上标签了。所以也可以计算很快。 类似hbase这样的数据库</li>
</ul>
<h1 id="kafka提升写入速度"><a href="#kafka提升写入速度" class="headerlink" title="kafka提升写入速度"></a>kafka提升写入速度</h1><h2 id="时间-2"><a href="#时间-2" class="headerlink" title="时间"></a>时间</h2><p>2018.04.26</p>
<h2 id="相关讨论"><a href="#相关讨论" class="headerlink" title="相关讨论"></a>相关讨论</h2><ul>
<li>加分区  我一天700亿，就是普通配置，加分区提升速度  我最大的topic 500亿一天，70个分区  qps我以前看大概单台1w多吧，非高峰期  分区计算要根据机器数和磁盘数来定 最好小于 机器数*磁盘数 磁盘大小决定你可以保存多久，有多长时间的备份</li>
</ul>
<h1 id="各大hadoop-版本比较-—by-刘明-阿黄生"><a href="#各大hadoop-版本比较-—by-刘明-阿黄生" class="headerlink" title="各大hadoop 版本比较  —by 刘明 阿黄生"></a>各大hadoop 版本比较  —by 刘明 阿黄生</h1><h2 id="时间-3"><a href="#时间-3" class="headerlink" title="时间"></a>时间</h2><p>2018.04.25</p>
<h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><p>1.原生hadoop，用脚本管理。<br>（缺点：无界面管理 优点：以后经常升级组件会比较方便）<br>2.Cloudera发行版（Cloudera’s Distribution Including Apache Hadoop，简称 CDH ） 免费<br>  Cloudera公司也提供了类似的工具：Cloudera Manager（简称 CM ）来配置、监控和管理CDH集群<br>（缺点：需要厂商支持的时候贵的一逼 优点：组件升级、性能图标方面更方面）<br>3.Hortonworks发行版（Hortonworks Data Platform，简称 HDP ） 免费<br>  Hortonworks公司的Apache Ambari项目的目的就是通过软件来配置、监控和管理Hadoop（HDP）集群，以使Hadoop的管理更加  简单。Ambari提供了一个基于它自身RESTful的api实现的直观的、简单易用的web界面。<br>（缺点：暂不明 优点：日常管理更方便）<br>总结：Cloudera、Hortonworks都是大厂发布，性能稳定，运维人员喜欢用，管理方便。原生hadoop一般适用于开发人员。</p>
<h1 id="关于数据挖掘中-模型、算法和数据的一些看法"><a href="#关于数据挖掘中-模型、算法和数据的一些看法" class="headerlink" title="关于数据挖掘中 模型、算法和数据的一些看法"></a>关于数据挖掘中 模型、算法和数据的一些看法</h1><h2 id="时间-4"><a href="#时间-4" class="headerlink" title="时间"></a>时间</h2><p>2018.04.20</p>
<h2 id="观点"><a href="#观点" class="headerlink" title="观点"></a>观点</h2><ul>
<li>一般是先看有哪些数据，再看要产出什么，算法是最后的，而且也不是最重要，顶多就是一个速度快慢的问题了。模型算法这种大家似懂非懂的，出去忽悠甲方，才好吹牛B啊。</li>
<li>底层的基础算法公式是一致的，然后去和业务做匹配，形成针对业务的模型迭代算法。比如，人口模型，用地模型什么的。</li>
<li>挖掘初期，能尽量用简单的方法处理好适当数据，产生效果就不错了。上升期，模型数据都可以进一步突破，也可以选择不同场景来做。是的，先要有东西出来，而不是说做的完美。先找合适场景，能产生数据价值，搞简单算法，先落地为主</li>
<li>分公司，像搜索的几家公司，除了全文检索的技术，后面自然语言的算法，模型太多了。</li>
<li>场景不同，除非老板给你的定位就是搞搜索，nlp，否则你开始就搞这个，几个月都弄不出来成绩就完了</li>
<li>算法模型是要有大量训练数据和测试数据进行模型验证的。要不然模型容易过拟合或者欠拟合。</li>
<li>数据量不够的话，可以用k折交叉验证来进行模型验证</li>
<li>我看过一本书忘记名字了，讲大数据的，书中一个观点挺有意思的，说大数据的本质不是数据量的大小，我的理解就是不抽样的数据，会有很多看似无用的数据，但当数据量上来了，这些没用的数据就能挖掘出实际价值</li>
</ul>
<h1 id="关于大公司的开源技术，解决方案的一些看法"><a href="#关于大公司的开源技术，解决方案的一些看法" class="headerlink" title="关于大公司的开源技术，解决方案的一些看法"></a>关于大公司的开源技术，解决方案的一些看法</h1><h2 id="时间-5"><a href="#时间-5" class="headerlink" title="时间"></a>时间</h2><p>2018.04.20_3</p>
<h2 id="观点-1"><a href="#观点-1" class="headerlink" title="观点"></a>观点</h2><ul>
<li>很多解决方案，大厂的虽然是out的，在外面也可以吃很长一段时间</li>
<li>大厂开源看思路，照搬就是作死，正好拿些小厂商做试验，如果可行，就分分钟钟拷贝出来</li>
<li>很多技术，小公司并没有机会用到比如数据上云，和数据挖掘</li>
<li>阿里主要是有盈利目标。就算懂了，也是自己小范围使用。最恶心就是阿里开源。明知道自己的东西需要一大堆外围配合，例如ssd阵列冷热分离，仍然阉割之后拿出来社区版，结果造成你上坑几率极大。</li>
</ul>
<h1 id="Spark中-hashshuffle-和-sortshuffle"><a href="#Spark中-hashshuffle-和-sortshuffle" class="headerlink" title="Spark中　hashshuffle 和 sortshuffle"></a>Spark中　hashshuffle 和 sortshuffle</h1><h2 id="时间-6"><a href="#时间-6" class="headerlink" title="时间"></a>时间</h2><p>2018.04.18</p>
<h2 id="说明-2"><a href="#说明-2" class="headerlink" title="说明"></a>说明</h2><p>1.6 以上默认Sort-Based Shuffle<br>1.2 是HashShuffle</p>
<p><strong>原始HashShuffle 机制</strong><br>思考核心点：上游数据是怎么分配给下游数据的。<br>假设有4个Map 要分成3类，也就是有3个Reducer,中间会生成4*3 个小文件</p>
<p><strong>优化后的HashShuffle 机制</strong><br>因为Hash算法会根据你的 Key 进行分类，在同一个进程中，无论是有多少过Task，都会把同样的Key放在同一个Buffer里，然后把Buffer中的数据写入以Core数量为单位的本地文件中，(一个Core只有一种类型的Key的数据)，每1个Task所在的进程中，分别写入共同进程中的3份本地文件，这里有4个Mapper Tasks，所以总共输出是 2个Cores x 3个分类文件 = 6个本地小文件。</p>
<p><strong>Sort Shuffle</strong></p>
<ul>
<li>首先每个ShuffleMapTask不会为每个Reducer单独生成一个文件，相反，Sort-based Shuffle会把Mapper中每个ShuffleMapTask所有的输出数据Data只写到一个文件中。因为每个ShuffleMapTask中的数据会被分类，所以Sort-based Shuffle使用了index文件存储具体ShuffleMapTask输出数据在同一个Data文件中是如何分类的信息！！</li>
<li>基于Sort-base的Shuffle会在Mapper中的每一个ShuffleMapTask中产生两个文件：Data文件和Index文件，其中Data文件是存储当前Task的Shuffle输出的。而index文件中则存储了Data文件中的数据通过Partitioner的分类信息，此时下一个阶段的Stage中的Task就是根据这个Index文件获取自己所要抓取的上一个Stage中的ShuffleMapTask产生的数据的，Reducer就是根据index文件来获取属于自己的数据。<br>Sorted-based Shuffle：会产生 2*M(M代表了Mapper阶段中并行的Partition的总数量，其实就是ShuffleMapTask的总数量)个Shuffle临时文件。</li>
</ul>
<h2 id="ref"><a href="#ref" class="headerlink" title="ref:"></a>ref:</h2><p><a href="https://www.cnblogs.com/jcchoiling/p/6431969.html">[Spark性能调优] 第二章：彻底解密Spark的HashShuffle</a><br><a href="http://zhou-yuefei.iteye.com/blog/2294132">Spark sort-based Shuffle内幕彻底解密</a></p>
<h1 id="一些零散的关于技术的想法："><a href="#一些零散的关于技术的想法：" class="headerlink" title="一些零散的关于技术的想法："></a>一些零散的关于技术的想法：</h1><p>技术的广度 Vs 深度<br>知识体系的力量</p>
<h2 id="时间-7"><a href="#时间-7" class="headerlink" title="时间"></a>时间</h2><p>2018.04.17</p>
<h2 id="相关讨论内容"><a href="#相关讨论内容" class="headerlink" title="相关讨论内容"></a>相关讨论内容</h2><ul>
<li>编程我的感觉就是：有基础就可以做了，但是要成为资深程序猿的话，学习能力、创新能力、思维开阔性很重要</li>
<li>自我认知的四个层次，不知道自己不知道，知道自己不知道，知道自己知道和不知道自己知道</li>
<li>系统的教育，比零散的学习，思维的深度 和强度要高不少，我是这么觉得的，受过这种教育的人，会构建自己的知识体系，然后学到新的东西，都能放到这个体系中</li>
<li>知识体系的例子：打个比方，你看到一个新的分布式技术，自然而然会想到并发，高可用，存储结构计算框架，按着一个套路去套，去学习</li>
<li>我老大是搞cdn的， 我那时候搞storm抓不住重点， 我老大听我说完 就告诉我看他的通信模型就够了， 现在想来就是他有知识体系 我没有</li>
<li>一般说30就不干程序员的，都是新创业公司，因为付不起工资其实30后能力和经验才开始体现，思考思维才逐步成熟。</li>
<li>30岁以前写代码是体力劳动，积累经验。一旦30岁开始，年龄和职业能力出现巨大偏差就有问题了。</li>
<li>昨天我和领导谈话，领导说做完项目，小组互相评价，我说好，后来我补充了一句，另外技术方面也得总结，最好这个项目能为下个项目做铺垫。后来领导问我做了这么长时间，有提高么，仔细想想，其实压力还是蛮大的，学习是一方面，但技术设限这点上确实是我没法独立去突破的，优质的程序员得到什么程度都是去自己评估，这就问题很大了。</li>
</ul>
<h1 id="面试相关"><a href="#面试相关" class="headerlink" title="面试相关"></a>面试相关</h1><h2 id="时间-8"><a href="#时间-8" class="headerlink" title="时间"></a>时间</h2><p>2018.04.17 及之后</p>
<h2 id="相关讨论内容-1"><a href="#相关讨论内容-1" class="headerlink" title="相关讨论内容"></a>相关讨论内容</h2><ul>
<li>怎么筛选人： 公司用什么？需要什么样的人？怎么去判断一个人技术是否牛逼？要人的标准是什么？</li>
<li>注重的方面：技术、团队、态度</li>
<li>面试是双向选择，并不是跪舔，打破这个规则以后工作都是不平等的。</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><h3 id="作为面试者："><a href="#作为面试者：" class="headerlink" title="作为面试者："></a>作为面试者：</h3><ul>
<li>态度要端正</li>
<li>打好基础，具备相应的实力。对于技术问题，一定要做，而且总结。</li>
<li>摆正心态，平稳应对难题，特别你的职位越高，心态越重要。遇到难题，就有点懊恼，这是不行的。</li>
<li>做好简历</li>
</ul>
<h3 id="作为面试官："><a href="#作为面试官：" class="headerlink" title="作为面试官："></a>作为面试官：</h3><ul>
<li>明确招人的需求和目的</li>
<li>通过阅读简历及提问，认清面试者的真实水平<br>这个地方，可以考虑设计一下有层次的问题。一来，逐层挖深，能更好的分析出面试者技术水平。二来，可以考验面试者思维的逻辑性和缜密性。三来，更有逻辑条理的问题，能更好的挖掘出一些深层次的问题，形成一些思维的交锋，能够让面试者感受面试官的技术水平。毕竟面试是个双向选择的过程，要是面试官太水，真的人才说不定就看不上了。</li>
<li>人品、态度考察。 这个可能hr会比技术人员更在行。技术人员从整体上给出一个把握就行。</li>
</ul>
<h3 id="一些面试问题"><a href="#一些面试问题" class="headerlink" title="一些面试问题"></a>一些面试问题</h3><h4 id="面试问题集合-—by-一杯咖啡。"><a href="#面试问题集合-—by-一杯咖啡。" class="headerlink" title="面试问题集合　—by 一杯咖啡。"></a>面试问题集合　—by 一杯咖啡。</h4><p>1、多线程的锁有哪几种<br>2、standby NN 和 secondary NN的区别？<br>3、Hadoop的HA如何避免脑裂？<br>4、讲讲HBase rowkey的设计<br>5、hbase访问热点问题<br>6、flume的数据源有哪些？<br>7、hive中数据倾斜问题？<br>8、写MR topN<br>9、spark rdd的lineage是怎么回事？<br>10、spark的宽依赖和窄依赖？（画出来）<br>11、rdd的懒加载是如何实现的？<br>12、HBase的major compaction机制，如何避免它对我们的业务的影响<br>13、oracle的分页？<br>14、窗口函数（我不知道这题问的是hive的窗口函数还是spark的窗口函数）<br>15、用SQL统计UV、PV指标<br>16、kafka是如何实现消息的副本？怎么控制N+1?<br>17、HBase查询的时候用什么对象？（这题不知道要问啥，面试官说：表操作的几个对象？？）</p>
<h4 id="从头写一个简单的分布式服务，我只要最简单的功能，work-向master发心跳，超时剔除。-—-by-王二铁"><a href="#从头写一个简单的分布式服务，我只要最简单的功能，work-向master发心跳，超时剔除。-—-by-王二铁" class="headerlink" title="从头写一个简单的分布式服务，我只要最简单的功能，work 向master发心跳，超时剔除。 — by 王二铁"></a>从头写一个简单的分布式服务，我只要最简单的功能，work 向master发心跳，超时剔除。 — by 王二铁</h4><p>参考zk 实现原理，要求对分布式架构有一定的了解</p>
<h4 id="大文件放到内存处理"><a href="#大文件放到内存处理" class="headerlink" title="大文件放到内存处理"></a>大文件放到内存处理</h4><p>有一个5000w的用户文件，一个2亿记录的用户看电影的记录文件，列出前1000个看电影最多的用户（内存只有1G) 那前1000w个呢<br>一个5000万长度的数组加一个5000万个key的HashMap，内存消耗会超过1个G吗？遍历2亿次加上5000万*log(5000wan）时间效率。这应该是最蠢的方法了。<br>所有数据肯定内存是没法容下的，先把大文件hash成许多个小文件直到内存可以处理，之后算每个小文件的top1000，然后再递归合并小文件，直到内存可以处理</p>
<h1 id="一些好的经验总结"><a href="#一些好的经验总结" class="headerlink" title="一些好的经验总结"></a>一些好的经验总结</h1><p><a href="https://mp.weixin.qq.com/s?__biz=MzA3MjQ1NDYzOQ==&amp;mid=2660384603&amp;idx=1&amp;sn=410f152d3fa02f928f43e0dd1b1ad6b8&amp;chksm=847887dbb30f0ecda1b66da265561166a31d7f71e2bafd66ce86ca9f39e4d410c7cc9553ce3a&amp;mpshare=1&amp;scene=1&amp;srcid=0422wzyGBSQvfUjbNHBlCUl7#rd">MapReduce执行过程分析</a><br><a href="https://www.cnblogs.com/hd-zg/p/5831219.html">kafka 的offset</a><br><a href="http://dongxicheng.org/mapreduce/hadoop-permission-management/">Hadoop权限管理</a><br><a href="http://www.importnew.com/27645.html">JVM 堆内存和非堆内存</a><br><a href="https://wenku.baidu.com/view/34c3529a804d2b160a4ec0b4.html?qq-pf-to=pcqq.group">YARN ResourceManager调度器的分析</a><br><a href="https://blog.csdn.net/cymvp/article/details/50781727">YARN 内存参数终极详解</a></p>
<p><a href="https://pan.baidu.com/s/1YjE34zgfV3k16I-Ye57CLA">Docker</a> 密码: 8m4a<br><a href="http://www.aboutyun.com/thread-24377-1-1.html">实战Docker到Kubernetes技术系列视频教程</a><br><a href="http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=22614">如何免费上谷歌</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/04/16/spark-discuss20180401/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="喵十八">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="喵十八の小窝">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/04/16/spark-discuss20180401/" class="post-title-link" itemprop="url">群内2018_４月讨论整理1</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2018-04-16 20:10:06 / Modified: 20:23:53" itemprop="dateCreated datePublished" datetime="2018-04-16T20:10:06+08:00">2018-04-16</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h1><p>以下内容来自群中出现的问题，大家讨论的结果<br>Q群：432600958<br>微信群：加微信w3aboutyun,附上about云铁粉</p>
<h1 id="2018-04-13-01"><a href="#2018-04-13-01" class="headerlink" title="2018.04.13_01"></a>2018.04.13_01</h1><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>面试资源题目分享</p>
<h2 id="资料"><a href="#资料" class="headerlink" title="资料"></a>资料</h2><p><a href="https://kd.youth.cn/article/s?uid=6552956&amp;app_version=1.2.3&amp;sid=1405269&amp;time=1523783769&amp;signature=p8ebgqKwdz5B3lN0vmO4GjQD2FJ6OkK1EZXnGLQox9rJAkWR2Y&amp;sign=93411672e114e162b3f6c902425db83a">TOP 25大常见Hadoop面试题及答案</a><br><a href="https://pan.baidu.com/s/1qKmsd7bJ4PEzMsDXWgzlDg">Spark面试题汇总</a>  密码：bcpc</p>
<h1 id="2018-04-12-01"><a href="#2018-04-12-01" class="headerlink" title="2018.04.12_01"></a>2018.04.12_01</h1><h2 id="问题描述-1"><a href="#问题描述-1" class="headerlink" title="问题描述"></a>问题描述</h2><p>sklearn 训练的模型如何在spark streaming 中使用</p>
<h2 id="解决思路"><a href="#解决思路" class="headerlink" title="解决思路"></a>解决思路</h2><p>使用PMML<br>虽然spark streaming 有的模型不支持<br>可以使用 java 导入，调用java类</p>
<h1 id="2018-04-11-01"><a href="#2018-04-11-01" class="headerlink" title="2018.04.11_01"></a>2018.04.11_01</h1><h2 id="问题描述-2"><a href="#问题描述-2" class="headerlink" title="问题描述"></a>问题描述</h2><p>已经建立好hive表，利用spark-streaming 往对应的hdfs目录写数据，却无法select出来。</p>
<h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>hive建好表，对应的目录为<br>/user/hive/warehouse/testdb.db/cljwifi</p>
<p>假设文件为file1<br>/user/hive/warehouse/testdb.db/cljwifi/file1<br>这种形式，访问正常</p>
<p>spark-streaming 落盘数据，假设目录已经存在会报错，因而只能新建一个目录，比如<br>/user/hive/warehouse/testdb.db/cljwifi/dir1/file1<br>这样的情况是无法访问的</p>
<h2 id="解决思路-1"><a href="#解决思路-1" class="headerlink" title="解决思路"></a>解决思路</h2><h3 id="1-建立分区表"><a href="#1-建立分区表" class="headerlink" title="1 建立分区表"></a>1 建立分区表</h3><p>这样spark-streaming 落盘的目录，加上分区字段<br>/user/hive/warehouse/testdb.db/cljwifi/day=20180304</p>
<h3 id="2-mv数据"><a href="#2-mv数据" class="headerlink" title="2 mv数据"></a>2 mv数据</h3><p>spark-streaming 落盘结束之后，编写定时脚本，移动数据</p>
<h1 id="2018-04-10-01"><a href="#2018-04-10-01" class="headerlink" title="2018.04.10_01"></a>2018.04.10_01</h1><h2 id="问题描述-3"><a href="#问题描述-3" class="headerlink" title="问题描述"></a>问题描述</h2><p>在代码中设定了appName<br> val sparkConf = new SparkConf().setAppName(“name1”)<br>提交时用了—name参数<br>spark-submit —master yarn-cluster —name  name2<br>最终在yarn 的监控上显示的name2</p>
<h2 id="解释"><a href="#解释" class="headerlink" title="解释"></a>解释</h2><p>代码中setMaster永远生效，优先级高于—master<br>代码中的setAppName 当 yarn-client 时，优先级高于—name 当yarn-cluster 模式时，优先级低于—name<br>原因待阅读源码细查</p>
<h1 id="2018-04-10-02"><a href="#2018-04-10-02" class="headerlink" title="2018.04.10_02"></a>2018.04.10_02</h1><h2 id="问题描述-4"><a href="#问题描述-4" class="headerlink" title="问题描述"></a>问题描述</h2><p>Yarn 监控界面中的Reserve 的作用</p>
<h2 id="解释-1"><a href="#解释-1" class="headerlink" title="解释"></a>解释</h2><p>An implementation detail of this change that prevents applications from starving under this new flexibility is the notion of reserved containers. Imagine two jobs are running that each have enough tasks to saturate more than the entire cluster. One job wants each of its mappers to get 1GB, and another job wants its mappers to get 2GB. Suppose the first job starts and fills up the entire cluster. Whenever one of its task finishes, it will leave open a 1GB slot. Even though the second job deserves the space, a naive policy will give it to the first one because it’s the only job with tasks that fit. This could cause the second job to be starved indefinitely. </p>
<p>One big change in the YARN Fair Scheduler is how it defines a “resource”.<br>To prevent this unfortunate situation, when space on a node is offered to an application, if the application cannot immediately use it, it reserves it, and no other application can be allocated a container on that node until the reservation is fulfilled. Each node may have only one reserved container. The total reserved memory amount is reported in the ResourceManager UI. A high number means that it may take longer for new jobs to get space.</p>
<p>简单来说，就是一个任务在执行的时候，预计还需要一定的资源，先声明了，将这些资源抢了，其他任务就无法使用这部分资源，这些资源就是 resever</p>
<p>当一个NodeManager上的资源不足以满足当前一个Application的请求却有不得不分配给这个Application时，当前节点会为此Application预留资源，逐渐累加空余的剩余资源直至满足要求后才把资源封装成一个Container发给ApplicationMaster。如果一个Container已经被创建，并且处在剩余资源的累加过程中，它就处于上图中的RESERVED状态。当此Container已经分配给ApplicationMaster，并且此时ApplicationMaster还没发送通知说它已经得到了资源时，此Container处于ALLOCATED状态，直至ApplicationMaster发送通知给ResourceManager说它已经拿到了资源，则状态变为ACQUIRED。</p>
<p>ref:<br><a href="http://blog.cloudera.com/blog/2013/06/improvements-in-the-hadoop-yarn-fair-scheduler/">Improvements in the Hadoop YARN Fair Scheduler</a><br><a href="https://www.cnblogs.com/Scott007/p/3893318.html">Yarn中的几种状态机</a></p>
<h1 id="2018-04-04-01"><a href="#2018-04-04-01" class="headerlink" title="2018.04.04_01"></a>2018.04.04_01</h1><h2 id="问题描述-5"><a href="#问题描述-5" class="headerlink" title="问题描述"></a>问题描述</h2><p>val rdd = sc.parallelize(Array(“a”,”ab”,”abc”,”abcd”,””),2)<br>val rdd3 = rdd.aggregate(“”)((x,y)=&gt;(x.length.toString + y.length.toString),(x,y)=&gt;(x.length.toString + y.length.toString))<br>问rdd3 的结果为什么是”22”</p>
<h2 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h2><p>aggregate 之后，对两个分区分别进行seqOP<br>第一个分区 “a” “ab”<br>“”.length.toString + “a”.length.toString = “0”+”1” = “01”<br>“01”.length.toString + “ab”.length.toString = “2”+”2” = “22”</p>
<p>第二个分区 “abc” “abcd” “”<br>“”.length.toString + “abc”.length.toString = “0”+”3” = “03”<br>“04”.length.toString + “abcd”.length.toString = “2” + “4” = “24”<br>“24”.length.toString + “”.length.toString = “2” + “0” = “20”</p>
<p>然后 对 “22” “20” 进行 comOP<br>“”.length.toString + “22”.length.toString = “0” + “2” = “02”<br>“02”.length.toString + “20”.length.toString = “2” + “2” = “22”</p>
<h2 id="拓展"><a href="#拓展" class="headerlink" title="拓展"></a>拓展</h2><ul>
<li><a href="https://blog.csdn.net/u013514928/article/details/56680825">aggregate 算子</a></li>
<li><a href="https://blog.csdn.net/Dax1n/article/details/72972256">序列分区</a><br>1，2，3，4，5  分区 1 2 |3 4 5</li>
</ul>
<h1 id="2018-04-04-02"><a href="#2018-04-04-02" class="headerlink" title="2018.04.04_02"></a>2018.04.04_02</h1><h2 id="问题描述-6"><a href="#问题描述-6" class="headerlink" title="问题描述"></a>问题描述</h2><p>请教大家一个问题<br>一个目录下有100个文件，假设都是SequenceFile,但是其中有1个文件为空，大小为0(由于集群限制无法删除)<br>spark 运行时报错 not a SequenceFile<br>（文件名比较奇怪，无法在提交脚本中用正则过滤）<br>请问该如何处理？</p>
<h2 id="解决思路-2"><a href="#解决思路-2" class="headerlink" title="解决思路"></a>解决思路</h2><p>那相当于在 sc.sequenceFile<a href="inputs">LongWritable,Text</a> 之前 ，先遍历一次inputs，然后其中不为空的文件，放入一个数组input2 ，然后执行sc.sequenceFile<a href="inputs2">LongWritable,Text</a>   </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val file_rdd = sc.sequenceFile[LongWritable,Text](inputs)</span><br></pre></td></tr></table></figure>
<p>改为<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val conf = new Configuration()</span><br><span class="line">val fileSystem = FileSystem.get(conf)</span><br><span class="line">val children = fileSystem.listStatus(new Path(inputs))</span><br><span class="line">val input2 = for(path &lt;- children if (path.getLen &gt; 0)) yield &#123;path.getPath.toString&#125;</span><br><span class="line">val file_rdd = sc.sequenceFile[LongWritable,Text](input2.mkString(&quot;,&quot;))</span><br></pre></td></tr></table></figure></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/04/14/Syrian/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="喵十八">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="喵十八の小窝">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/04/14/Syrian/" class="post-title-link" itemprop="url">中国不是一百多年的中国了，世界还是一百多年前的世界</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2018-04-14 18:41:22 / Modified: 19:19:04" itemprop="dateCreated datePublished" datetime="2018-04-14T18:41:22+08:00">2018-04-14</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%B8%80%E8%AF%AD/" itemprop="url" rel="index"><span itemprop="name">一语</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>下午喝完一杯饮料的时间，被叙利亚的新闻刷频了。<br>美国发射了导弹，俄罗斯拦截了部分，剩下的都打在已经疏散了民众的区域内。<br>据报道，受伤了６个人，无人员死亡，万幸。</p>
<p>想到前几日的看到的叙利亚杜马镇“化物事件”的会议，<a href="https://www.bilibili.com/video/av21905800?from=search&amp;seid=15825215720879292194">叙利亚：这些化学物质永远只攻击女人和孩子，它都学会区分武装人员了？！</a>　叙利亚大使压抑着愤怒，在控诉着那些假装道义，却伤天害理，颠倒黑白的不公时，美国等相关国家人员，却傲慢的离场。从叙利亚大使的身上，仿佛看到了近百年前，拒绝在凡尔赛合约上签字的顾维钧先生的身影。世界还是那个世界，只是中国不再是那个中国。</p>
<p>弱国无外交，这是对的，却不应该是正确的。愿世界和平，王道于行。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/04/11/dttry1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="喵十八">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="喵十八の小窝">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/04/11/dttry1/" class="post-title-link" itemprop="url">DT近期合作爬坑记录</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2018-04-11 22:24:52 / Modified: 22:26:18" itemprop="dateCreated datePublished" datetime="2018-04-11T22:24:52+08:00">2018-04-11</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="DT代码中的坑"><a href="#DT代码中的坑" class="headerlink" title="DT代码中的坑"></a>DT代码中的坑</h1><p>连续两周时间都在支持DT以及相关的label的开发<br>能够明显的发现DT提供的代码质量非常之差。列举出来，前事不忘后事之师。</p>
<h2 id="hard-core"><a href="#hard-core" class="headerlink" title="hard core"></a>hard core</h2><p>在spark的代码中，将master 以及入参全部hard core，入参不必说他，将master 设置之后，我spark submit可是会报错的啊。</p>
<h2 id="不转为String-直接saveAsTextFile"><a href="#不转为String-直接saveAsTextFile" class="headerlink" title="不转为String 直接saveAsTextFile"></a>不转为String 直接saveAsTextFile</h2><p>常常出现<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(ABACD,ADF,1)</span><br><span class="line"></span><br><span class="line">[Ljava.long.String;@76abcd405]</span><br></pre></td></tr></table></figure><br>前者是元组直接输出，后者输出的是地址，写代码的时候一定需要注意</p>
<h2 id="集群600G-内存，输入200G-全部cache"><a href="#集群600G-内存，输入200G-全部cache" class="headerlink" title="集群600G 内存，输入200G 全部cache"></a>集群600G 内存，输入200G 全部cache</h2><p>恩 cache 的确可以提高效率，但是你这个样子做，确定不会oom？</p>
<h2 id="多次join"><a href="#多次join" class="headerlink" title="多次join"></a>多次join</h2><p>输入为 （A,B,C）<br>希望得到的输出 （A,B/sum(B),B,D）<br>做了多次join,开销非常之大<br>在实践之前，可以先进行采样，加入对A进行reduce 之后 ，按key分布的数据量不大，倾斜不严重的情况下，<br>可以将join 转变<br>map 处理为 RDD[String,Map]<br>再reduce， 得到RDD[String,Map]之后，在map内部进行相似逻辑的操作，这样能提高效率。</p>
<h2 id="不做异常检测"><a href="#不做异常检测" class="headerlink" title="不做异常检测"></a>不做异常检测</h2><p>维表可能存在空值，不做异常检测，直接进行string =》 int 的转化，必然异常。</p>
<h2 id="过滤数据"><a href="#过滤数据" class="headerlink" title="过滤数据"></a>过滤数据</h2><p>接上，对空值的过滤，需要谨慎再谨慎，每条数据都是很宝贵的，需要非常认真的对待，建议在filter之前，先sample一下，看看数据是什么样子，看看要filter的数据是什么样子，再做决断。</p>
<h2 id="sample的重要性"><a href="#sample的重要性" class="headerlink" title="sample的重要性"></a>sample的重要性</h2><p>既然用到了spark 处理的数据量级自然不会小，在大数据量测试之前务必使用小数据量进行逻辑的验证，直接用大数据量跑的话，耗时耗资源不去说，万一错了，代价也很大。</p>
<h1 id="其他非代码的坑"><a href="#其他非代码的坑" class="headerlink" title="其他非代码的坑"></a>其他非代码的坑</h1><h2 id="维表过多"><a href="#维表过多" class="headerlink" title="维表过多"></a>维表过多</h2><p>维表过多，导致管理起来非常困难，一定要协商好一个更新机制</p>
<h2 id="Spark-submit-脚本"><a href="#Spark-submit-脚本" class="headerlink" title="Spark-submit 脚本"></a>Spark-submit 脚本</h2><p>这个必须有，整理的晚了，每次提交都要重新编写，虽然时间不多，但多几次，很容易让人狂躁<br>整理了如下一个模板<br><a href="https://github.com/Yao544303/shell/blob/master/java/spark_submit.sh">spark-submit模板</a></p>
<h2 id="信息沟通必须及时"><a href="#信息沟通必须及时" class="headerlink" title="信息沟通必须及时"></a>信息沟通必须及时</h2>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/04/11/lookalike/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="喵十八">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="喵十八の小窝">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/04/11/lookalike/" class="post-title-link" itemprop="url">Lookalike 技术调研</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2018-04-11 22:20:08 / Modified: 23:28:33" itemprop="dateCreated datePublished" datetime="2018-04-11T22:20:08+08:00">2018-04-11</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/DM/" itemprop="url" rel="index"><span itemprop="name">DM</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="What"><a href="#What" class="headerlink" title="What"></a>What</h1><p>基本上所有的互联网公司都有其广告投放平台，这是给广告主投放广告的一个页面。广告主可以通过广告提交页面提交自己的广告需求，后台会给广告主圈定一部分潜在用户，这个就是我们称为Lookalike的模块。<br><strong>lookalike 不是某一种特定的算法，而是一类方法的统称，这类方法综合运用多种技术，最终达到目的。</strong></p>
<h1 id="How"><a href="#How" class="headerlink" title="How"></a>How</h1><p><strong>第一种就是显性的定位，广告主根据用户的标签直接定位</strong><br>比如说通过年龄、性别、地域这样的标签来直接圈定一部分用户进行投放。这个时候我们的技术支持就是后台的用户画像的挖掘。</p>
<p><strong>第二种做法，通过一个机器学习的模型，来定位广告主的潜在用户</strong><br>广告主提交一批客户名单，我们称之为种子用户，它作为机器学习的正样本。负样本我们会从非种子用户，或者是说平台会积累历史的一些相似的广告作为负样本，这个问题就转化为一个二分类的模型，正负样本组成学习的样本，训练模型之后，利用模型结构对活跃用户进行打分，最后得到广告主需要的目标人群。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3698622-811ff11444c330b7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="通过机器学习方法定位目标人群"></p>
<h1 id="各大厂的做法"><a href="#各大厂的做法" class="headerlink" title="各大厂的做法"></a>各大厂的做法</h1><p>对于特征和模型算法，不同的公司各有差异：特征取决于公司有哪些数据；在模型算法上，Facebook 和Google对外公布的说法就是一个预测模型，Yahoo发表过几篇论文，详细介绍过它的算法，比如LR，Linear SVM，GBDT都有尝试，论文里面提到的是GBDT的效果比较好。下图列出了不同公司的做法，供大家参考</p>
<p><strong><em>吐槽  google 和 非死不可的 predict model 怕不是用了深度学习？   私以为 两家公司 节操没这么高。</em></strong></p>
<p><img src="https://upload-images.jianshu.io/upload_images/3698622-b51da7acbcb0f90c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="各大厂做法"></p>
<h1 id="需要哪些特征"><a href="#需要哪些特征" class="headerlink" title="需要哪些特征"></a>需要哪些特征</h1><ol>
<li>行为结果数据<br>所谓行为结果数据是已经采取了具体行动的数据，例如购买数据，入资数据等。</li>
<li>行为意向数据<br>所谓行为意向数据是倾向于采取某种行为的人群数据，最典型的是搜索引擎的数据，一般来说消费者在做最终的购买决策之前，往往会通过搜索引擎了解产品周边的一些相关信息，相关搜索关键字数据可以定位到一个有强购买倾向的人。这也是很多广告主投入较多预算在SEM上的原因。但这种数据一般很难从搜索引擎侧获取，购买关键字的成本也越来越高。一般来说，通过行为意向数据来寻找人群，转化率会比较高，因为行为意向人群往往已经达到了转化前的最后一步的关键时刻，此时对意向人群进行营销，效果往往很明显。但同时广告主也面临一定的风险，因为这时客户可能已被别的竞品在更早的环节进行了影响，转化成本也相应提高。</li>
<li>行为偏好数据<br>对于大多数第三方DMP平台来说，主要还是通过这一类数据来帮助广告主找到潜在的人群，从业务逻辑来说，具有某种偏好或者属于某种类型的人群往往会更倾向于购买某款产品，对于这部分数据的学习也能促成最终的转化。而且行为偏好数据会保证广告主在潜在客群覆盖规模和精准度之间达到一个很好的平衡，因此也是广告主普遍选用的一种数据。</li>
<li>行为模式数据<br>所谓行为模式是指通过分析消费者的行为与时间、空间的关系，以及一系列行为之间的时间和空间序列关系，总结出的具有一定一致性意义的行为表现，通过这些一致性模式预测相关行为。行为模式数据往往应用于场景营销，但是由于加工行为模式的数据计算复杂度较高，同时对分析的实时性要求也很高，因此目前还处在探索和优化阶段，实际的应用落地不多。</li>
</ol>
<h1 id="应用tips"><a href="#应用tips" class="headerlink" title="应用tips"></a>应用tips</h1><ol>
<li>结合聚类算法一起使用<br>有时候客户提供过来的种子人群成分是非常复杂的，往往是参杂了大量子类人群的总和，如果直接拿这些种子人群进行lookaLike，则相当于把人群的特征进行了弱化，最终找出来的相似人群特征会变得不明显。例如某奢侈品牌，他们的一方种子人群中包含2类，一类是真正有钱的人群，平时开豪车住别墅的，另外一类是普通的城市小白领，他们往往攒好几个月的工资进行一次消费。这2种人群必须先通过聚类算法区分出来，然后再输入lookaLike算法去扩大。</li>
<li>在什么媒体上用<br>LookaLike算法选出的人群最终是在媒体的流量人群中实现触达，因此媒体自身流量对最终lookaLike算法落地的效果影响非常大，例如我们做过的某次营销案例，选取某DSP做为精准营销的落地媒体，在整个4周的营销过程中，最终选取的精准人群只有2%曝光成功。（一方面由于该DSP媒体流量均为长尾流量，而我们选取的目标人群为金融类目标人群，该DSP对目标人群覆盖率低，另外由于低价策略，竞价成功率低也导致了最终触达的精准人群规模比较小。）最终我们分析了这2%成功曝光的人群，发现他们也是Lookalike算法相似度相对较低的，也就是说最相似的那部分目标人群在该媒体上并没有出现和竞得。<br>因此为了保证lookaLike算法落地的效果，选取与广告主自身产品相对匹配的目标媒体以及合适的出价都非常重要。</li>
<li>根据效果数据优化lookaLike算法<br>一旦精准营销活动开始后，就可以回收消费者对营销的反馈数据做为正样本来对lookaLike算法进行优化。通过TalkingData对大量历史投放数据的分析，动态优化lookaike算法可以极大的提升算法的转化效果：在同样选取相似度TOP100w样本进行精准投放的情况下，每日优化样本库组相比较不优化组在一周的投放周期内，可提升激活率180％以上。样本库优化的周期可以根据效果数据回收的量级、媒体的技术支持能力、以及DMP平台自身的数据更新周期综合决定，建议每1-2日更新目标用户群。</li>
</ol>
<h1 id="一些实际例子"><a href="#一些实际例子" class="headerlink" title="一些实际例子"></a>一些实际例子</h1><h2 id="利用用户画像，给用户打标签，利用相同标签找到目标人群"><a href="#利用用户画像，给用户打标签，利用相同标签找到目标人群" class="headerlink" title="利用用户画像，给用户打标签，利用相同标签找到目标人群"></a>利用用户画像，给用户打标签，利用相同标签找到目标人群</h2><p>实例：美的豆浆机通过Youmi DSP进行了Look-alike人群扩展投放<br>有米广告取得美的家电第一方消费者数据，涵盖浏览、购买行为等ID信息。通过导入Youmi DMP进行全库记录匹配，找到个体的在线历史大数据。经由人群分析模型，有米洞察到美的用户的个性倾向特征，通过标签算法挖掘，将数据库中拥有高相似画像的人群列为一类精准用户。根据标签模型，得出这些用户具有较多且重合的“健康”“时尚”“亲子”“女性”“中高收入”“一二线城市”等细分人群画像。</p>
<p>分析：利用用户画像给用户打上各类标签。根据种子人群分析大部分种子用户具有的标签特征 例如：家庭女性、30-40岁、已婚，未生小孩，健康。那么对于一个标签为：上班族，30-40岁、已婚，未生小孩，健康 女性就是其目标人群。</p>
<h2 id="利用分类算法来实现的：种子人群为正样本，候选对象为负样本，训练分类模型，然后用模型对所有候选对象进行筛选。"><a href="#利用分类算法来实现的：种子人群为正样本，候选对象为负样本，训练分类模型，然后用模型对所有候选对象进行筛选。" class="headerlink" title="利用分类算法来实现的：种子人群为正样本，候选对象为负样本，训练分类模型，然后用模型对所有候选对象进行筛选。"></a>利用分类算法来实现的：种子人群为正样本，候选对象为负样本，训练分类模型，然后用模型对所有候选对象进行筛选。</h2><p>显然候选样本并发所有的样本都是负样本，所有这是一个典型的PU learning问题<br>PU learning：Positive and unlabeled learning</p>
<h2 id="利用社交网络进行人群扩散：利用好友关系，将种子人群标签传给社区中的好友，从而实现人群扩散"><a href="#利用社交网络进行人群扩散：利用好友关系，将种子人群标签传给社区中的好友，从而实现人群扩散" class="headerlink" title="利用社交网络进行人群扩散：利用好友关系，将种子人群标签传给社区中的好友，从而实现人群扩散"></a>利用社交网络进行人群扩散：利用好友关系，将种子人群标签传给社区中的好友，从而实现人群扩散</h2><h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><ol>
<li><p>数据准备<br>　　① 获得用户的属性（User Profile），如性别、年龄、学历、职业、地域、能力标签等；<br>　　② 根据项目内容和活动内容制定一套受众标签（Audience Label）；<br>　　③ 提取用户之间的关注关系，微博之间的转发关系；<br>　　④ 获取微博message 中的文本内容；<br>　　⑤ 获得微博message 中的图片内容。</p>
</li>
<li><p>用户标签特征处理<br>　　① 根据步骤1 中用户属性信息和已有的部分受众标签系统。利用GBDT 算法（可以直接用xgboost）将没有标签的受众全部打上标签。这个分类问题中请注意处理连续值变量以及归一化。<br>　　② 将标签进行向量化处理，这个问题转化成对中文单词进行向量化，这里用word2vec 处理后得到用户标签的向量化信息Label2vec。这一步也可以使用word2vec在中文的大数据样本下进行预训练，再用该模型对标签加以提取，对特征的提取有一定的提高，大约在0.5%左右。</p>
</li>
<li><p>文本特征处理<br>　　清洗整理步骤1 中提取到的所有微博message 文本内容，训练doc2vec 模型，得到单个文本的向量化表示，对所得的文本作聚类（KMeans，在30 万的微博用户的message 上测试，K 取128 对文本的区分度较强），最后提取每个cluster 的中心向量，并根据每个用户所占有的cluster 获得用户所发微博的文本信息的向量表示Content2vec。</p>
</li>
<li><p>图像特征<br>　　将步骤1 中提取到的所有的message 图片信息进行整理分类，使用预训练卷积网络模型（这里为了平衡效率选取VGG16 作为卷积网络）提取图像信息，对每个用户message 中的图片做向量化处理，形成Image2vec，如果有多张图片则将多张图片分别提取特征值再接一层Max Pooling 提取重要信息后输出。</p>
</li>
<li><p>社交关系建立（node2vec 向量化）<br>　　将步骤1 数据准备中获得的用户之间的关系和微博之间的转发评论关系转化成图结构，并提取用户关系sub-graph，最后使用node2vec 算法得到每个用户的社交网络图向量化表示。下图为社交关系化后的部分图示。</p>
</li>
</ol>
<p><img src="https://upload-images.jianshu.io/upload_images/3698622-fae87cac852a4e89.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="社交关系向量化"></p>
<p><strong>fully connected layers,FC</strong><br>        将步骤2345 得到的向量做拼接，经过两层FC，得到表示每个用户的多特征向量集（User Vector Set, UVS）。这里取的输出单元个数时可以根据性能和准确度做平衡，目前英特实现的是输出512 个单元，最后的特征输出表达了用户的社交关系、用户属性、发出的内容、感兴趣的内容等的混合特征向量，这些特征向量将作为下一步比对相似性的输入值。<br>　　分别计算种子用户和潜在目标用户的向量集，并比对相似性。英特使用的是余弦相似度计算相似性，将步骤6 得到的用户特征向量集作为输入x 和y，代入下面公式计算相似性。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3698622-6081cbd4c2554129.gif?imageMogr2/auto-orient/strip" alt=""><br>注意：余弦相似度更多是从方向上区分差异，而对绝对的数值不敏感，因此没法衡量每个维度值的差异。这里要在每个维度上减去一个均值或者乘以一个系数，或者在之前做好归一化。</p>
<ol>
<li>受众扩展<br>　　① 获取种子受众名单，以及目标受众的数量N；<br>　　② 检查种子用户是否存在于UVS 中，将存在的用户向量化；<br>　　③ 计算受众名单中用户和UVS 中用户的相似度，提取最相似的前N 个用户作为目标受众。</li>
</ol>
<p>　　最后将以上步骤串联起来，形成流程图。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3698622-947d0c0c9ded25e4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Lookalike 算法流程图
"></p>
<p>在以上步骤提取完特征后，英特使用一个两层的神经网络做最后的特征归并提取，算法结构示意图如下</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3698622-c59e57ca74f9f2a3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Lookalike 算法结构示意图"><br>其中FC1 层也可以替换成Max Pooling，Max Pooling 层具有强解释性，也就是把用户特征群上提取的最重要的特征点作为下一层的输入，读者可以自行尝试，这里限于篇幅问题就不做展开了。</p>
<h1 id="来自Youtube-amp-google"><a href="#来自Youtube-amp-google" class="headerlink" title="来自Youtube &amp;  google"></a>来自Youtube &amp;  google</h1><p>深度候选人生成模型 + 分布式打分模型<br>使用的是分类方式，将客户分成可能的N类，选取打分最高的类<br>引入DNN 的好处在于大多数类型的连续特征和离散特征可以直接添加到模型当中。</p>
<h1 id="ref"><a href="#ref" class="headerlink" title="ref:"></a>ref:</h1><p><a href="">Deep Neural Networks for YouTube Recommendations</a><br><a href="">Audience Expansion for Online Social Network Advertising</a><br><a href="https://zhuanlan.zhihu.com/p/25509178">微信广告推广的</a><br><a href="http://blog.csdn.net/kwame211/article/details/77568021">综述类型的</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/31/spark-discuss201803/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="喵十八">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="喵十八の小窝">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/03/31/spark-discuss201803/" class="post-title-link" itemprop="url">群内2018_3月讨论整理</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-03-31 22:28:09" itemprop="dateCreated datePublished" datetime="2018-03-31T22:28:09+08:00">2018-03-31</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2018-04-16 20:22:11" itemprop="dateModified" datetime="2018-04-16T20:22:11+08:00">2018-04-16</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h1><p>以下内容来自群中出现的问题，大家讨论的结果<br>Q群：432600958<br>微信群：加微信w3aboutyun,附上about云铁粉</p>
<h1 id="2018-03-29-01"><a href="#2018-03-29-01" class="headerlink" title="2018.03.29_01"></a>2018.03.29_01</h1><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>如何成为技术大牛</p>
<h2 id="根据阿里的分享"><a href="#根据阿里的分享" class="headerlink" title="根据阿里的分享"></a>根据阿里的分享</h2><p>do more<br>do better<br>do exercise<br><a href="http://mp.weixin.qq.com/s/t1P0mw9Hf4y27EiZB2biXw">如何成为技术大牛</a></p>
<h1 id="2018-03-29-02"><a href="#2018-03-29-02" class="headerlink" title="2018.03.29_02"></a>2018.03.29_02</h1><h2 id="问题描述-1"><a href="#问题描述-1" class="headerlink" title="问题描述"></a>问题描述</h2><p>淘宝如何保持宝贝数量的一致性</p>
<h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>淘宝宝贝数量先减去购卖数，不为0就可以同步处理，处理失败再加回来。如果宝贝数量减到一，则竞争。</p>
<h1 id="2018-03-28-01"><a href="#2018-03-28-01" class="headerlink" title="2018.03.28_01"></a>2018.03.28_01</h1><h2 id="问题描述-2"><a href="#问题描述-2" class="headerlink" title="问题描述"></a>问题描述</h2><p>worker 与 executor 线程和进程的相关理解，以及引申为spark 分区等概念</p>
<h2 id="相关思路讨论"><a href="#相关思路讨论" class="headerlink" title="相关思路讨论"></a>相关思路讨论</h2><h3 id="MR的多进程-Vs-Spark的多线程"><a href="#MR的多进程-Vs-Spark的多线程" class="headerlink" title="MR的多进程　Vs Spark的多线程"></a>MR的多进程　Vs Spark的多线程</h3><p>executor 是进程，在其中执行的task 是线程，spark 所谓的多线程，是一个executor 中可以多个task 执行。　　<br>MR 采用了多进程模型，Spark采用了多线程模式，这里指的是同一个节点上多个任务的运行模式。因为无论MR 和 Spark 整体上看，都是多进程：MR是由多个独立的Task 进程组成，Spark 应用程序的运行环境是由多个独立的Executor 进程构建的临时资源池构成的。<br>多进程模型便于细粒度控制每个任务占用的资源，但会消耗较多的启动时间，不适合运行低延迟类型的作业，这是MapReduce广为诟病的原因之一。而多线程模型则相反，该模型使得Spark很适合运行低延迟类型的作业。总之，Spark同节点上的任务以多线程的方式运行在一个JVM进程中，可带来以下好处：<br>1）任务启动速度快，与之相反的是MapReduce Task进程的慢启动速度，通常需要1s左右；<br>2）同节点上所有任务运行在一个进程中，有利于共享内存。这非常适合内存密集型任务，尤其对于那些需要加载大量词典的应用程序，可大大节省内存。<br>3）同节点上所有任务可运行在一个JVM进程(Executor)中，且Executor所占资源可连续被多批任务使用，不会在运行部分任务后释放掉，这避免了每个任务重复申请资源带来的时间开销，对于任务数目非常多的应用，可大大降低运行时间。与之对比的是MapReduce中的Task：每个Task单独申请资源，用完后马上释放，不能被其他任务重用，尽管1.0支持JVM重用在一定程度上弥补了该问题，但2.0尚未支持该功能。<br>尽管Spark的过线程模型带来了很多好处，但同样存在不足，主要有：<br>1）由于同节点上所有任务运行在一个进程中，因此，会出现严重的资源争用，难以细粒度控制每个任务占用资源。与之相反的是MapReduce，它允许用户单独为Map Task和Reduce Task设置不同的资源，进而细粒度控制任务占用资源量，有利于大作业的正常平稳运行<br>ref: <a href="http://dongxicheng.org/framework-on-yarn/apache-spark-multi-threads-model/">董西城的解释</a></p>
<h3 id="spark分区数-task数目-core数-worker节点个数-excutor数量梳理"><a href="#spark分区数-task数目-core数-worker节点个数-excutor数量梳理" class="headerlink" title="spark分区数,task数目,core数,worker节点个数,excutor数量梳理"></a>spark分区数,task数目,core数,worker节点个数,excutor数量梳理</h3><p>输入可能以多个文件的形式存储在HDFS上，每个File都包含了很多块，称为Block。<br>当Spark读取这些文件作为输入时，会根据具体数据格式对应的InputFormat进行解析，一般是将若干个Block合并成一个输入分片，称为InputSplit，注意InputSplit不能跨越文件。<br>随后将为这些输入分片生成具体的Task。InputSplit与Task是一一对应的关系。<br>随后这些具体的Task每个都会被分配到集群上的某个节点的某个Executor去执行。</p>
<ul>
<li>每个节点可以起一个或多个Executor。</li>
<li>每个Executor由若干core组成，每个Executor的每个core一次只能执行一个Task。</li>
<li>每个Task执行的结果就是生成了目标RDD的一个partiton。<br>这里的core是虚拟的core而不是机器的物理CPU核，可以理解为就是Executor的一个工作线程。</li>
</ul>
<h3 id="至于partition的数目："><a href="#至于partition的数目：" class="headerlink" title="至于partition的数目："></a>至于partition的数目：</h3><ul>
<li>对于数据读入阶段，例如sc.textFile，输入文件被划分为多少InputSplit就会需要多少初始Task。</li>
<li>在Map阶段partition数目保持不变。</li>
<li>在Reduce阶段，RDD的聚合会触发shuffle操作，聚合后的RDD的partition数目跟具体操作有关，例如repartition操作会聚合成指定分区数，还有一些算子是可配置的。</li>
</ul>
<p>ref <a href="https://www.cnblogs.com/hadoop-dev/p/6669232.html">梳理</a></p>
<h2 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h2><h3 id="一种理解："><a href="#一种理解：" class="headerlink" title="一种理解："></a>一种理解：</h3><p>executor 不是越多越好，假设你给spark 分配总的资源 为48 个vcore<br>(这个不是物理的CPU核，一个物理CPU核可以划分成多个vcore，是为了更细粒度的资源控制，<br>这样对应小任务较多情况，能提升资源利用率，打个比方，我一个CPU能分成4个vcore 和 1 个vcore，<br> 我任务1,2,3,4 其实都只要使用1个vcore 即1/4 个物理CPU，在分成4个vcore 的时候，<br> 一个CPU能并行处理4个任务，在分1个vcore时，只能串行)<br>然后96G 的内存<br>及 48c 96g<br>假设你的参数如下：<br>num-executor 8<br>executor-cores 8</p>
<p>8*8 =64 &gt; 48 了，这个时候，只会给你 48/8 =6 个executor</p>
<p>同理对于<br>num-executor    8<br>executor-menory 18<br>8*18=144 &gt; 96 了， 这个时候只会给你 96/8 =12 个executor</p>
<p>当 cores 和 menory 都超的时候，取小的，总之资源不能超总的。</p>
<p>在每个stage的时候，会根据partition的数据，划分出task的数量，一个vcore 同一时间只能处理一个task<br>假设有4个executor ，每个executor 有4个vcore，即同时处理的task数量为16 （这是你集群的处理能力）<br>那么当你的分区，只有4个的时候，即只有4个task，就意味着你的集群资源是空着的，没有利用满。（最好reparation）</p>
<p>当你的分区有32个的时候， 意味着需要 32/16 2轮能处理完。这时候，你再调大并行度，也没有用，集群资源就那么多。</p>
<p>还有关于并行度的提高，也并不是越大越好。<br>1是小文件的问题<br>2是任务太小的话，启动任务的时间占比 相对任务的处理时间占比也会很高，这样得不偿失。</p>
<h3 id="另一种思路"><a href="#另一种思路" class="headerlink" title="另一种思路"></a>另一种思路</h3><p>一般每个partition对应一个task。在我的测试过程中，如果没有设置spark.default.parallelism参数，spark计算出来的partition非常巨大，与我的cores非常不搭。我在两台机器上（8cores <em>2 +6g </em> 2）上，spark计算出来的partition达到2.8万个，也就是2.9万个tasks，每个task完成时间都是几毫秒或者零点几毫秒，执行起来非常缓慢。在我尝试设置了 spark.default.parallelism 后，任务数减少到10，执行一次计算过程从minute降到20second</p>
<p>spark.default.parallelism 的说明见 <a href="https://blog.csdn.net/bbaiggey/article/details/51984753">说明</a><br>一句话，就是触发shuffle 操作之后的默认分区数，相当于手动reparation</p>
<h1 id="2018-03-28-02"><a href="#2018-03-28-02" class="headerlink" title="2018.03.28_02"></a>2018.03.28_02</h1><h2 id="问题描述-3"><a href="#问题描述-3" class="headerlink" title="问题描述"></a>问题描述</h2><p>两个巨大的表,默认都要用reduce join. 且其中一个表中join依赖的相同key的数据大量重复.考虑到数据倾斜,这个partitioner怎么实现?</p>
<h2 id="思路-1"><a href="#思路-1" class="headerlink" title="思路"></a>思路</h2><p>这是一个半连接的问题<br>先把重复比较厉害的key 过滤了，（合并了） 再做map侧的join</p>
<p>ref：<br><a href="https://www.zhihu.com/question/39680151">如何利用spark快速计算笛卡尔积？</a><br><a href="https://www.zhihu.com/question/31979689?utm_medium=social&amp;utm_source=qq">spark千万数据join问题?</a><br><a href="https://blog.csdn.net/lzm1340458776/article/details/43017425">MapReduce表连接之半连接SemiJoin</a></p>
<h1 id="2018-03-28-03"><a href="#2018-03-28-03" class="headerlink" title="2018.03.28_03"></a>2018.03.28_03</h1><h2 id="问题描述-4"><a href="#问题描述-4" class="headerlink" title="问题描述"></a>问题描述</h2><p>spark sql 的partition 数目</p>
<h2 id="思路-2"><a href="#思路-2" class="headerlink" title="思路"></a>思路</h2><p>我是这么理解Spark 200G 内存处理2T数据的。<br>打个比方，内存为100G 有2000G 的数据，一共10000个partition，每个partition就是200M （方便起见按1G=1000M 计算）<br>假设触发shuffle的stage1，每个partition处理的约为200M数据。<br>默认的配置，一个executor 1C2G。 2G内存处理200M数据，还是没问题的。<br>这样就有100/2=50 个executor，处理10000个task 一共需要10000/50 = 200轮。</p>
<p>stage1的数据处理的结果，你可以选择cache 到内存，也可以选择到disk。这样再进行stage2的操作。<br>（假设处理结果为1000G 还是10000个partition， 这样只能到disk 了）<br>那stage2处理方式一样，从disk读数据，然后进行10000/50 = 200轮 的操作。每个executor 有2G，处理100M 还是可以的。</p>
<p>hive的分区数目和Spark 的partition 数无关<br>你可以去hdfs上查看，hive的分区目录下也有分片的文件如：(假设分区字段为时间)<br>table/20180101/part-00000<br>~<br>table/20180101/part-00010<br>那这个分区下的文件，对应Spark 中10个partition<br>然后通过spark.sql.shuffle.partitions 参数来调节执行sql中shuffle 时的task数量</p>
<h1 id="2017-03-27-01"><a href="#2017-03-27-01" class="headerlink" title="2017.03.27_01"></a>2017.03.27_01</h1><h2 id="问题描述："><a href="#问题描述：" class="headerlink" title="问题描述："></a>问题描述：</h2><p>如这样的 rdd(k,v):RDD[(String, String)]<br>需要对第一个字段 k进行归并，然后统计v去重之后出现次数最多的字符（分组 top1）。</p>
<p>例如<br>a,A<br>a,B<br>a,A<br>b,C<br>b,D<br>c,D<br>c,D  </p>
<p>结果为 (a,A) (b,C) (c,D)</p>
<h2 id="思路-3"><a href="#思路-3" class="headerlink" title="思路"></a>思路</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">rdd.map(x=&gt;(x._1 + &quot;|&quot; +  x._2, 1))</span><br><span class="line">  .reduceByKey(_ + _)</span><br><span class="line">  .map(x=&gt;(x._1.split(&quot;\\|&quot;, -1)(0), (x._1.split(&quot;\\|&quot;, -1)(1),x._2)))</span><br><span class="line">  .reduceByKey((x,y) =&gt;&#123;</span><br><span class="line">    val re = if (x._2 &gt; y._2)&#123;</span><br><span class="line">      x</span><br><span class="line">    &#125;else &#123;</span><br><span class="line">      y</span><br><span class="line">    &#125;</span><br><span class="line">    re</span><br><span class="line">  &#125;)</span><br><span class="line">  .map(x=&gt;(x._1,x._2._1))</span><br><span class="line">  </span><br></pre></td></tr></table></figure>
<h2 id="扩展-1"><a href="#扩展-1" class="headerlink" title="扩展"></a>扩展</h2><ul>
<li>topN 的问题该如何处理？ 除了groupBy 还有别的思路么？ 效率？</li>
<li>次数相同该如何处理？</li>
</ul>
<h1 id="2018-03-27-02"><a href="#2018-03-27-02" class="headerlink" title="2018.03.27_02"></a>2018.03.27_02</h1><h2 id="问题描述：-1"><a href="#问题描述：-1" class="headerlink" title="问题描述："></a>问题描述：</h2><p>如这样的 rdd(k,v):RDD[(String, String)]<br>需要对第一个字段 k进行归并，然后统计v去重之后出现的次数。</p>
<p>例如<br>a,A<br>a,B<br>a,A<br>b,C<br>b,D<br>c,D<br>c,D  </p>
<p>结果为 (a,2) (b,2) (c,1)</p>
<h2 id="目前整理的几种思路是："><a href="#目前整理的几种思路是：" class="headerlink" title="目前整理的几种思路是："></a>目前整理的几种思路是：</h2><h3 id="先进行reduceBy-将v合并为字符串，之后再拆分、去重，统计出现次数"><a href="#先进行reduceBy-将v合并为字符串，之后再拆分、去重，统计出现次数" class="headerlink" title="先进行reduceBy,将v合并为字符串，之后再拆分、去重，统计出现次数"></a>先进行reduceBy,将v合并为字符串，之后再拆分、去重，统计出现次数</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">map(a,b).reduceByKey((x1,x2)=&gt;&#123;</span><br><span class="line">val sum =x1+&quot;\t&quot;+x2</span><br><span class="line">sum&#125;).map&#123;x._2.split(&quot;\t&quot;).distinct.size&#125;</span><br></pre></td></tr></table></figure>
<h3 id="将k-v-拼接为一个字符串，去重，之后再差分，然后再进行reduceBy"><a href="#将k-v-拼接为一个字符串，去重，之后再差分，然后再进行reduceBy" class="headerlink" title="将k,v 拼接为一个字符串，去重，之后再差分，然后再进行reduceBy"></a>将k,v 拼接为一个字符串，去重，之后再差分，然后再进行reduceBy</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.map(x=&gt;(x._1 +&quot;|&quot; x._2)).distinct().map((_.split(&quot;|&quot;)(0),1)).reduceByKey(_+_)</span><br></pre></td></tr></table></figure>
<h3 id="将v-改为Set，reduce-之后再统计size"><a href="#将v-改为Set，reduce-之后再统计size" class="headerlink" title="将v 改为Set，reduce 之后再统计size"></a>将v 改为Set，reduce 之后再统计size</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.map(x=&gt;(x._1, Set(x._2)).reduceByKey(_ ++ _).map(x=&gt;(x._1, x._2.size))</span><br></pre></td></tr></table></figure>
<h3 id="使用dropDuplicates"><a href="#使用dropDuplicates" class="headerlink" title="使用dropDuplicates()"></a>使用dropDuplicates()</h3><p>需要spark 2.x ，将rdd转为DataSet 之后再进行操作。</p>
<h2 id="扩展-2"><a href="#扩展-2" class="headerlink" title="扩展"></a>扩展</h2><ul>
<li>四种思路比较？ 是否有更好的解决方案？</li>
<li>当v 也是多个字段该如何处理？</li>
<li>Set 和 String的序列化效率差异？即 String 合并之后再拆分的开销，和使用Set 序列化增长的开销 哪个比较大？</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/3/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><a class="page-number" href="/page/6/">6</a><a class="extend next" rel="next" href="/page/5/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">喵十八</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">54</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">77</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/Yao544303" title="GitHub → https://github.com/Yao544303" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:yao544303963@gmail.com" title="E-Mail → mailto:yao544303963@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">喵十八</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  


</body>
</html>
