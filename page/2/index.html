<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.0.0-rc2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":true,"nav":{"gitalk":{"order":-2}},"activeClass":"gitalk"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="喵十八の小窝">
<meta property="og:url" content="http://yoursite.com/page/2/index.html">
<meta property="og:site_name" content="喵十八の小窝">
<meta property="og:locale">
<meta property="article:author" content="喵十八">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-Hans'
  };
</script>

  <title>喵十八の小窝</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">喵十八の小窝</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-commonweal">

    <a href="/404/" rel="section"><i class="fa fa-heartbeat fa-fw"></i>Commonweal 404</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2023/08/20/fate-log-7-upload-task-execute/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="喵十八">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="喵十八の小窝">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/08/20/fate-log-7-upload-task-execute/" class="post-title-link" itemprop="url">FATE学习：跟着日志读源码（七）upload任务task excute阶段</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2023-08-20 12:12:47 / Modified: 13:36:55" itemprop="dateCreated datePublished" datetime="2023-08-20T12:12:47+08:00">2023-08-20</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/FATE/" itemprop="url" rel="index"><span itemprop="name">FATE</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h1><p>前文对upload task 进行schedule 之后，最终调用 task_executor 进行执行，进入具体的执行部分。<br>每个task的具体日志会打在${job_log_dir}/ $ {role}/ ${party} 中（为便于记录，这里简记为 ${task_log_dir}<br>这里就按照 ${task_log_dir}/DEBUG.log看 会比较清晰一点</p>
<h1 id="执行细节"><a href="#执行细节" class="headerlink" title="执行细节"></a>执行细节</h1><p><img src="/image/fate/c1ec280a31b64fc6bc0c81c8bece859e.png" alt="在这里插入图片描述"></p>
<ol>
<li>执行命令<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/opt/app-root/bin/python /data/projects/fate/python/fate_flow/operation/task_executor.py -j 202107260820309976351 -n upload_0 -t 202107260820309976351_upload_0 -v 0 -r local -p 0 -c /data/projects/fate/jobs/202107260820309976351/local/0/upload_0/202107260820309976351_upload_0/0/task_parameters.json --run_ip 10.200.96.235 --job_server 10.200.96.235:9380</span><br></pre></td></tr></table></figure></li>
<li><p>task_executor.py：执行run_task<br>先解析各参数。<br>在${job_log_dir}/fate_flow_schedule.log 输出日志</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:33,391] [90:139687957583680] - task_executor.py[line:56]: enter task process</span><br><span class="line">[INFO] [2021-07-26 08:20:33,391] [90:139687957583680] - task_executor.py[line:57]: Namespace(component_name=&#x27;upload_0&#x27;, config=&#x27;/data/projects/fate/jobs/202107260820309976351/local/0/upload_0/202107260820309976351_upload_0/0/task_parameters.json&#x27;, job_id=&#x27;202107260820309976351&#x27;, job_server=&#x27;10.200.96.235:9380&#x27;, party_id=0, role=&#x27;local&#x27;, run_ip=&#x27;10.200.96.235&#x27;, task_id=&#x27;202107260820309976351_upload_0&#x27;, task_version=0)</span><br></pre></td></tr></table></figure>
<p>根据参数解析的结果，调用 schedule_utils.get_job_dsl_parser() 生成dsl_parser，并配置各项参数。<br>设置job_log_dir 和 task_log_dir。<br><strong>注：设置完目录后，task产生的所有日志，都是输出到task_log_dir 下了，和外层的fate_flow_schedule.log 分离了</strong></p>
</li>
<li><p>task_executor.py：初始化Tracker 和 TrackerClient，获取run_class_paths、run_class_package、run_class_name，调用 report_task_update_to_driver(task_info=task_info)</p>
</li>
<li><p>task_executor.py：执行report_task_update_to_driver()<br>先在 ${task_log_dir}/fate_flow_schedule.log 输出日志</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:33,395] [90:139687957583680] - task_executor.py[line:318]: report task 202107260820309976351_upload_0 0 local 0 to driver</span><br></pre></td></tr></table></figure>
<p>然后调用ControllerClient.report_task</p>
</li>
<li>control_client.py：执行report_task<br>先在${job_log_dir}/${role}/${party}/DEBUG.log 中打日志<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:33,396] [90:139687957583680] - control_client.py[line:42]: request update job 202107260820309976351 task 202107260820309976351_upload_0 0 on local 0</span><br></pre></td></tr></table></figure>
然后发起http请求，endpoint是report。<br>流转流程同前文，调用链为 ControllerClient.report_task -&gt; fate_flow_server 通过flask -&gt; party_app.report_task() -&gt;   TaskController.update_task(task_info=task_info) -&gt; TaskController.update_task_status(task_info=task_info)</li>
</ol>
<p>在${job_log_dir}/fate_flow_audit.log 中的日志为<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:33,396] [90:139687957583680] - api_utils.py[line:122]: remote http api request: http://10.200.96.235:9380/v1/party/202107260820309976351/upload_0/202107260820309976351_upload_0/0/local/0/report</span><br><span class="line">[INFO] [2021-07-26 08:20:33,452] [90:139687957583680] - api_utils.py[line:129]: &#123;&quot;retcode&quot;:103,&quot;retmsg&quot;:&quot;update task status failed&quot;&#125;</span><br><span class="line"></span><br><span class="line">[INFO] [2021-07-26 08:20:33,452] [90:139687957583680] - api_utils.py[line:131]: remote http api response: /v1/party/202107260820309976351/upload_0/202107260820309976351_upload_0/0/local/0/report &#123;&#x27;retcode&#x27;: 103, &#x27;retmsg&#x27;: &#x27;update task status failed&#x27;&#125;</span><br></pre></td></tr></table></figure><br>因为status一致，无法update，故failed.(同五中所述)</p>
<p>在${job_log_dir}/fate_flow_schedule.log 中输出日志<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:33,404] [1:140259119585024] - job_saver.py[line:81]: try to update job 202107260820309976351 task 202107260820309976351_upload_0 0</span><br><span class="line">[INFO] [2021-07-26 08:20:33,422] [1:140259119585024] - job_saver.py[line:84]: job 202107260820309976351 task 202107260820309976351_upload_0 0 update successfully</span><br><span class="line">[INFO] [2021-07-26 08:20:33,429] [1:140259119585024] - job_saver.py[line:71]: try to update job 202107260820309976351 task 202107260820309976351_upload_0 0 status</span><br><span class="line">[INFO] [2021-07-26 08:20:33,438] [1:140259119585024] - job_saver.py[line:76]: update job 202107260820309976351 task 202107260820309976351_upload_0 0 status update does not take effect: &#123;&#x27;job_id&#x27;: &#x27;202107260820309976351&#x27;, &#x27;component_name&#x27;: &#x27;upload_0&#x27;, &#x27;task_id&#x27;: &#x27;202107260820309976351_upload_0&#x27;, &#x27;task_version&#x27;: &#x27;0&#x27;, &#x27;role&#x27;: &#x27;local&#x27;, &#x27;party_id&#x27;: &#x27;0&#x27;, &#x27;run_ip&#x27;: &#x27;10.200.96.235&#x27;, &#x27;run_pid&#x27;: 90, &#x27;party_status&#x27;: &#x27;running&#x27;&#125;</span><br></pre></td></tr></table></figure></p>
<p>在${job_log_dir}/peewee.log中输出日志为<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[DEBUG] [2021-07-26 08:20:33,409] [1:140259119585024] - peewee.py[line:2863]: (&#x27;SELECT `t1`.`f_create_time`, `t1`.`f_create_date`, `t1`.`f_update_time`, `t1`.`f_update_date`, `t1`.`f_job_id`, `t1`.`f_component_name`, `t1`.`f_task_id`, `t1`.`f_task_version`, `t1`.`f_initiator_role`, `t1`.`f_initiator_party_id`, `t1`.`f_federated_mode`, `t1`.`f_federated_status_collect_type`, `t1`.`f_status`, `t1`.`f_status_code`, `t1`.`f_role`, `t1`.`f_party_id`, `t1`.`f_run_on_this_party`, `t1`.`f_run_ip`, `t1`.`f_run_pid`, `t1`.`f_party_status`, `t1`.`f_start_time`, `t1`.`f_start_date`, `t1`.`f_end_time`, `t1`.`f_end_date`, `t1`.`f_elapsed` FROM `t_task` AS `t1` WHERE (((((`t1`.`f_job_id` = %s) AND (`t1`.`f_task_id` = %s)) AND (`t1`.`f_task_version` = %s)) AND (`t1`.`f_role` = %s)) AND (`t1`.`f_party_id` = %s))&#x27;, [&#x27;202107260820309976351&#x27;, &#x27;202107260820309976351_upload_0&#x27;, 0, &#x27;local&#x27;, &#x27;0&#x27;])</span><br><span class="line">[DEBUG] [2021-07-26 08:20:33,416] [1:140259119585024] - peewee.py[line:2863]: (&#x27;UPDATE `t_task` SET `f_component_name` = %s, `f_run_ip` = %s, `f_run_pid` = %s WHERE (((((`t_task`.`f_job_id` = %s) AND (`t_task`.`f_task_id` = %s)) AND (`t_task`.`f_task_version` = %s)) AND (`t_task`.`f_role` = %s)) AND (`t_task`.`f_party_id` = %s))&#x27;, [&#x27;upload_0&#x27;, &#x27;10.200.96.235&#x27;, 90, &#x27;202107260820309976351&#x27;, &#x27;202107260820309976351_upload_0&#x27;, 0, &#x27;local&#x27;, &#x27;0&#x27;])</span><br><span class="line"></span><br><span class="line">[DEBUG] [2021-07-26 08:20:33,425] [1:140259119585024] - peewee.py[line:2863]: (&#x27;SELECT `t1`.`f_create_time`, `t1`.`f_create_date`, `t1`.`f_update_time`, `t1`.`f_update_date`, `t1`.`f_job_id`, `t1`.`f_component_name`, `t1`.`f_task_id`, `t1`.`f_task_version`, `t1`.`f_initiator_role`, `t1`.`f_initiator_party_id`, `t1`.`f_federated_mode`, `t1`.`f_federated_status_collect_type`, `t1`.`f_status`, `t1`.`f_status_code`, `t1`.`f_role`, `t1`.`f_party_id`, `t1`.`f_run_on_this_party`, `t1`.`f_run_ip`, `t1`.`f_run_pid`, `t1`.`f_party_status`, `t1`.`f_start_time`, `t1`.`f_start_date`, `t1`.`f_end_time`, `t1`.`f_end_date`, `t1`.`f_elapsed` FROM `t_task` AS `t1` WHERE ((((`t1`.`f_task_id` = %s) AND (`t1`.`f_task_version` = %s)) AND (`t1`.`f_role` = %s)) AND (`t1`.`f_party_id` = %s))&#x27;, [&#x27;202107260820309976351_upload_0&#x27;, 0, &#x27;local&#x27;, &#x27;0&#x27;])</span><br><span class="line"></span><br><span class="line">[DEBUG] [2021-07-26 08:20:33,433] [1:140259119585024] - peewee.py[line:2863]: (&#x27;SELECT `t1`.`f_create_time`, `t1`.`f_create_date`, `t1`.`f_update_time`, `t1`.`f_update_date`, `t1`.`f_job_id`, `t1`.`f_component_name`, `t1`.`f_task_id`, `t1`.`f_task_version`, `t1`.`f_initiator_role`, `t1`.`f_initiator_party_id`, `t1`.`f_federated_mode`, `t1`.`f_federated_status_collect_type`, `t1`.`f_status`, `t1`.`f_status_code`, `t1`.`f_role`, `t1`.`f_party_id`, `t1`.`f_run_on_this_party`, `t1`.`f_run_ip`, `t1`.`f_run_pid`, `t1`.`f_party_status`, `t1`.`f_start_time`, `t1`.`f_start_date`, `t1`.`f_end_time`, `t1`.`f_end_date`, `t1`.`f_elapsed` FROM `t_task` AS `t1` WHERE (((((`t1`.`f_job_id` = %s) AND (`t1`.`f_task_id` = %s)) AND (`t1`.`f_task_version` = %s)) AND (`t1`.`f_role` = %s)) AND (`t1`.`f_party_id` = %s))&#x27;, [&#x27;202107260820309976351&#x27;, &#x27;202107260820309976351_upload_0&#x27;, 0, &#x27;local&#x27;, &#x27;0&#x27;])</span><br><span class="line"></span><br><span class="line">[DEBUG] [2021-07-26 08:20:33,441] [1:140259119585024] - peewee.py[line:2863]: (&#x27;SELECT `t1`.`f_create_time`, `t1`.`f_create_date`, `t1`.`f_update_time`, `t1`.`f_update_date`, `t1`.`f_job_id`, `t1`.`f_component_name`, `t1`.`f_task_id`, `t1`.`f_task_version`, `t1`.`f_initiator_role`, `t1`.`f_initiator_party_id`, `t1`.`f_federated_mode`, `t1`.`f_federated_status_collect_type`, `t1`.`f_status`, `t1`.`f_status_code`, `t1`.`f_role`, `t1`.`f_party_id`, `t1`.`f_run_on_this_party`, `t1`.`f_run_ip`, `t1`.`f_run_pid`, `t1`.`f_party_status`, `t1`.`f_start_time`, `t1`.`f_start_date`, `t1`.`f_end_time`, `t1`.`f_end_date`, `t1`.`f_elapsed` FROM `t_task` AS `t1` WHERE ((((`t1`.`f_task_id` = %s) AND (`t1`.`f_task_version` = %s)) AND (`t1`.`f_role` = %s)) AND (`t1`.`f_party_id` = %s))&#x27;, [&#x27;202107260820309976351_upload_0&#x27;, 0, &#x27;local&#x27;, &#x27;0&#x27;])</span><br></pre></td></tr></table></figure></p>
<ol>
<li><p>task<em>executor.py：初始化环境变量，设置session<br>sess.init<em>federation 会调用federation/eggroll/_federation.py 的 __init</em></em>，调用如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[DEBUG] [2021-07-26 08:20:39,837] [90:139687957583680] - _federation.py[line:35]: [federation.eggroll]init federation: rp_session_id=202107260820309976351_upload_0_0_local_0, rs_session_id=202107260820309976351_upload_0_0, party=Party(role=local, party_id=0), proxy_endpoint=rollsite:9370</span><br><span class="line">[DEBUG] [2021-07-26 08:20:39,837] [90:139687957583680] - _federation.py[line:45]: [federation.eggroll]init federation context done</span><br></pre></td></tr></table></figure>
</li>
<li><p>task_executor.py：开始run task<br>先在${job_log_dir}/${role}/${party}/DEBUG.log 打出日志</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:39,837] [90:139687957583680] - task_executor.py[line:156]: Run 202107260820309976351 upload_0 202107260820309976351_upload_0 local 0 task</span><br><span class="line">[INFO] [2021-07-26 08:20:39,838] [90:139687957583680] - task_executor.py[line:157]: Component parameters on party &#123;&#x27;UploadParam&#x27;: &#123;&#x27;file&#x27;: &#x27;/data/projects/fate/jobs/202107260820309976351/fate_upload_tmp/breast_hetero_guest.csv&#x27;, &#x27;head&#x27;: 1, &#x27;id_delimiter&#x27;: &#x27;,&#x27;, &#x27;partition&#x27;: 1, &#x27;namespace&#x27;: &#x27;cl&#x27;, &#x27;name&#x27;: &#x27;hetero_guest&#x27;, &#x27;storage_engine&#x27;: &#x27;&#x27;, &#x27;storage_address&#x27;: None, &#x27;destroy&#x27;: False&#125;, &#x27;initiator&#x27;: &#123;&#x27;role&#x27;: &#x27;local&#x27;, &#x27;party_id&#x27;: 0&#125;, &#x27;job_parameters&#x27;: &#123;&#x27;job_type&#x27;: &#x27;train&#x27;, &#x27;work_mode&#x27;: 1, &#x27;backend&#x27;: 0, &#x27;computing_engine&#x27;: &#x27;EGGROLL&#x27;, &#x27;federation_engine&#x27;: &#x27;EGGROLL&#x27;, &#x27;storage_engine&#x27;: &#x27;EGGROLL&#x27;, &#x27;engines_address&#x27;: &#123;&#x27;computing&#x27;: &#123;&#x27;cores_per_node&#x27;: 20, &#x27;nodes&#x27;: 1&#125;, &#x27;federation&#x27;: &#123;&#x27;host&#x27;: &#x27;rollsite&#x27;, &#x27;port&#x27;: 9370&#125;, &#x27;storage&#x27;: &#123;&#x27;cores_per_node&#x27;: 20, &#x27;nodes&#x27;: 1&#125;&#125;, &#x27;federated_mode&#x27;: &#x27;MULTIPLE&#x27;, &#x27;task_parallelism&#x27;: 1, &#x27;computing_partitions&#x27;: 4, &#x27;federated_status_collect_type&#x27;: &#x27;PULL&#x27;, &#x27;model_id&#x27;: &#x27;local-0#model&#x27;, &#x27;model_version&#x27;: &#x27;202107260820309976351&#x27;, &#x27;eggroll_run&#x27;: &#123;&#x27;eggroll.session.processors.per.node&#x27;: 4&#125;, &#x27;spark_run&#x27;: &#123;&#125;, &#x27;rabbitmq_run&#x27;: &#123;&#125;, &#x27;adaptation_parameters&#x27;: &#123;&#x27;task_nodes&#x27;: 1, &#x27;task_cores_per_node&#x27;: 4, &#x27;task_memory_per_node&#x27;: 0, &#x27;request_task_cores&#x27;: 4, &#x27;if_initiator_baseline&#x27;: False&#125;&#125;, &#x27;role&#x27;: &#123;&#x27;local&#x27;: [0]&#125;, &#x27;component_parameters&#x27;: &#123;&#x27;role&#x27;: &#123;&#x27;local&#x27;: &#123;&#x27;0&#x27;: &#123;&#x27;upload_0&#x27;: &#123;&#x27;name&#x27;: &#x27;hetero_guest&#x27;, &#x27;head&#x27;: 1, &#x27;file&#x27;: &#x27;/data/projects/fate/jobs/202107260820309976351/fate_upload_tmp/breast_hetero_guest.csv&#x27;, &#x27;partition&#x27;: 1, &#x27;namespace&#x27;: &#x27;cl&#x27;, &#x27;destroy&#x27;: False&#125;&#125;&#125;&#125;&#125;, &#x27;dsl_version&#x27;: 2, &#x27;local&#x27;: &#123;&#x27;role&#x27;: &#x27;local&#x27;, &#x27;party_id&#x27;: 0&#125;, &#x27;CodePath&#x27;: &#x27;fate_flow/components/upload.py/Upload&#x27;, &#x27;module&#x27;: &#x27;Upload&#x27;, &#x27;output_data_name&#x27;: None&#125;</span><br><span class="line">[INFO] [2021-07-26 08:20:39,838] [90:139687957583680] - task_executor.py[line:158]: Task input dsl &#123;&#125;</span><br></pre></td></tr></table></figure>
<p>然后获取 task_run_args，配置run_object，后执行</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">profile.profile_start()</span><br><span class="line">run_object.run(component_parameters_on_party, task_run_args)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>这里的run_object 是 Upload，run_object.run 即调用Upload.run()</p>
<ol>
<li>upload.py：执行run<br>根据 component_parameters 获取参数。<br>在${job_log_dir}/${role}/${party}/DEBUG.log 打出日志<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:39,883] [90:139687957583680] - upload.py[line:41]: &#123;&#x27;file&#x27;: &#x27;/data/projects/fate/jobs/202107260820309976351/fate_upload_tmp/breast_hetero_guest.csv&#x27;, &#x27;head&#x27;: 1, &#x27;id_delimiter&#x27;: &#x27;,&#x27;, &#x27;partition&#x27;: 1, &#x27;namespace&#x27;: &#x27;cl&#x27;, &#x27;name&#x27;: &#x27;hetero_guest&#x27;, &#x27;storage_engine&#x27;: &#x27;&#x27;, &#x27;storage_address&#x27;: None, &#x27;destroy&#x27;: False&#125;</span><br><span class="line">[INFO] [2021-07-26 08:20:39,883] [90:139687957583680] - upload.py[line:42]: &#123;&#x27;job_parameters&#x27;: &lt;fate_flow.entity.types.RunParameters object at 0x7f0b81a98b38&gt;&#125;</span><br></pre></td></tr></table></figure>
根据参数，设置各变量。</li>
</ol>
<p>build session<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[DEBUG] [2021-07-26 08:20:39,883] [90:139687957583680] - pool.py[line:129]: No connection available in pool.</span><br><span class="line">[DEBUG] [2021-07-26 08:20:39,889] [90:139687957583680] - pool.py[line:158]: Created new connection 139687331472832.</span><br><span class="line">[DEBUG] [2021-07-26 08:20:39,891] [90:139687957583680] - peewee.py[line:2863]: (&#x27;SELECT `t1`.`f_create_date`, `t1`.`f_update_date`, `t1`.`f_name`, `t1`.`f_namespace`, `t1`.`f_address`, `t1`.`f_engine`, `t1`.`f_type`, `t1`.`f_options`, `t1`.`f_partitions`, `t1`.`f_id_delimiter`, `t1`.`f_in_serialized`, `t1`.`f_have_head`, `t1`.`f_schema`, `t1`.`f_count`, `t1`.`f_part_of_data`, `t1`.`f_description`, `t1`.`f_create_time`, `t1`.`f_update_time` FROM `t_storage_table_meta` AS `t1` WHERE ((`t1`.`f_name` = %s) AND (`t1`.`f_namespace` = %s))&#x27;, [&#x27;hetero_guest&#x27;, &#x27;cl&#x27;])</span><br><span class="line">[DEBUG] [2021-07-26 08:20:39,894] [90:139687957583680] - pool.py[line:185]: Returning 139687331472832 to pool.</span><br><span class="line">[DEBUG] [2021-07-26 08:20:39,966] [90:139687957583680] - peewee.py[line:2863]: (&#x27;INSERT INTO `t_session_record` (`f_session_id`, `f_create_date`, `f_update_time`, `f_update_date`, `f_engine_name`, `f_engine_type`, `f_engine_address`, `f_create_time`) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)&#x27;, [&#x27;202107260820309976351_upload_0_0_local_0_storage_5cbb3a88edea11ebbccccaf5cc2d708a&#x27;, datetime.datetime(2021, 7, 26, 8, 20, 39), 1627287639962, datetime.datetime(2021, 7, 26, 8, 20, 39), &#x27;STANDALONE&#x27;, &#x27;storage&#x27;, &#x27;&#123;&#125;&#x27;, 1627287639962])</span><br><span class="line">[DEBUG] [2021-07-26 08:20:39,973] [90:139687957583680] - _session.py[line:144]: save session 202107260820309976351_upload_0_0_local_0_storage_5cbb3a88edea11ebbccccaf5cc2d708a record</span><br><span class="line">[DEBUG] [2021-07-26 08:20:39,974] [90:139687957583680] - pool.py[line:185]: Returning 139687331472832 to pool.</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[DEBUG] [2021-07-26 08:20:39,975] [90:139687957583680] - peewee.py[line:2863]: (&#x27;DELETE FROM `t_session_record` WHERE (`t_session_record`.`f_session_id` = %s)&#x27;, [&#x27;202107260820309976351_upload_0_0_local_0_storage_5cbb3a88edea11ebbccccaf5cc2d708a&#x27;])</span><br><span class="line">[DEBUG] [2021-07-26 08:20:39,981] [90:139687957583680] - _session.py[line:153]: delete session 202107260820309976351_upload_0_0_local_0_storage_5cbb3a88edea11ebbccccaf5cc2d708a record</span><br><span class="line">[DEBUG] [2021-07-26 08:20:39,981] [90:139687957583680] - pool.py[line:185]: Returning 139687331472832 to pool.</span><br><span class="line">[DEBUG] [2021-07-26 08:20:39,985] [90:139687957583680] - peewee.py[line:2863]: (&#x27;INSERT INTO `t_session_record` (`f_session_id`, `f_create_date`, `f_update_time`, `f_update_date`, `f_engine_name`, `f_engine_type`, `f_engine_address`, `f_create_time`) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)&#x27;, [&#x27;202107260820309976351_upload_0_0_local_0_storage_5cca2ef8edea11ebbccccaf5cc2d708a&#x27;, datetime.datetime(2021, 7, 26, 8, 20, 39), 1627287639985, datetime.datetime(2021, 7, 26, 8, 20, 39), &#x27;EGGROLL&#x27;, &#x27;storage&#x27;, &#x27;&#123;&#125;&#x27;, 1627287639985])</span><br><span class="line">[DEBUG] [2021-07-26 08:20:39,989] [90:139687957583680] - _session.py[line:144]: save session 202107260820309976351_upload_0_0_local_0_storage_5cca2ef8edea11ebbccccaf5cc2d708a record</span><br><span class="line">[DEBUG] [2021-07-26 08:20:39,989] [90:139687957583680] - pool.py[line:185]: Returning 139687331472832 to pool.</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">if table:</span><br><span class="line">    LOGGER.info(f&quot;destroy table name: &#123;name&#125; namespace: &#123;namespace&#125; engine: &#123;table.get_engine()&#125;&quot;)</span><br><span class="line">    table.destroy()</span><br><span class="line">else:</span><br><span class="line">    LOGGER.info(f&quot;can not found table name: &#123;name&#125; namespace: &#123;namespace&#125;, pass destroy&quot;)</span><br></pre></td></tr></table></figure>
<p><strong>注：如果有destroy 参数，会调用table.destroy</strong><br>更新upload的address,在${job_log_dir}/${role}/${party}/DEBUG.log 打出日志<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:41,836] [90:139687957583680] - upload.py[line:95]: upload to EGGROLL storage, address: &#123;&#x27;cores_per_node&#x27;: 20, &#x27;nodes&#x27;: 1, &#x27;name&#x27;: &#x27;hetero_guest&#x27;, &#x27;namespace&#x27;: &#x27;cl&#x27;, &#x27;storage_type&#x27;: &#x27;LMDB&#x27;&#125;</span><br></pre></td></tr></table></figure><br>这个address 就是LMDB 的存储位置<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">address = storage.StorageTableMeta.create_address(storage_engine=storage_engine, address_dict=address_dict)</span><br><span class="line">self.parameters[&quot;partitions&quot;] = partitions</span><br><span class="line">self.parameters[&quot;name&quot;] = name</span><br><span class="line">self.table = storage_session.create_table(address=address, **self.parameters)</span><br><span class="line">data_table_count = self.save_data_table(job_id, name, namespace, head)</span><br><span class="line">self.table.get_meta().update_metas(in_serialized=True)</span><br></pre></td></tr></table></figure><br>如果是使用local 模式调试，在项目目录下会生成data目录。<br><img src="/image/fate/02d76fd6588e43c39d28b9b6bd8484e2.png" alt="在这里插入图片描述"></p>
<p>如果是kubeFATE 部署，该数据位于 nodemanager 的 /data/projects/fate/eggroll/data/LMDB 目录下<br><img src="/image/fate/15bb01d3c0854489a5f209ec5736ac4b.png" alt="在这里插入图片描述"></p>
<p>依次为创建元数据，建表，保存数据，更新元数据</p>
<ol>
<li>upload.py：执行save_data_table ，<br>获取文件schma。<br>按最大文件块读取（    lines = fin.readlines(self.MAX_BYTES)），保存至LMDB<br>进度计算如下：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">save_progress = lines_count/input_feature_count*100//1</span><br></pre></td></tr></table></figure>
故而这里的进度和MAX_BYTES 有关。</li>
</ol>
<ol>
<li>upload.py： 调用ControllerClient.update_job(job_info=job_info) 更新task 状态（主要是进度），control_client.update_job<br>流转流程同前文，调用链为 ControllerClient.update_job -&gt; fate_flow_server 通过flask -&gt; party_app.update_task() -&gt;   TaskController.update_task(task_info=task_info) </li>
</ol>
<p>输出日志</p>
<p>${job_log_dir}/${role}/${party}/DEBUG.log<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:41,942] [90:139687957583680] - control_client.py[line:26]: request update job 202107260820309976351 on local 0</span><br></pre></td></tr></table></figure></p>
<p>在${job_log_dir}/fate_flow_audit.log 中的日志为<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:41,942] [90:139687957583680] - api_utils.py[line:122]: remote http api request: http://10.200.96.235:9380/v1/party/202107260820309976351/local/0/update</span><br><span class="line">[INFO] [2021-07-26 08:20:41,966] [90:139687957583680] - api_utils.py[line:129]: &#123;&quot;retcode&quot;:0,&quot;retmsg&quot;:&quot;success&quot;&#125;</span><br><span class="line"></span><br><span class="line">[INFO] [2021-07-26 08:20:41,966] [90:139687957583680] - api_utils.py[line:131]: remote http api response: /v1/party/202107260820309976351/local/0/update &#123;&#x27;retcode&#x27;: 0, &#x27;retmsg&#x27;: &#x27;success&#x27;&#125;</span><br></pre></td></tr></table></figure></p>
<p>在${job_log_dir}/fate_flow_schedule.log 中输出日志<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:41,950] [1:140259119585024] - job_saver.py[line:61]: try to update job 202107260820309976351</span><br><span class="line">[INFO] [2021-07-26 08:20:41,963] [1:140259119585024] - job_saver.py[line:64]: job 202107260820309976351 update successfully: &#123;&#x27;progress&#x27;: 100.0, &#x27;job_id&#x27;: &#x27;202107260820309976351&#x27;, &#x27;role&#x27;: &#x27;local&#x27;, &#x27;party_id&#x27;: &#x27;0&#x27;&#125;</span><br></pre></td></tr></table></figure></p>
<p>在${job_log_dir}/peewee.log中输出日志为<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[DEBUG] [2021-07-26 08:20:41,954] [1:140259119585024] - peewee.py[line:2863]: (&#x27;SELECT `t1`.`f_create_time`, `t1`.`f_create_date`, `t1`.`f_update_time`, `t1`.`f_update_date`, `t1`.`f_user_id`, `t1`.`f_job_id`, `t1`.`f_name`, `t1`.`f_description`, `t1`.`f_tag`, `t1`.`f_dsl`, `t1`.`f_runtime_conf`, `t1`.`f_runtime_conf_on_party`, `t1`.`f_train_runtime_conf`, `t1`.`f_roles`, `t1`.`f_work_mode`, `t1`.`f_initiator_role`, `t1`.`f_initiator_party_id`, `t1`.`f_status`, `t1`.`f_status_code`, `t1`.`f_role`, `t1`.`f_party_id`, `t1`.`f_is_initiator`, `t1`.`f_progress`, `t1`.`f_ready_signal`, `t1`.`f_ready_time`, `t1`.`f_cancel_signal`, `t1`.`f_cancel_time`, `t1`.`f_rerun_signal`, `t1`.`f_end_scheduling_updates`, `t1`.`f_engine_name`, `t1`.`f_engine_type`, `t1`.`f_cores`, `t1`.`f_memory`, `t1`.`f_remaining_cores`, `t1`.`f_remaining_memory`, `t1`.`f_resource_in_use`, `t1`.`f_apply_resource_time`, `t1`.`f_return_resource_time`, `t1`.`f_start_time`, `t1`.`f_start_date`, `t1`.`f_end_time`, `t1`.`f_end_date`, `t1`.`f_elapsed` FROM `t_job` AS `t1` WHERE (((`t1`.`f_job_id` = %s) AND (`t1`.`f_role` = %s)) AND (`t1`.`f_party_id` = %s))&#x27;, [&#x27;202107260820309976351&#x27;, &#x27;local&#x27;, &#x27;0&#x27;])</span><br><span class="line">[DEBUG] [2021-07-26 08:20:41,960] [1:140259119585024] - peewee.py[line:2863]: (&#x27;UPDATE `t_job` SET `f_progress` = %s WHERE ((((`t_job`.`f_job_id` = %s) AND (`t_job`.`f_role` = %s)) AND (`t_job`.`f_party_id` = %s)) AND (`t_job`.`f_progress` &lt;= %s))&#x27;, [100, &#x27;202107260820309976351&#x27;, &#x27;local&#x27;, &#x27;0&#x27;, 100])</span><br></pre></td></tr></table></figure></p>
<ol>
<li>upload.py： 回到9，继续保存文件，将data存入LMDB。当所有文件都存入完毕，更新metadata，更新tracker<br>在task_executor.py 中是<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">run_object.set_tracker(tracker=tracker_client)</span><br></pre></td></tr></table></figure>
故而这里self.tracker 调用的是TrackerClient<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">self.tracker.log_output_data_info(data_name=&#x27;upload&#x27;,</span><br><span class="line">                                  table_namespace=dst_table_namespace,</span><br><span class="line">                                  table_name=dst_table_name)</span><br><span class="line"></span><br><span class="line">self.tracker.log_metric_data(metric_namespace=&quot;upload&quot;,</span><br><span class="line">                             metric_name=&quot;data_access&quot;,</span><br><span class="line">                             metrics=[Metric(&quot;count&quot;, table_count)])</span><br><span class="line">self.tracker.set_metric_meta(metric_namespace=&quot;upload&quot;,</span><br><span class="line">                             metric_name=&quot;data_access&quot;,</span><br><span class="line">                             metric_meta=MetricMeta(name=&#x27;upload&#x27;, metric_type=&#x27;UPLOAD&#x27;))</span><br></pre></td></tr></table></figure>
对应的分别是log_output_data_DEBUG.log_metric_data，set_metric_meta三个方法<br>输出日志${job_log_dir}/${role}/${party}/DEBUG.log <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:42,704] [90:139687957583680] - tracker_client.py[line:127]: Request save job 202107260820309976351 task 202107260820309976351_upload_0 0 on local 0 data upload info</span><br><span class="line">[INFO] [2021-07-26 08:20:43,044] [1:140259119585024] - job_tracker.py[line:97]: save job 202107260820309976351 component upload_0 on local 0 upload data_access metric meta</span><br><span class="line">[INFO] [2021-07-26 08:20:44,838] [90:139687957583680] - job_tracker.py[line:159]: task id 202107260820309976351_upload_0 output data table is none</span><br><span class="line"></span><br></pre></td></tr></table></figure>
发起http请求，对应的${job_log_dir}/fate_flow_audit.log 中的日志为<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:42,704] [90:139687957583680] - api_utils.py[line:122]: remote http api request: http://10.200.96.235:9380/v1/tracker/202107260820309976351/upload_0/202107260820309976351_upload_0/0/local/0/output_data_info/save</span><br><span class="line">[INFO] [2021-07-26 08:20:43,009] [90:139687957583680] - api_utils.py[line:129]: &#123;&quot;retcode&quot;:0,&quot;retmsg&quot;:&quot;success&quot;&#125;</span><br><span class="line"></span><br><span class="line">[INFO] [2021-07-26 08:20:43,010] [90:139687957583680] - api_utils.py[line:131]: remote http api response: /v1/tracker/202107260820309976351/upload_0/202107260820309976351_upload_0/0/local/0/output_data_info/save &#123;&#x27;retcode&#x27;: 0, &#x27;retmsg&#x27;: &#x27;success&#x27;&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[INFO] [2021-07-26 08:20:43,011] [90:139687957583680] - api_utils.py[line:122]: remote http api request: http://10.200.96.235:9380/v1/tracker/202107260820309976351/upload_0/202107260820309976351_upload_0/0/local/0/metric_data/save</span><br><span class="line">[INFO] [2021-07-26 08:20:43,036] [90:139687957583680] - api_utils.py[line:129]: &#123;&quot;retcode&quot;:0,&quot;retmsg&quot;:&quot;success&quot;&#125;</span><br><span class="line"></span><br><span class="line">[INFO] [2021-07-26 08:20:43,036] [90:139687957583680] - api_utils.py[line:131]: remote http api response: /v1/tracker/202107260820309976351/upload_0/202107260820309976351_upload_0/0/local/0/metric_data/save &#123;&#x27;retcode&#x27;: 0, &#x27;retmsg&#x27;: &#x27;success&#x27;&#125;</span><br><span class="line"></span><br><span class="line">[INFO] [2021-07-26 08:20:43,037] [90:139687957583680] - api_utils.py[line:122]: remote http api request: http://10.200.96.235:9380/v1/tracker/202107260820309976351/upload_0/202107260820309976351_upload_0/0/local/0/metric_meta/save</span><br><span class="line">[INFO] [2021-07-26 08:20:43,056] [90:139687957583680] - api_utils.py[line:129]: &#123;&quot;retcode&quot;:0,&quot;retmsg&quot;:&quot;success&quot;&#125;</span><br><span class="line"></span><br><span class="line">[INFO] [2021-07-26 08:20:43,056] [90:139687957583680] - api_utils.py[line:131]: remote http api response: /v1/tracker/202107260820309976351/upload_0/202107260820309976351_upload_0/0/local/0/metric_meta/save &#123;&#x27;retcode&#x27;: 0, &#x27;retmsg&#x27;: &#x27;success&#x27;&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>对应的调用链是 TrackerClient.log_output_data_info() -&gt; fate_flow_server -&gt; tracker_app -&gt; job_tracker<br>在${job_log_dir}/fate_flow_schedule.log 中输出日志<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:43,018] [1:140259119585024] - job_tracker.py[line:74]: save job 202107260820309976351 component upload_0 on local 0 upload data_access metric data</span><br><span class="line">[INFO] [2021-07-26 08:20:43,044] [1:140259119585024] - job_tracker.py[line:97]: save job 202107260820309976351 component upload_0 on local 0 upload data_access metric meta</span><br><span class="line">[INFO] [2021-07-26 08:20:44,838] [90:139687957583680] - job_tracker.py[line:159]: task id 202107260820309976351_upload_0 output data table is none</span><br><span class="line"></span><br></pre></td></tr></table></figure><br>save完毕，返回table_count。</p>
<ol>
<li>upload.py： 打印完成日志，并清理临时文件，输出统计信息<br>${job_log_dir}/${role}/${party}/DEBUG.log <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:44,822] [90:139687957583680] - upload.py[line:102]: ------------load data finish!-----------------</span><br><span class="line">[INFO] [2021-07-26 08:20:44,822] [90:139687957583680] - upload.py[line:106]: remove tmp upload file</span><br><span class="line">[INFO] [2021-07-26 08:20:44,823] [90:139687957583680] - upload.py[line:107]: /data/projects/fate/jobs/202107260820309976351/fate_upload_tmp</span><br><span class="line">[INFO] [2021-07-26 08:20:44,823] [90:139687957583680] - upload.py[line:111]: file: /data/projects/fate/jobs/202107260820309976351/fate_upload_tmp/breast_hetero_guest.csv</span><br><span class="line">[INFO] [2021-07-26 08:20:44,824] [90:139687957583680] - upload.py[line:112]: total data_count: 569</span><br><span class="line">[INFO] [2021-07-26 08:20:44,824] [90:139687957583680] - upload.py[line:113]: table name: hetero_guest, table namespace: cl</span><br></pre></td></tr></table></figure></li>
<li><p>upload.py：回到8 ，执行profile.profile_ends()<br>profile.profile_ends() 会打出${task_log_dir}/PROFILING.log中的日志（分别收集到INFO，DEBUG中）<br>${job_log_dir}/${role}/${party}/DEBUG.log </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:44,837] [90:139687957583680] - profile.py[line:249]: </span><br><span class="line">Computing:</span><br><span class="line">+----------+------------------------------------------+</span><br><span class="line">| function |                                          |</span><br><span class="line">+----------+------------------------------------------+</span><br><span class="line">|  total   | n=0, sum=0.0000, mean=0.0000, max=0.0000 |</span><br><span class="line">+----------+------------------------------------------+</span><br><span class="line"></span><br><span class="line">Federation:</span><br><span class="line">+--------+------------------------------------------+</span><br><span class="line">|  get   |                                         |</span><br><span class="line">+--------+------------------------------------------+</span><br><span class="line">| remote |                                          |</span><br><span class="line">+--------+------------------------------------------+</span><br><span class="line">| total  | n=0, sum=0.0000, mean=0.0000, max=0.0000 |</span><br><span class="line">+--------+------------------------------------------+</span><br><span class="line"></span><br><span class="line">[DEBUG] [2021-07-26 08:20:44,838] [90:139687957583680] - profile.py[line:250]: </span><br><span class="line">Detailed Computing:</span><br><span class="line">+-------+------------------------------------------+</span><br><span class="line">| stack |                                          |</span><br><span class="line">+-------+------------------------------------------+</span><br><span class="line">| total | n=0, sum=0.0000, mean=0.0000, max=0.0000 |</span><br><span class="line">+-------+------------------------------------------+</span><br></pre></td></tr></table></figure>
</li>
<li><p>task_executor.py：执行完毕之后，再save_data ，save_out_model，然后调用report_task_update_to_driver。同4、5。再输出统计信息<br>${job_log_dir}/${role}/${party}/DEBUG.log </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:44,838] [90:139687957583680] - task_executor.py[line:318]: report task 202107260820309976351_upload_0 0 local 0 to driver</span><br><span class="line">[INFO] [2021-07-26 08:20:44,838] [90:139687957583680] - control_client.py[line:42]: request update job 202107260820309976351 task 202107260820309976351_upload_0 0 on local 0</span><br><span class="line">[INFO] [2021-07-26 08:20:44,938] [90:139687957583680] - task_executor.py[line:207]: task 202107260820309976351_upload_0 local 0 start time: 2021-07-26 08:20:33</span><br><span class="line">[INFO] [2021-07-26 08:20:44,939] [90:139687957583680] - task_executor.py[line:209]: task 202107260820309976351_upload_0 local 0 end time: 2021-07-26 08:20:44</span><br><span class="line">[INFO] [2021-07-26 08:20:44,939] [90:139687957583680] - task_executor.py[line:211]: task 202107260820309976351_upload_0 local 0 takes 11.447s</span><br><span class="line">[INFO] [2021-07-26 08:20:44,939] [90:139687957583680] - task_executor.py[line:214]: Finish 202107260820309976351 upload_0 202107260820309976351_upload_0 0 local 0 task success</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>对应${job_log_dir}/fate_flow_schedule.log 中输出日志<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:44,844] [1:140259119585024] - job_saver.py[line:81]: try to update job 202107260820309976351 task 202107260820309976351_upload_0 0</span><br><span class="line">[INFO] [2021-07-26 08:20:44,864] [1:140259119585024] - job_saver.py[line:84]: job 202107260820309976351 task 202107260820309976351_upload_0 0 update successfully</span><br><span class="line">[INFO] [2021-07-26 08:20:44,873] [1:140259119585024] - job_saver.py[line:71]: try to update job 202107260820309976351 task 202107260820309976351_upload_0 0 status</span><br><span class="line">[INFO] [2021-07-26 08:20:44,908] [1:140259119585024] - job_saver.py[line:74]: update job 202107260820309976351 task 202107260820309976351_upload_0 0 status successfully: &#123;&#x27;job_id&#x27;: &#x27;202107260820309976351&#x27;, &#x27;component_name&#x27;: &#x27;upload_0&#x27;, &#x27;task_id&#x27;: &#x27;202107260820309976351_upload_0&#x27;, &#x27;task_version&#x27;: &#x27;0&#x27;, &#x27;role&#x27;: &#x27;local&#x27;, &#x27;party_id&#x27;: &#x27;0&#x27;, &#x27;run_ip&#x27;: &#x27;10.200.96.235&#x27;, &#x27;run_pid&#x27;: 90, &#x27;party_status&#x27;: &#x27;success&#x27;, &#x27;end_time&#x27;: 1627287644838, &#x27;elapsed&#x27;: 11447&#125;</span><br></pre></td></tr></table></figure></p>
<ol>
<li>task_executor.py：TaskExecutor.run_task()执行完毕，执行 TaskExecutor.report_task_update_to_driver(task_info=task_info)<br>${job_log_dir}/${role}/${party}/DEBUG.log <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:44,940] [90:139687957583680] - task_executor.py[line:318]: report task 202107260820309976351_upload_0 0 local 0 to driver</span><br><span class="line">[INFO] [2021-07-26 08:20:44,940] [90:139687957583680] - control_client.py[line:42]: request update job 202107260820309976351 task 202107260820309976351_upload_0 0 on local 0</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>对应${job_log_dir}/fate_flow_schedule.log 中输出日志<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:44,947] [1:140259119585024] - job_saver.py[line:81]: try to update job 202107260820309976351 task 202107260820309976351_upload_0 0</span><br><span class="line">[WARNING] [2021-07-26 08:20:44,972] [1:140259119585024] - job_saver.py[line:86]: job 202107260820309976351 task 202107260820309976351_upload_0 0 update does not take effect</span><br><span class="line">[INFO] [2021-07-26 08:20:44,996] [1:140259119585024] - job_saver.py[line:71]: try to update job 202107260820309976351 task 202107260820309976351_upload_0 0 status</span><br><span class="line">[INFO] [2021-07-26 08:20:45,015] [1:140259119585024] - job_saver.py[line:76]: update job 202107260820309976351 task 202107260820309976351_upload_0 0 status update does not take effect: &#123;&#x27;job_id&#x27;: &#x27;202107260820309976351&#x27;, &#x27;component_name&#x27;: &#x27;upload_0&#x27;, &#x27;task_id&#x27;: &#x27;202107260820309976351_upload_0&#x27;, &#x27;task_version&#x27;: &#x27;0&#x27;, &#x27;role&#x27;: &#x27;local&#x27;, &#x27;party_id&#x27;: &#x27;0&#x27;, &#x27;run_ip&#x27;: &#x27;10.200.96.235&#x27;, &#x27;run_pid&#x27;: 90, &#x27;party_status&#x27;: &#x27;success&#x27;, &#x27;end_time&#x27;: 1627287644838, &#x27;elapsed&#x27;: 11447&#125;</span><br></pre></td></tr></table></figure><br>注：这里的日志会和dag_scheduler 轮询的日志混在一起。区别就是 这个部分有run_pid。</p>
<ol>
<li>返回执行结果，对应1<br>至此，整个task 执行完毕。</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2023/08/20/fate-log-6-upload-job-schedule/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="喵十八">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="喵十八の小窝">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/08/20/fate-log-6-upload-job-schedule/" class="post-title-link" itemprop="url">FATE学习：跟着日志读源码（六）upload任务task schedule阶段</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2023-08-20 12:11:47 / Modified: 13:36:39" itemprop="dateCreated datePublished" datetime="2023-08-20T12:11:47+08:00">2023-08-20</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/FATE/" itemprop="url" rel="index"><span itemprop="name">FATE</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h1><p>在前文的基础上，因为job已经处于running状态，会按照task的依赖关系，依次调度该job下的task。</p>
<p> 调度之后的操作有2部分： </p>
<ul>
<li>Part1. 申请资源  </li>
<li>Part2. start job：将job的状态从waiting -&gt; running 状态<br>涉及的主要方法：   dag_scheduler.schedule_running_jobs()</li>
</ul>
<h1 id="执行步骤"><a href="#执行步骤" class="headerlink" title="执行步骤"></a>执行步骤</h1><p><img src="/image/fate/8ea2c9e7fee841dc82c8448a00efbf5d.png" alt="在这里插入图片描述"></p>
<ol>
<li>dag_scheduler.py：rundo轮询，发现处于running状态的job（这一部分见fate flow server 启动部分）<br>对应fate_flow/fate_flow_schedule.log依次为<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,117] [1:140259369826048] - dag_scheduler.py[line:148]: start schedule running jobs</span><br><span class="line">[INFO] [2021-07-26 08:20:32,127] [1:140259369826048] - dag_scheduler.py[line:150]: have 1 running jobs</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>对应查询running状态的fate_flow/peewee.log为<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[DEBUG] [2021-07-26 08:20:34,354] [1:140259369826048] - peewee.py[line:2863]: (&#x27;SELECT `t1`.`f_create_time`, `t1`.`f_create_date`, `t1`.`f_update_time`, `t1`.`f_update_date`, `t1`.`f_user_id`, `t1`.`f_job_id`, `t1`.`f_name`, `t1`.`f_description`, `t1`.`f_tag`, `t1`.`f_dsl`, `t1`.`f_runtime_conf`, `t1`.`f_runtime_conf_on_party`, `t1`.`f_train_runtime_conf`, `t1`.`f_roles`, `t1`.`f_work_mode`, `t1`.`f_initiator_role`, `t1`.`f_initiator_party_id`, `t1`.`f_status`, `t1`.`f_status_code`, `t1`.`f_role`, `t1`.`f_party_id`, `t1`.`f_is_initiator`, `t1`.`f_progress`, `t1`.`f_ready_signal`, `t1`.`f_ready_time`, `t1`.`f_cancel_signal`, `t1`.`f_cancel_time`, `t1`.`f_rerun_signal`, `t1`.`f_end_scheduling_updates`, `t1`.`f_engine_name`, `t1`.`f_engine_type`, `t1`.`f_cores`, `t1`.`f_memory`, `t1`.`f_remaining_cores`, `t1`.`f_remaining_memory`, `t1`.`f_resource_in_use`, `t1`.`f_apply_resource_time`, `t1`.`f_return_resource_time`, `t1`.`f_start_time`, `t1`.`f_start_date`, `t1`.`f_end_time`, `t1`.`f_end_date`, `t1`.`f_elapsed` FROM `t_job` AS `t1` WHERE ((`t1`.`f_is_initiator` = %s) AND (`t1`.`f_status` = %s)) ORDER BY `t1`.`f_create_time` ASC&#x27;, [True, &#x27;running&#x27;])</span><br></pre></td></tr></table></figure></p>
<ol>
<li>dag_scheduler.py：执行schedule_running_jobs 调度处于running 状态的job。<br>先在${job_log_dir}/fate_flow_schedule.log 输出<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,128] [1:140259369826048] - dag_scheduler.py[line:152]: schedule running job 202107260820309976351</span><br></pre></td></tr></table></figure></li>
<li>dag_scheduler.py：调用schedule_utils.get_job_dsl_parser 解析参数</li>
<li>schedule_utils.py：返回解析完毕的 dsl_parser</li>
<li>dag_scheduler.py：调用TaskScheduler.schedule(job=job, dsl_parser=dsl_parser, canceled=job.f_cancel_signal)调度task，注意这里的f_cancel_signal，这个是在detector 轮询时，更新的状态。</li>
<li>task_scheduler.py：执行task调度。<br>先在${job_log_dir}/fate_flow_schedule.log 输出<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,130] [1:140259369826048] - task_scheduler.py[line:28]: scheduling job 202107260820309976351 tasks</span><br></pre></td></tr></table></figure>
调用JobSaver.get_tasks_asc 获取相关信息。</li>
<li>job_saver.py： 执行如下源码，获取该job对应的task<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tasks = Task.select().where(Task.f_job_id == job_id, Task.f_role == role, Task.f_party_id == party_id).order_by(Task.f_create_time.asc())</span><br><span class="line">tasks_group = cls.get_latest_tasks(tasks=tasks)</span><br></pre></td></tr></table></figure>
按照create_time，升序获取当前job中涉及的task。从前文可知，create job 的时候，会根据job涉及的component， 依次create 不同的task ，所以在这里按照create_time顺序获取，也就是task的执行顺序（？如果db出问题，create_time 乱了，咋玩？）<br>操作数据库的日志在fate_flow/peewee.log 中</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[DEBUG] [2021-07-26 08:20:32,133] [1:140259369826048] - peewee.py[line:2863]: (&#x27;SELECT `t1`.`f_create_time`, `t1`.`f_create_date`, `t1`.`f_update_time`, `t1`.`f_update_date`, `t1`.`f_job_id`, `t1`.`f_component_name`, `t1`.`f_task_id`, `t1`.`f_task_version`, `t1`.`f_initiator_role`, `t1`.`f_initiator_party_id`, `t1`.`f_federated_mode`, `t1`.`f_federated_status_collect_type`, `t1`.`f_status`, `t1`.`f_status_code`, `t1`.`f_role`, `t1`.`f_party_id`, `t1`.`f_run_on_this_party`, `t1`.`f_run_ip`, `t1`.`f_run_pid`, `t1`.`f_party_status`, `t1`.`f_start_time`, `t1`.`f_start_date`, `t1`.`f_end_time`, `t1`.`f_end_date`, `t1`.`f_elapsed` FROM `t_task` AS `t1` WHERE (((`t1`.`f_job_id` = %s) AND (`t1`.`f_role` = %s)) AND (`t1`.`f_party_id` = %s)) ORDER BY `t1`.`f_create_time` ASC&#x27;, [&#x27;202107260820309976351&#x27;, &#x27;local&#x27;, &#x27;0&#x27;])</span><br></pre></td></tr></table></figure>
<p>get_latest_tasks 的作用是，获取new version 的task。<br>最后，将需要执行的tasks 返回给task_scheduler</p>
<ol>
<li>task_scheduler.py：遍历待执行的task，针对每个task执行如下操作：<br>判断federated_status_collect_type，如是PULL 需要执行collect_task_of_all_party()通过调用JobSaver.query_task()获取各个party 上task的信息。（步骤9 -10）<br>调用federated_task_status 并计算当前task状态（11-13）<br>更新task状态 14<br>加入waiting task 队列 15<br><strong>注：PULL需要主动拉取多方计算下，其余party上，对应task的信息</strong></li>
<li>job_saver.py：执行query_task，返回结果，对应的fate_flow/peewee.log日志如下<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[DEBUG] [2021-07-26 08:20:32,139] [1:140259369826048] - peewee.py[line:2863]: (&#x27;SELECT `t1`.`f_create_time`, `t1`.`f_create_date`, `t1`.`f_update_time`, `t1`.`f_update_date`, `t1`.`f_job_id`, `t1`.`f_component_name`, `t1`.`f_task_id`, `t1`.`f_task_version`, `t1`.`f_initiator_role`, `t1`.`f_initiator_party_id`, `t1`.`f_federated_mode`, `t1`.`f_federated_status_collect_type`, `t1`.`f_status`, `t1`.`f_status_code`, `t1`.`f_role`, `t1`.`f_party_id`, `t1`.`f_run_on_this_party`, `t1`.`f_run_ip`, `t1`.`f_run_pid`, `t1`.`f_party_status`, `t1`.`f_start_time`, `t1`.`f_start_date`, `t1`.`f_end_time`, `t1`.`f_end_date`, `t1`.`f_elapsed` FROM `t_task` AS `t1` WHERE ((`t1`.`f_task_id` = %s) AND (`t1`.`f_task_version` = %s))&#x27;, [&#x27;202107260820309976351_upload_0&#x27;, 0])</span><br></pre></td></tr></table></figure></li>
<li><p>task_scheduler.py：紧接着8，对9的查询结果进行判断。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">if not len(tasks_status_on_all) &gt; 1 and not TaskStatus.RUNNING in tasks_status_on_all:</span><br><span class="line">    return</span><br></pre></td></tr></table></figure>
<p>当前是upload任务，不涉及多方计算，故len(tasks_status_on_all) = 1，且此时task处于waiting，故直接返回，后面task 变为runing，便会执行下文的collect。</p>
</li>
<li><p>task_scheduler.py：执行federated_task_status 获取当前task状态。先调用JobSaver.query_job()进行查询</p>
</li>
<li>job_saver.py：执行query_task，返回结果，对应的fate_flow/peewee.log日志如下<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[DEBUG] [2021-07-26 08:20:32,144] [1:140259369826048] - peewee.py[line:2863]: (&#x27;SELECT `t1`.`f_create_time`, `t1`.`f_create_date`, `t1`.`f_update_time`, `t1`.`f_update_date`, `t1`.`f_job_id`, `t1`.`f_component_name`, `t1`.`f_task_id`, `t1`.`f_task_version`, `t1`.`f_initiator_role`, `t1`.`f_initiator_party_id`, `t1`.`f_federated_mode`, `t1`.`f_federated_status_collect_type`, `t1`.`f_status`, `t1`.`f_status_code`, `t1`.`f_role`, `t1`.`f_party_id`, `t1`.`f_run_on_this_party`, `t1`.`f_run_ip`, `t1`.`f_run_pid`, `t1`.`f_party_status`, `t1`.`f_start_time`, `t1`.`f_start_date`, `t1`.`f_end_time`, `t1`.`f_end_date`, `t1`.`f_elapsed` FROM `t_task` AS `t1` WHERE ((`t1`.`f_task_id` = %s) AND (`t1`.`f_task_version` = %s))&#x27;, [&#x27;202107260820309976351_upload_0&#x27;, 0])</span><br></pre></td></tr></table></figure></li>
<li><p>task_scheduler.py：判断除non-idmapping role之外，所有的party上，该task的状态（？non-idmapping 这段逻辑没看懂），如存在非SUCCESS状态的，则收集所有状态，调用calculate_multi_party_task_status() 计算当前状态。并输出日志到${job_log_dir}/fate_flow_schedule.log </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,148] [1:140259369826048] - task_scheduler.py[line:143]: job 202107260820309976351 task 202107260820309976351_upload_0 0 status is waiting, calculate by task party status list: [&#x27;waiting&#x27;]</span><br></pre></td></tr></table></figure>
</li>
<li><p>task_scheduler.py：判断13计算得到的状态和原状态是否一致，如不一致，则更新原状态，并调用FederatedScheduler.sync_task_status 更新各个party该task的状态。（这里都为waiting，状态一致无需更新）</p>
</li>
<li><p>task_scheduler.py：判断状态，如为waiting则加入waiting_tasks队列，如为EndStatus，则调用FederatedScheduler.stop_task</p>
</li>
<li><p>task_scheduler.py：判断是否canceled，如未cancel 则遍历waiting_tasks执行17-48。（疑问：为啥不是先判断？）</p>
</li>
<li><p>task_scheduler.py：针对每个task，调用dsl_parser.get_upstream_dependent_components() 获取其依赖的components，如果前置components中存在未success的，break跳出遍历。<br>（？？ 存疑 这里的else缩进）</p>
</li>
<li><p>task_scheduler.py：调用start_task，启动task。<br>执行第一步，先输出日志到${job_log_dir}/fate_flow_schedule.log </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,148] [1:140259369826048] - task_scheduler.py[line:80]: try to start job 202107260820309976351 task 202107260820309976351_upload_0 0 on local 0</span><br></pre></td></tr></table></figure>
</li>
<li><p>task_scheduler.py：调用ResourceManager.apply_for_task_resource 申请资源</p>
</li>
<li><p>resource_manager.py：执行resource_for_task，先调用calculate_task_resource计算该task所需的资源。<br>这里需要注意的是，调用的代码是</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cores_per_task, memory_per_task = cls.calculate_task_resource(task_info=task_info)</span><br></pre></td></tr></table></figure>
<p>而calculate_task_resource的定义是</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def calculate_task_resource(cls, task_parameters: RunParameters = None, task_info: dict = None):</span><br><span class="line">    if not task_parameters:</span><br><span class="line">        job_parameters = job_utils.get_job_parameters(job_id=task_info[&quot;job_id&quot;],</span><br><span class="line">                                                      role=task_info[&quot;role&quot;],</span><br><span class="line">                                                      party_id=task_info[&quot;party_id&quot;])</span><br><span class="line">        task_parameters = RunParameters(**job_parameters)</span><br><span class="line">    if task_info[&quot;role&quot;] in IGNORE_RESOURCE_ROLES and task_parameters.computing_engine in SUPPORT_IGNORE_RESOURCE_ENGINES:</span><br><span class="line">        cores_per_task = 0</span><br><span class="line">        memory_per_task = 0</span><br><span class="line">    else:</span><br><span class="line">        cores_per_task = task_parameters.adaptation_parameters[&quot;task_cores_per_node&quot;] * \</span><br><span class="line">                         task_parameters.adaptation_parameters[&quot;task_nodes&quot;]</span><br><span class="line">        memory_per_task = task_parameters.adaptation_parameters[&quot;task_memory_per_node&quot;] * \</span><br><span class="line">                          task_parameters.adaptation_parameters[&quot;task_nodes&quot;]</span><br><span class="line">    return cores_per_task, memory_per_task</span><br></pre></td></tr></table></figure>
<p>task_parameters 为空，故而会调用job_utils.get_job_parameters 对应的fate_flow/peewee.log日志为</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[DEBUG] [2021-07-26 08:20:32,149] [1:140259369826048] - peewee.py[line:2863]: (&#x27;SELECT `t1`.`f_runtime_conf_on_party` FROM `t_job` AS `t1` WHERE (((`t1`.`f_job_id` = %s) AND (`t1`.`f_role` = %s)) AND (`t1`.`f_party_id` = %s))&#x27;, [&#x27;202107260820309976351&#x27;, &#x27;local&#x27;, &#x27;0&#x27;])</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>如申请到的cores_per_task 和 memory_per_task 有非0值，则执行update_resource_sql 生成filters 和updates 操作，再更新resource表。<br>执行sql对应的fate_flow/peewee.log日志为<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[DEBUG] [2021-07-26 08:20:32,152] [1:140259369826048] - peewee.py[line:2863]: (&#x27;UPDATE `t_job` SET `f_remaining_cores` = (`t_job`.`f_remaining_cores` - %s), `f_remaining_memory` = (`t_job`.`f_remaining_memory` - %s) WHERE ((((((`t_job`.`f_remaining_cores` &gt;= %s) AND (`t_job`.`f_remaining_memory` &gt;= %s)) AND (`t_job`.`f_job_id` = %s)) AND (`t_job`.`f_role` = %s)) AND (`t_job`.`f_party_id` = %s)) AND (`t_job`.`f_resource_in_use` = %s))&#x27;, [4, 0, 4, 0, &#x27;202107260820309976351&#x27;, &#x27;local&#x27;, &#x27;0&#x27;, True])</span><br></pre></td></tr></table></figure><br>执行成功，会在${job_log_dir}/fate_flow_schedule.log 输出日志，并将成功状态返回<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,158] [1:140259369826048] - resource_manager.py[line:285]: task 202107260820309976351_upload_0 0 apply resource successfully</span><br></pre></td></tr></table></figure></p>
<ol>
<li><p>task_scheduler.py：若未申请资源成功，返回无资源。否则将task 状态置为running，并调用jobsaver.update_task_status 更新状态。</p>
</li>
<li><p>job_saver.py：执行update_task_status()<br>先在${job_log_dir}/fate_flow_schedule.log 输出日志</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,159] [1:140259369826048] - job_saver.py[line:71]: try to update job 202107260820309976351 task 202107260820309976351_upload_0 0 status</span><br></pre></td></tr></table></figure>
<p>执行update_status，先通过select，获取task基本信息，fate_flow/peewee.log日志为</p>
</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[DEBUG] [2021-07-26 08:20:32,160] [1:140259369826048] - peewee.py[line:2863]: (&#x27;SELECT `t1`.`f_create_time`, `t1`.`f_create_date`, `t1`.`f_update_time`, `t1`.`f_update_date`, `t1`.`f_job_id`, `t1`.`f_component_name`, `t1`.`f_task_id`, `t1`.`f_task_version`, `t1`.`f_initiator_role`, `t1`.`f_initiator_party_id`, `t1`.`f_federated_mode`, `t1`.`f_federated_status_collect_type`, `t1`.`f_status`, `t1`.`f_status_code`, `t1`.`f_role`, `t1`.`f_party_id`, `t1`.`f_run_on_this_party`, `t1`.`f_run_ip`, `t1`.`f_run_pid`, `t1`.`f_party_status`, `t1`.`f_start_time`, `t1`.`f_start_date`, `t1`.`f_end_time`, `t1`.`f_end_date`, `t1`.`f_elapsed` FROM `t_task` AS `t1` WHERE (((((`t1`.`f_job_id` = %s) AND (`t1`.`f_task_id` = %s)) AND (`t1`.`f_task_version` = %s)) AND (`t1`.`f_role` = %s)) AND (`t1`.`f_party_id` = %s))&#x27;, [&#x27;202107260820309976351&#x27;, &#x27;202107260820309976351_upload_0&#x27;, 0, &#x27;local&#x27;, &#x27;0&#x27;])</span><br></pre></td></tr></table></figure>
<p>修改之后再update，fate_flow/peewee.log日志<br><strong>注：这里update 会是调用update_status() 方法，会先判断能否从oldStatus 转换成newStatus，如不能，不会执行update语句，直接返回false</strong><br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[DEBUG] [2021-07-26 08:20:32,165] [1:140259369826048] - peewee.py[line:2863]: (&#x27;UPDATE `t_task` SET `f_status` = %s WHERE ((((((`t_task`.`f_job_id` = %s) AND (`t_task`.`f_task_id` = %s)) AND (`t_task`.`f_task_version` = %s)) AND (`t_task`.`f_role` = %s)) AND (`t_task`.`f_party_id` = %s)) AND (`t_task`.`f_status` = %s))&#x27;, [&#x27;running&#x27;, &#x27;202107260820309976351&#x27;, &#x27;202107260820309976351_upload_0&#x27;, 0, &#x27;local&#x27;, &#x27;0&#x27;, &#x27;waiting&#x27;])</span><br></pre></td></tr></table></figure><br>如执行成功，则在${job_log_dir}/fate_flow_schedule.log 输出日志，并返回执行状态（如失败，打出失败日志update does not take effect）<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,172] [1:140259369826048] - job_saver.py[line:74]: update job 202107260820309976351 task 202107260820309976351_upload_0 0 status successfully: &#123;&#x27;job_id&#x27;: &#x27;202107260820309976351&#x27;, &#x27;task_id&#x27;: &#x27;202107260820309976351_upload_0&#x27;, &#x27;task_version&#x27;: 0, &#x27;role&#x27;: &#x27;local&#x27;, &#x27;party_id&#x27;: &#x27;0&#x27;, &#x27;status&#x27;: &#x27;running&#x27;&#125;</span><br></pre></td></tr></table></figure></p>
<ol>
<li>task_scheduler.py：如状态未更新成功，打出失败日志${job_log_dir}/fate_flow_schedule.log ，并回收资源。否则输出成功日志，对应18<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,173] [1:140259369826048] - task_scheduler.py[line:93]: start job 202107260820309976351 task 202107260820309976351_upload_0 0 on local 0</span><br></pre></td></tr></table></figure></li>
<li>task_scheduler.py：调用FederatedScheduler.sync_task_status() 同步状态至所有party</li>
<li><p>federated_scheduler.py： 执行sync_task_status()<br>先打印日志到${job_log_dir}/fate_flow_schedule.log </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,173] [1:140259369826048] - federated_scheduler.py[line:192]: job 202107260820309976351 task 202107260820309976351_upload_0 0 is running, sync to all party</span><br></pre></td></tr></table></figure>
<p>再调用 task_command() 通过 federated_coordination_on_http 发起http 请求</p>
</li>
<li><p>api_utils.py：发起请求，日志见 ${job_log_dir}/fate_flow_audit.log </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,175] [1:140259369826048] - api_utils.py[line:122]: remote http api request: http://10.200.96.235:9380/v1/party/202107260820309976351/upload_0/202107260820309976351_upload_0/0/local/0/status/running</span><br></pre></td></tr></table></figure>
</li>
<li><p>fate_flow_server.py：接受请求，通过flask，转移到party_app</p>
</li>
<li><p>party_app.py： 执行task_status，调用TaskController.update_task_status(task_info=task_info)，更新task信息。实际操作是调用JobSaver.update_task_status</p>
</li>
<li><p>job_saver.py：执行update_task_status()<br>先输出日志到${job_log_dir}/fate_flow_schedule.log </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,183] [1:140259119585024] - job_saver.py[line:71]: try to update job 202107260820309976351 task 202107260820309976351_upload_0 0 status</span><br></pre></td></tr></table></figure>
<p>再执行update_status，先通过select，获取task基本信息，fate_flow/peewee.log日志为</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[DEBUG] [2021-07-26 08:20:32,187] [1:140259119585024] - peewee.py[line:2863]: (&#x27;SELECT `t1`.`f_create_time`, `t1`.`f_create_date`, `t1`.`f_update_time`, `t1`.`f_update_date`, `t1`.`f_job_id`, `t1`.`f_component_name`, `t1`.`f_task_id`, `t1`.`f_task_version`, `t1`.`f_initiator_role`, `t1`.`f_initiator_party_id`, `t1`.`f_federated_mode`, `t1`.`f_federated_status_collect_type`, `t1`.`f_status`, `t1`.`f_status_code`, `t1`.`f_role`, `t1`.`f_party_id`, `t1`.`f_run_on_this_party`, `t1`.`f_run_ip`, `t1`.`f_run_pid`, `t1`.`f_party_status`, `t1`.`f_start_time`, `t1`.`f_start_date`, `t1`.`f_end_time`, `t1`.`f_end_date`, `t1`.`f_elapsed` FROM `t_task` AS `t1` WHERE (((((`t1`.`f_job_id` = %s) AND (`t1`.`f_task_id` = %s)) AND (`t1`.`f_task_version` = %s)) AND (`t1`.`f_role` = %s)) AND (`t1`.`f_party_id` = %s))&#x27;, [&#x27;202107260820309976351&#x27;, &#x27;202107260820309976351_upload_0&#x27;, 0, &#x27;local&#x27;, &#x27;0&#x27;])</span><br></pre></td></tr></table></figure>
<p>这里和22 的操作一样，但是时间戳不同。接下去update部分，则因为running状态不能转变为running 状态，不会执行update，所以就返回了false。<br>输出失败的日志到${job_log_dir}/fate_flow_schedule.log </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,192] [1:140259119585024] - job_saver.py[line:76]: update job 202107260820309976351 task 202107260820309976351_upload_0 0 status update does not take effect: &#123;&#x27;job_id&#x27;: &#x27;202107260820309976351&#x27;, &#x27;task_id&#x27;: &#x27;202107260820309976351_upload_0&#x27;, &#x27;task_version&#x27;: &#x27;0&#x27;, &#x27;role&#x27;: &#x27;local&#x27;, &#x27;party_id&#x27;: &#x27;0&#x27;, &#x27;status&#x27;: &#x27;running&#x27;&#125;</span><br></pre></td></tr></table></figure>
<p>返回false 信息。<br>执行report_task_to_initiator(), 查询task信息，如task.f_federated_status_collect_typePUSH 架构下，调用FederatedScheduler.report_task_to_initiator() 主动推送信息。<br>对应的fate_flow/peewee.log日志为</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[DEBUG] [2021-07-26 08:20:32,196] [1:140259119585024] - peewee.py[line:2863]: (&#x27;SELECT `t1`.`f_create_time`, `t1`.`f_create_date`, `t1`.`f_update_time`, `t1`.`f_update_date`, `t1`.`f_job_id`, `t1`.`f_component_name`, `t1`.`f_task_id`, `t1`.`f_task_version`, `t1`.`f_initiator_role`, `t1`.`f_initiator_party_id`, `t1`.`f_federated_mode`, `t1`.`f_federated_status_collect_type`, `t1`.`f_status`, `t1`.`f_status_code`, `t1`.`f_role`, `t1`.`f_party_id`, `t1`.`f_run_on_this_party`, `t1`.`f_run_ip`, `t1`.`f_run_pid`, `t1`.`f_party_status`, `t1`.`f_start_time`, `t1`.`f_start_date`, `t1`.`f_end_time`, `t1`.`f_end_date`, `t1`.`f_elapsed` FROM `t_task` AS `t1` WHERE ((((`t1`.`f_task_id` = %s) AND (`t1`.`f_task_version` = %s)) AND (`t1`.`f_role` = %s)) AND (`t1`.`f_party_id` = %s))&#x27;, [&#x27;202107260820309976351_upload_0&#x27;, 0, &#x27;local&#x27;, &#x27;0&#x27;])</span><br></pre></td></tr></table></figure>
<p>前文所述，这里是PULL，故不会执行。直接返回状态给app_party.py 对应28</p>
</li>
<li><p>party_app.py：返回执行失败信息，对应27</p>
</li>
<li>fate_flow_server.py：返回失败信息给api_utils.py 对应26</li>
<li>api_utils.py：接受失败信息，在${job_log_dir}/fate_flow_audit.log  输出如下日志<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,203] [1:140259369826048] - api_utils.py[line:129]: &#123;&quot;retcode&quot;:103,&quot;retmsg&quot;:&quot;update task status failed&quot;&#125;</span><br><span class="line"></span><br><span class="line">[INFO] [2021-07-26 08:20:32,203] [1:140259369826048] - api_utils.py[line:131]: remote http api response: /v1/party/202107260820309976351/upload_0/202107260820309976351_upload_0/0/local/0/status/running &#123;&#x27;retcode&#x27;: 103, &#x27;retmsg&#x27;: &#x27;update task status failed&#x27;&#125;</span><br></pre></td></tr></table></figure>
对应的容器日志为<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">10.200.96.235 - - [26/Jul/2021 08:20:32] &quot;POST /v1/party/202107260820309976351/upload_0/202107260820309976351_upload_0/0/local/0/status/running HTTP/1.1&quot; 200 -</span><br></pre></td></tr></table></figure>
并返回结果给federated_scheduler.py 对应25</li>
<li><p>federated_scheduler.py：在${job_log_dir}/fate_flow_schedule.log 输出日志，并返回信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[WARNING] [2021-07-26 08:20:32,203] [1:140259369826048] - federated_scheduler.py[line:260]: an error occurred while status/running the task to role local party 0: </span><br><span class="line">update task status failed</span><br><span class="line">[INFO] [2021-07-26 08:20:32,203] [1:140259369826048] - federated_scheduler.py[line:197]: sync job 202107260820309976351 task 202107260820309976351_upload_0 0 status running to all party failed: </span><br><span class="line">&#123;&#x27;local&#x27;: &#123;0: &#123;&#x27;retcode&#x27;: 103, &#x27;retmsg&#x27;: &#x27;update task status failed&#x27;&#125;&#125;&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>task_scheduler.py：调用FederatedScheduler.start_task() 启动多方任务</p>
</li>
<li><p>federated_scheduler.py：执行start_task()<br>发起http请求 ${job_log_dir}/fate_flow_audit.log</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,205] [1:140259369826048] - api_utils.py[line:122]: remote http api request: http://10.200.96.235:9380/v1/party/202107260820309976351/upload_0/202107260820309976351_upload_0/0/local/0/start</span><br></pre></td></tr></table></figure>
</li>
<li><p>fate_flow_server.py：接受请求，通过flask，转移到party_app</p>
</li>
<li><p>party_app.py： 执行task_status，调用TaskController.start_task<br>这里不论调用结果，返回的都是success。</p>
</li>
<li><p>task_controller.py：执行start_task。<br>先调用job_utils.get_job_dsl() 获取job的dsl。这里需要查询DB，产生fate_flow/peewee.log日志</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[DEBUG] [2021-07-26 08:20:32,212] [1:140259119585024] - peewee.py[line:2863]: (&#x27;SELECT `t1`.`f_dsl` FROM `t_job` AS `t1` WHERE (((`t1`.`f_job_id` = %s) AND (`t1`.`f_role` = %s)) AND (`t1`.`f_party_id` = %s))&#x27;, [&#x27;202107260820309976351&#x27;, &#x27;local&#x27;, &#x27;0&#x27;])</span><br></pre></td></tr></table></figure>
<p>鉴权，（接上文，本job无鉴权，故不会执行）。<br>输出日志到${job_log_dir}/fate_flow_schedule.log</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,214] [1:140259119585024] - task_controller.py[line:71]: try to start job 202107260820309976351 task 202107260820309976351_upload_0 0 on local 0 executor subprocess</span><br></pre></td></tr></table></figure>
<p>生成task等目录，并创建。将task_parameters_path 写入对应文件。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">task_dir = os.path.join(job_utils.get_job_directory(job_id=job_id), role, party_id, component_name, task_id, task_version)</span><br><span class="line">/data/projects/fate/jobs/202107260820309976351/local/0/upload_0/202107260820309976351_upload_0/0/</span><br><span class="line"></span><br><span class="line">task_parameters_path = os.path.join(task_dir, &#x27;task_parameters.json&#x27;)</span><br><span class="line">run_parameters_dict = job_utils.get_job_parameters(job_id, role, party_id)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>获取run_parameters，对应fate_flow/peewee.log日志<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[DEBUG] [2021-07-26 08:20:32,216] [1:140259119585024] - peewee.py[line:2863]: (&#x27;SELECT `t1`.`f_runtime_conf_on_party` FROM `t_job` AS `t1` WHERE (((`t1`.`f_job_id` = %s) AND (`t1`.`f_role` = %s)) AND (`t1`.`f_party_id` = %s))&#x27;, [&#x27;202107260820309976351&#x27;, &#x27;local&#x27;, &#x27;0&#x27;])</span><br></pre></td></tr></table></figure></p>
<p>输出日志：${job_log_dir}/fate_flow_schedule.log<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,218] [1:140259119585024] - task_controller.py[line:90]: use computing engine EGGROLL</span><br></pre></td></tr></table></figure></p>
<p>根据computing engine，生成process_cmd。这里可以看下源码，比较有意思<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">if run_parameters.computing_engine in &#123;ComputingEngine.EGGROLL, ComputingEngine.STANDALONE&#125;:</span><br><span class="line">    process_cmd = [</span><br><span class="line">        sys.executable,</span><br><span class="line">        sys.modules[TaskExecutor.__module__].__file__,</span><br><span class="line">        &#x27;-j&#x27;, job_id,</span><br><span class="line">        &#x27;-n&#x27;, component_name,</span><br><span class="line">        &#x27;-t&#x27;, task_id,</span><br><span class="line">        &#x27;-v&#x27;, task_version,</span><br><span class="line">        &#x27;-r&#x27;, role,</span><br><span class="line">        &#x27;-p&#x27;, party_id,</span><br><span class="line">        &#x27;-c&#x27;, task_parameters_path,</span><br><span class="line">        &#x27;--run_ip&#x27;, RuntimeConfig.JOB_SERVER_HOST,</span><br><span class="line">        &#x27;--job_server&#x27;, &#x27;&#123;&#125;:&#123;&#125;&#x27;.format(RuntimeConfig.JOB_SERVER_HOST, RuntimeConfig.HTTP_PORT),</span><br><span class="line">    ]</span><br><span class="line">elif run_parameters.computing_engine == ComputingEngine.SPARK:</span><br><span class="line">    if &quot;SPARK_HOME&quot; not in os.environ:</span><br><span class="line">        raise EnvironmentError(&quot;SPARK_HOME not found&quot;)</span><br><span class="line">    spark_home = os.environ[&quot;SPARK_HOME&quot;]</span><br><span class="line"></span><br><span class="line">    # additional configs</span><br><span class="line">    spark_submit_config = run_parameters.spark_run</span><br><span class="line"></span><br><span class="line">    deploy_mode = spark_submit_config.get(&quot;deploy-mode&quot;, &quot;client&quot;)</span><br><span class="line">    if deploy_mode not in [&quot;client&quot;]:</span><br><span class="line">        raise ValueError(f&quot;deploy mode &#123;deploy_mode&#125; not supported&quot;)</span><br><span class="line"></span><br><span class="line">    spark_submit_cmd = os.path.join(spark_home, &quot;bin/spark-submit&quot;)</span><br><span class="line">    process_cmd = [spark_submit_cmd, f&#x27;--name=&#123;task_id&#125;#&#123;role&#125;&#x27;]</span><br><span class="line">    for k, v in spark_submit_config.items():</span><br><span class="line">        if k != &quot;conf&quot;:</span><br><span class="line">            process_cmd.append(f&#x27;--&#123;k&#125;=&#123;v&#125;&#x27;)</span><br><span class="line">    if &quot;conf&quot; in spark_submit_config:</span><br><span class="line">        for ck, cv in spark_submit_config[&quot;conf&quot;].items():</span><br><span class="line">            process_cmd.append(f&#x27;--conf&#x27;)</span><br><span class="line">            process_cmd.append(f&#x27;&#123;ck&#125;=&#123;cv&#125;&#x27;)</span><br><span class="line">    process_cmd.extend([</span><br><span class="line">        sys.modules[TaskExecutor.__module__].__file__,</span><br><span class="line">        &#x27;-j&#x27;, job_id,</span><br><span class="line">        &#x27;-n&#x27;, component_name,</span><br><span class="line">        &#x27;-t&#x27;, task_id,</span><br><span class="line">        &#x27;-v&#x27;, task_version,</span><br><span class="line">        &#x27;-r&#x27;, role,</span><br><span class="line">        &#x27;-p&#x27;, party_id,</span><br><span class="line">        &#x27;-c&#x27;, task_parameters_path,</span><br><span class="line">        &#x27;--run_ip&#x27;, RuntimeConfig.JOB_SERVER_HOST,</span><br><span class="line">        &#x27;--job_server&#x27;, &#x27;&#123;&#125;:&#123;&#125;&#x27;.format(RuntimeConfig.JOB_SERVER_HOST, RuntimeConfig.HTTP_PORT),</span><br><span class="line">    ])</span><br><span class="line">else:</span><br><span class="line">    raise ValueError(f&quot;$&#123;run_parameters.computing_engine&#125; is not supported&quot;)</span><br></pre></td></tr></table></figure></p>
<p><strong>注：spark 只支持client模式</strong><br><strong>注：spark 的submint 是 $sparkhome/bin/spark-submit</strong><br>建立task的日志目录<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">task_log_dir = os.path.join(job_utils.get_job_log_directory(job_id=job_id), role, party_id, component_name)</span><br><span class="line">对应</span><br><span class="line">local/0/upload_0/</span><br></pre></td></tr></table></figure><br>输出日志：${job_log_dir}/fate_flow_schedule.log<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,218] [1:140259119585024] - task_controller.py[line:144]: job 202107260820309976351 task 202107260820309976351_upload_0 0 on local 0 executor subprocess is ready</span><br></pre></td></tr></table></figure><br>调用job_utils.run_subprocess() 真正执行的部分来了</p>
<ol>
<li>job_utils.py：执行run_subprocess<br>先输出日志：${job_log_dir}/fate_flow_schedule.log<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,218] [1:140259119585024] - job_utils.py[line:310]: start process command: /opt/app-root/bin/python /data/projects/fate/python/fate_flow/operation/task_executor.py -j 202107260820309976351 -n upload_0 -t 202107260820309976351_upload_0 -v 0 -r local -p 0 -c /data/projects/fate/jobs/202107260820309976351/local/0/upload_0/202107260820309976351_upload_0/0/task_parameters.json --run_ip 10.200.96.235 --job_server 10.200.96.235:9380</span><br></pre></td></tr></table></figure>
这里会把真实执行的命令打出。<br><strong>注：小技巧，本地debug 可以直接执行这条命令，这也是真正的入口，可以参考文档<a href="https://github.com/FederatedAI/DOC-CHN/blob/master/%E6%9C%89%E5%A5%96%E5%BE%81%E9%9B%86%E6%B4%BB%E5%8A%A8/%E6%95%99%E7%A8%8B%E7%B1%BB/Mac%E4%B8%8B%E4%BD%BF%E7%94%A8Pycharm/MAC%E4%B8%8B%E4%BD%BF%E7%94%A8PyCharm%E8%BF%9B%E8%A1%8C%E5%BC%80%E5%8F%91%E5%92%8C%E8%B0%83%E8%AF%95fate.md">Mac下使用Pycharm</a></strong></li>
</ol>
<p>获取相关目录<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">os.makedirs(config_dir, exist_ok=True)</span><br><span class="line">if log_dir:</span><br><span class="line">    os.makedirs(log_dir, exist_ok=True)</span><br><span class="line">std_log = open(os.path.join(log_dir if log_dir else config_dir, &#x27;std.log&#x27;), &#x27;w&#x27;)</span><br><span class="line">pid_path = os.path.join(config_dir, &#x27;pid&#x27;)</span><br></pre></td></tr></table></figure><br>判断操作系统。<br>执行cmd<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">p = subprocess.Popen(process_cmd,</span><br><span class="line">                         stdout=std_log,</span><br><span class="line">                         stderr=std_log,</span><br><span class="line">                         startupinfo=startupinfo</span><br><span class="line">                         )</span><br></pre></td></tr></table></figure><br>并将pid 写入对应目录。<br>打出日志${job_log_dir}/fate_flow_schedule.log<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,258] [1:140259119585024] - job_utils.py[line:333]: start process command: /opt/app-root/bin/python /data/projects/fate/python/fate_flow/operation/task_executor.py -j 202107260820309976351 -n upload_0 -t 202107260820309976351_upload_0 -v 0 -r local -p 0 -c /data/projects/fate/jobs/202107260820309976351/local/0/upload_0/202107260820309976351_upload_0/0/task_parameters.json --run_ip 10.200.96.235 --job_server 10.200.96.235:9380 successfully, pid is 90</span><br></pre></td></tr></table></figure><br>至此 schedule 结束，并返回进程给task_controller.py 对应38</p>
<ol>
<li><p>task_controller.py：接收38返回值，更新task变量状态。在finally部分，调用update_task，update_task_status更新DB中task状态。两个方法均会调用job_saver.py 的对应方法。</p>
</li>
<li><p>task_controller.py：update_task调用执行job_saver.update_task()<br>先打印日志${job_log_dir}/fate_flow_schedule.log</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,259] [1:140259119585024] - job_saver.py[line:81]: try to update job 202107260820309976351 task 202107260820309976351_upload_0 0</span><br></pre></td></tr></table></figure>
<p><strong>注：这里和update_task_status 相比，少了一个status（参考步骤20）</strong><br>执行update_entity_table()，先查询，再执行update，对应fate_flow/peewee.log日志为</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[DEBUG] [2021-07-26 08:20:32,263] [1:140259119585024] - peewee.py[line:2863]: (&#x27;SELECT `t1`.`f_create_time`, `t1`.`f_create_date`, `t1`.`f_update_time`, `t1`.`f_update_date`, `t1`.`f_job_id`, `t1`.`f_component_name`, `t1`.`f_task_id`, `t1`.`f_task_version`, `t1`.`f_initiator_role`, `t1`.`f_initiator_party_id`, `t1`.`f_federated_mode`, `t1`.`f_federated_status_collect_type`, `t1`.`f_status`, `t1`.`f_status_code`, `t1`.`f_role`, `t1`.`f_party_id`, `t1`.`f_run_on_this_party`, `t1`.`f_run_ip`, `t1`.`f_run_pid`, `t1`.`f_party_status`, `t1`.`f_start_time`, `t1`.`f_start_date`, `t1`.`f_end_time`, `t1`.`f_end_date`, `t1`.`f_elapsed` FROM `t_task` AS `t1` WHERE (((((`t1`.`f_job_id` = %s) AND (`t1`.`f_task_id` = %s)) AND (`t1`.`f_task_version` = %s)) AND (`t1`.`f_role` = %s)) AND (`t1`.`f_party_id` = %s))&#x27;, [&#x27;202107260820309976351&#x27;, &#x27;202107260820309976351_upload_0&#x27;, 0, &#x27;local&#x27;, &#x27;0&#x27;])</span><br><span class="line">[DEBUG] [2021-07-26 08:20:32,270] [1:140259119585024] - peewee.py[line:2863]: (&#x27;UPDATE `t_task` SET `f_start_time` = %s, `f_start_date` = %s WHERE (((((`t_task`.`f_job_id` = %s) AND (`t_task`.`f_task_id` = %s)) AND (`t_task`.`f_task_version` = %s)) AND (`t_task`.`f_role` = %s)) AND (`t_task`.`f_party_id` = %s))&#x27;, [1627287632259, datetime.datetime(2021, 7, 26, 8, 20, 32), &#x27;202107260820309976351&#x27;, &#x27;202107260820309976351_upload_0&#x27;, 0, &#x27;local&#x27;, &#x27;0&#x27;])</span><br></pre></td></tr></table></figure>
<p>update成功，输出日志${job_log_dir}/fate_flow_schedule.log</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,275] [1:140259119585024] - job_saver.py[line:84]: job 202107260820309976351 task 202107260820309976351_upload_0 0 update successfully</span><br></pre></td></tr></table></figure>
<p>执行 report_task_to_initiator，同29中，只执行了query，不发起请求。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[DEBUG] [2021-07-26 08:20:32,277] [1:140259119585024] - peewee.py[line:2863]: (&#x27;SELECT `t1`.`f_create_time`, `t1`.`f_create_date`, `t1`.`f_update_time`, `t1`.`f_update_date`, `t1`.`f_job_id`, `t1`.`f_component_name`, `t1`.`f_task_id`, `t1`.`f_task_version`, `t1`.`f_initiator_role`, `t1`.`f_initiator_party_id`, `t1`.`f_federated_mode`, `t1`.`f_federated_status_collect_type`, `t1`.`f_status`, `t1`.`f_status_code`, `t1`.`f_role`, `t1`.`f_party_id`, `t1`.`f_run_on_this_party`, `t1`.`f_run_ip`, `t1`.`f_run_pid`, `t1`.`f_party_status`, `t1`.`f_start_time`, `t1`.`f_start_date`, `t1`.`f_end_time`, `t1`.`f_end_date`, `t1`.`f_elapsed` FROM `t_task` AS `t1` WHERE ((((`t1`.`f_task_id` = %s) AND (`t1`.`f_task_version` = %s)) AND (`t1`.`f_role` = %s)) AND (`t1`.`f_party_id` = %s))&#x27;, [&#x27;202107260820309976351_upload_0&#x27;, 0, &#x27;local&#x27;, &#x27;0&#x27;])</span><br></pre></td></tr></table></figure>
<p>返回update_status</p>
</li>
<li><p>task_controller.py：update_task_status调用job_saver.update_task_status()<br>过程参照22，对应fate_flow/peewee.log日志</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[DEBUG] [2021-07-26 08:20:32,283] [1:140259119585024] - peewee.py[line:2863]: (&#x27;SELECT `t1`.`f_create_time`, `t1`.`f_create_date`, `t1`.`f_update_time`, `t1`.`f_update_date`, `t1`.`f_job_id`, `t1`.`f_component_name`, `t1`.`f_task_id`, `t1`.`f_task_version`, `t1`.`f_initiator_role`, `t1`.`f_initiator_party_id`, `t1`.`f_federated_mode`, `t1`.`f_federated_status_collect_type`, `t1`.`f_status`, `t1`.`f_status_code`, `t1`.`f_role`, `t1`.`f_party_id`, `t1`.`f_run_on_this_party`, `t1`.`f_run_ip`, `t1`.`f_run_pid`, `t1`.`f_party_status`, `t1`.`f_start_time`, `t1`.`f_start_date`, `t1`.`f_end_time`, `t1`.`f_end_date`, `t1`.`f_elapsed` FROM `t_task` AS `t1` WHERE (((((`t1`.`f_job_id` = %s) AND (`t1`.`f_task_id` = %s)) AND (`t1`.`f_task_version` = %s)) AND (`t1`.`f_role` = %s)) AND (`t1`.`f_party_id` = %s))&#x27;, [&#x27;202107260820309976351&#x27;, &#x27;202107260820309976351_upload_0&#x27;, 0, &#x27;local&#x27;, &#x27;0&#x27;])</span><br><span class="line">[DEBUG] [2021-07-26 08:20:32,289] [1:140259119585024] - peewee.py[line:2863]: (&#x27;UPDATE `t_task` SET `f_party_status` = %s WHERE ((((((`t_task`.`f_job_id` = %s) AND (`t_task`.`f_task_id` = %s)) AND (`t_task`.`f_task_version` = %s)) AND (`t_task`.`f_role` = %s)) AND (`t_task`.`f_party_id` = %s)) AND (`t_task`.`f_party_status` = %s))&#x27;, [&#x27;running&#x27;, &#x27;202107260820309976351&#x27;, &#x27;202107260820309976351_upload_0&#x27;, 0, &#x27;local&#x27;, &#x27;0&#x27;, &#x27;waiting&#x27;])</span><br></pre></td></tr></table></figure>
<p>对应${job_log_dir}/fate_flow_schedule.log</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,281] [1:140259119585024] - job_saver.py[line:71]: try to update job 202107260820309976351 task 202107260820309976351_upload_0 0 status</span><br><span class="line">[INFO] [2021-07-26 08:20:32,295] [1:140259119585024] - job_saver.py[line:74]: update job 202107260820309976351 task 202107260820309976351_upload_0 0 status successfully: &#123;&#x27;job_id&#x27;: &#x27;202107260820309976351&#x27;, &#x27;task_id&#x27;: &#x27;202107260820309976351_upload_0&#x27;, &#x27;task_version&#x27;: &#x27;0&#x27;, &#x27;role&#x27;: &#x27;local&#x27;, &#x27;party_id&#x27;: &#x27;0&#x27;, &#x27;party_status&#x27;: &#x27;running&#x27;, &#x27;start_time&#x27;: 1627287632259&#125;</span><br></pre></td></tr></table></figure>
<p><strong>注：29失败，这里成功,因为字段不同，29是f_status，这里是f_party_status</strong><br>执行 report_task_to_initiator，同29中，只执行了query，不发起请求。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[DEBUG] [2021-07-26 08:20:32,300] [1:140259119585024] - peewee.py[line:2863]: (&#x27;SELECT `t1`.`f_create_time`, `t1`.`f_create_date`, `t1`.`f_update_time`, `t1`.`f_update_date`, `t1`.`f_job_id`, `t1`.`f_component_name`, `t1`.`f_task_id`, `t1`.`f_task_version`, `t1`.`f_initiator_role`, `t1`.`f_initiator_party_id`, `t1`.`f_federated_mode`, `t1`.`f_federated_status_collect_type`, `t1`.`f_status`, `t1`.`f_status_code`, `t1`.`f_role`, `t1`.`f_party_id`, `t1`.`f_run_on_this_party`, `t1`.`f_run_ip`, `t1`.`f_run_pid`, `t1`.`f_party_status`, `t1`.`f_start_time`, `t1`.`f_start_date`, `t1`.`f_end_time`, `t1`.`f_end_date`, `t1`.`f_elapsed` FROM `t_task` AS `t1` WHERE ((((`t1`.`f_task_id` = %s) AND (`t1`.`f_task_version` = %s)) AND (`t1`.`f_role` = %s)) AND (`t1`.`f_party_id` = %s))&#x27;, [&#x27;202107260820309976351_upload_0&#x27;, 0, &#x27;local&#x27;, &#x27;0&#x27;])</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p><strong>注：对比22 和 29 可以发现，job_saver 不发起report ,task_control 中的方法有report</strong></p>
<ol>
<li><p>task_controller.py：执行完毕，无异常，在${job_log_dir}/fate_flow_schedule.log输出日志</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,304] [1:140259119585024] - task_controller.py[line:163]: job 202107260820309976351 task 202107260820309976351_upload_0 0 on local 0 executor subprocess start success</span><br></pre></td></tr></table></figure>
</li>
<li><p>party_app.py： 对应37，返回结果。这里不论上文的执行情况，返回的都是success。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">TaskController.create_task(role, party_id, True, request.json)</span><br><span class="line">return get_json_result(retcode=0, retmsg=&#x27;success&#x27;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>fate_flow_server.py：返回结果给api_utils.py 对应36</p>
</li>
<li><p>api_utils.py：接受信息，在${job_log_dir}/fate_flow_audit.log  输出如下日志</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,310] [1:140259369826048] - api_utils.py[line:129]: &#123;&quot;retcode&quot;:0,&quot;retmsg&quot;:&quot;success&quot;&#125;</span><br><span class="line"></span><br><span class="line">[INFO] [2021-07-26 08:20:32,311] [1:140259369826048] - api_utils.py[line:131]: remote http api response: /v1/party/202107260820309976351/upload_0/202107260820309976351_upload_0/0/local/0/start &#123;&#x27;retcode&#x27;: 0, &#x27;retmsg&#x27;: &#x27;success&#x27;&#125;</span><br></pre></td></tr></table></figure>
<p>对应的容器日志为</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">10.200.96.235 - - [26/Jul/2021 08:20:32] &quot;POST /v1/party/202107260820309976351/upload_0/202107260820309976351_upload_0/0/local/0/start HTTP/1.1&quot; 200 -</span><br></pre></td></tr></table></figure>
<p>并返回结果给federated_scheduler.py 对应35</p>
</li>
<li><p>federated_scheduler.py：返回结果给task_scheduler.py，对应34</p>
</li>
<li>task_scheduler.py：status_code 为SUCCESS，返回SchedulingStatusCode.SUCCESS，对应16。</li>
<li>task_scheduler.py：对应16，依次遍历，完成本轮该job下task的调度，并在${job_log_dir}/fate_flow_schedule.log输出日志<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,311] [1:140259369826048] - task_scheduler.py[line:75]: finish scheduling job 202107260820309976351 tasks</span><br></pre></td></tr></table></figure>
<strong>注：这里不是一次性调度该job下的所有task，当task的前置依赖task不满足时，会跳出循环，等下一次轮询</strong></li>
<li>task_scheduler.py：返回各task的当前状态给dag_scheduler.py 对应5</li>
<li>dag_scheduler.py：调用calculate_job_status() 计算当前job状态，如果收到cancel信号，且job处于waiting，将状态置为canceled。<br>计算当前的进度。完成的task/总task（吐槽，无视了各个task的耗时）<br>${job_log_dir}/fate_flow_schedule.log输出日志<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,312] [1:140259369826048] - dag_scheduler.py[line:310]: Job 202107260820309976351 status is running, calculate by task status list: [&#x27;running&#x27;]</span><br></pre></td></tr></table></figure></li>
<li>dag_scheduler.py：根据job状态和进度变化情况，向多方同步相关信息。</li>
<li>dag_scheduler.py：整个schedule_running_job结束。${job_log_dir}/fate_flow_schedule.log输出日志<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,312] [1:140259369826048] - dag_scheduler.py[line:325]: finish scheduling job 202107260820309976351</span><br></pre></td></tr></table></figure>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2023/08/20/fate-log-5-upload-task-schedule/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="喵十八">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="喵十八の小窝">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/08/20/fate-log-5-upload-task-schedule/" class="post-title-link" itemprop="url">FATE学习：跟着日志读源码（五）upload任务job schedule阶段</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2023-08-20 12:11:32 / Modified: 13:31:04" itemprop="dateCreated datePublished" datetime="2023-08-20T12:11:32+08:00">2023-08-20</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/FATE/" itemprop="url" rel="index"><span itemprop="name">FATE</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h1><p>由于是异步提交的，job启动是由server 进行轮询，对于处于waiting的 job 按照FIFO进行调度。<br>调度之后的操作有2部分：</p>
<ul>
<li>Part1. 申请资源 </li>
<li>Part2. start job：将job的状态从waiting -&gt; running 状态</li>
</ul>
<p>涉及的主要方法： dag_scheduler.schedule_waiting_jobs()</p>
<h1 id="执行细节"><a href="#执行细节" class="headerlink" title="执行细节"></a>执行细节</h1><p><img src="/image/fate/84129cf514ec4762a73cc32d06f34b74.png" alt="在这里插入图片描述"></p>
<ol>
<li>dag_scheduler.py：轮询，发现处于waiting状态的job（这一部分见fate flow server 启动部分），调用schedule_waiting_jobs 调度处于waiting 状态的job。<br>轮询日志输出在 fate_flow/fate_flow_schedule.log 中<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:31,994] [1:140259369826048] - dag_scheduler.py[line:134]: start schedule waiting jobs</span><br><span class="line">[INFO] [2021-07-26 08:20:32,007] [1:140259369826048] - dag_scheduler.py[line:136]: have 1 waiting jobs</span><br><span class="line">[INFO] [2021-07-26 08:20:32,008] [1:140259369826048] - dag_scheduler.py[line:140]: schedule waiting job 202107260820309976351</span><br><span class="line">[INFO] [2021-07-26 08:20:32,117] [1:140259369826048] - dag_scheduler.py[line:146]: schedule waiting jobs finished</span><br></pre></td></tr></table></figure>
对应查询waiting状态的fate_flow/peewee.log为<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[DEBUG] [2021-07-26 08:20:32,001] [1:140259369826048] - peewee.py[line:2863]: (&#x27;SELECT `t1`.`f_create_time`, `t1`.`f_create_date`, `t1`.`f_update_time`, `t1`.`f_update_date`, `t1`.`f_user_id`, `t1`.`f_job_id`, `t1`.`f_name`, `t1`.`f_description`, `t1`.`f_tag`, `t1`.`f_dsl`, `t1`.`f_runtime_conf`, `t1`.`f_runtime_conf_on_party`, `t1`.`f_train_runtime_conf`, `t1`.`f_roles`, `t1`.`f_work_mode`, `t1`.`f_initiator_role`, `t1`.`f_initiator_party_id`, `t1`.`f_status`, `t1`.`f_status_code`, `t1`.`f_role`, `t1`.`f_party_id`, `t1`.`f_is_initiator`, `t1`.`f_progress`, `t1`.`f_ready_signal`, `t1`.`f_ready_time`, `t1`.`f_cancel_signal`, `t1`.`f_cancel_time`, `t1`.`f_rerun_signal`, `t1`.`f_end_scheduling_updates`, `t1`.`f_engine_name`, `t1`.`f_engine_type`, `t1`.`f_cores`, `t1`.`f_memory`, `t1`.`f_remaining_cores`, `t1`.`f_remaining_memory`, `t1`.`f_resource_in_use`, `t1`.`f_apply_resource_time`, `t1`.`f_return_resource_time`, `t1`.`f_start_time`, `t1`.`f_start_date`, `t1`.`f_end_time`, `t1`.`f_end_date`, `t1`.`f_elapsed` FROM `t_job` AS `t1` WHERE ((`t1`.`f_is_initiator` = %s) AND (`t1`.`f_status` = %s)) ORDER BY `t1`.`f_create_time` ASC&#x27;, [True, &#x27;waiting&#x27;])</span><br></pre></td></tr></table></figure></li>
<li>dag_scheduler.py： 将job ready_signal 置为true后，调用FederatedScheduler.resource_for_job() 申请资源<br>ready_signal 部分的代码为<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">if not cls.ready_signal(job_id=job_id, set_or_reset=True):</span><br></pre></td></tr></table></figure>
会在fate_flow/peewee.log 中输出<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[DEBUG] [2021-07-26 08:20:32,009] [1:140259369826048] - peewee.py[line:2863]: (&#x27;UPDATE `t_job` SET `f_ready_signal` = %s, `f_ready_time` = %s WHERE ((`t_job`.`f_job_id` = %s) AND (`t_job`.`f_ready_signal` = %s))&#x27;, [True, 1627287632009, &#x27;202107260820309976351&#x27;, False])</span><br></pre></td></tr></table></figure></li>
<li>federated_scheduler.py：开始申请资源，这里会在${job_log_dir}/fate_flow_schedule.log 打出日志，<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,022] [1:140259369826048] - federated_scheduler.py[line:39]: try to apply job 202107260820309976351 resource</span><br></pre></td></tr></table></figure>
然后根据入参，调用job_command发起请求，申请资源。</li>
<li><p>api_utils.py：如前文所述，本地的是http请求，故而调用的是ederated_coordination_on_http。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">url = &quot;http://&#123;&#125;:&#123;&#125;&#123;&#125;&quot;.format(host, port, endpoint)</span><br><span class="line">audit_logger(job_id).info(&#x27;remote http api request: &#123;&#125;&#x27;.format(url))</span><br><span class="line">action = getattr(requests, method.lower(), None)</span><br><span class="line">headers = HEADERS.copy()</span><br><span class="line">headers[&quot;dest-party-id&quot;] = str(dest_party_id)</span><br><span class="line">headers[&quot;src-party-id&quot;] = str(src_party_id)</span><br><span class="line">headers[&quot;src-role&quot;] = str(src_role)</span><br><span class="line">http_response = action(url=url, data=json_dumps(json_body), headers=headers)</span><br><span class="line">audit_logger(job_id).info(http_response.text)</span><br><span class="line">response = http_response.json()</span><br><span class="line">audit_logger(job_id).info(&#x27;remote http api response: &#123;&#125; &#123;&#125;&#x27;.format(endpoint, response))</span><br><span class="line">return response</span><br></pre></td></tr></table></figure>
<p>先生成请求的url。并在${job_log_dir}/fate_flow_audit.log 中打印出</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,023] [1:140259369826048] - api_utils.py[line:122]: remote http api request: http://10.200.96.235:9380/v1/party/202107260820309976351/local/0/resource/apply</span><br></pre></td></tr></table></figure>
</li>
<li><p>fate_flow_server.py：接收请求，因为endpoint 是party/<em>**</em>，通过flask机制，跳转至party_app</p>
</li>
<li>party_app.py：调用 ResourceManager.apply_for_job_resource() 申请资源</li>
<li>resource_manager.py：在resource_for_job()给job分配资源。主要操作为<br>-&gt;调用calculate_job_resource，查询表t_job，得到f_runtime_conf_on_party进一步计算得到engine_name，cores，memory<br>这里会在fate_flow/peewee.log<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[DEBUG] [2021-07-26 08:20:32,031] [1:140259119585024] - peewee.py[line:2863]: (&#x27;SELECT `t1`.`f_runtime_conf_on_party` FROM `t_job` AS `t1` WHERE (((`t1`.`f_job_id` = %s) AND (`t1`.`f_role` = %s)) AND (`t1`.`f_party_id` = %s))&#x27;, [&#x27;202107260820309976351&#x27;, &#x27;local&#x27;, &#x27;0&#x27;])</span><br></pre></td></tr></table></figure>
-&gt;获取相关信息后，更新t_job表<br>fate_flow/peewee.log 中有<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[DEBUG] [2021-07-26 08:20:32,037] [1:140259119585024] - peewee.py[line:2863]: (&#x27;UPDATE `t_job` SET `f_engine_name` = %s, `f_engine_type` = %s, `f_cores` = %s, `f_memory` = %s, `f_remaining_cores` = %s, `f_remaining_memory` = %s, `f_resource_in_use` = %s, `f_apply_resource_time` = %s WHERE ((((`t_job`.`f_job_id` = %s) AND (`t_job`.`f_role` = %s)) AND (`t_job`.`f_party_id` = %s)) AND (`t_job`.`f_resource_in_use` = %s))&#x27;, [&#x27;EGGROLL&#x27;, &#x27;computing&#x27;, 4, 0, 4, 0, True, 1627287632035, &#x27;202107260820309976351&#x27;, &#x27;local&#x27;, &#x27;0&#x27;, False])</span><br></pre></td></tr></table></figure>
-&gt; t_job表更新成功后，再更新t_engine_registry表<br>代码<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">operate = EngineRegistry.update(updates).where(*filters)</span><br><span class="line">apply_status = operate.execute() &gt; 0</span><br></pre></td></tr></table></figure>
fate_flow/peewee.log<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[DEBUG] [2021-07-26 08:20:32,040] [1:140259119585024] - peewee.py[line:2863]: (&#x27;UPDATE `t_engine_registry` SET `f_remaining_cores` = (`t_engine_registry`.`f_remaining_cores` - %s), `f_remaining_memory` = (`t_engine_registry`.`f_remaining_memory` - %s) WHERE ((((`t_engine_registry`.`f_remaining_cores` &gt;= %s) AND (`t_engine_registry`.`f_remaining_memory` &gt;= %s)) AND (`t_engine_registry`.`f_engine_type` = %s)) AND (`t_engine_registry`.`f_engine_name` = %s))&#x27;, [4, 0, 4, 0, &#x27;computing&#x27;, &#x27;EGGROLL&#x27;])</span><br></pre></td></tr></table></figure>
获取当前剩余的资源<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">remaining_cores, remaining_memory = cls.get_remaining_resource(EngineRegistry,</span><br><span class="line">                                                               [</span><br><span class="line">                                                                   EngineRegistry.f_engine_type == EngineType.COMPUTING,</span><br><span class="line">                                                                   EngineRegistry.f_engine_name == engine_name])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
对应fate_flow/peewee.log<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[DEBUG] [2021-07-26 08:20:32,046] [1:140259119585024] - peewee.py[line:2863]: (&#x27;SELECT `t1`.`f_remaining_cores`, `t1`.`f_remaining_memory` FROM `t_engine_registry` AS `t1` WHERE ((`t1`.`f_engine_type` = %s) AND (`t1`.`f_engine_name` = %s))&#x27;, [&#x27;computing&#x27;, &#x27;EGGROLL&#x27;])</span><br></pre></td></tr></table></figure>
若查到数据且一切正常，在${job_log_dir}/fate_flow_schedule.log中打出如下日志<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,049] [1:140259119585024] - resource_manager.py[line:175]: apply job 202107260820309976351 resource(cores 4 memory 0) on local 0 successfully, remaining cores: 16 remaining memory: 0</span><br></pre></td></tr></table></figure></li>
<li>resource_manager.py：返回资源申请结果</li>
<li>party_app.py： 返回response给fate_flow_server.py，对应5</li>
<li>fate_flow_server.py：返回response给api_utils.py，对应4<br>参考 4中的代码，这里会在fate_flow/fate_flow_audit.log中输出如下日志<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,052] [1:140259369826048] - api_utils.py[line:129]: &#123;&quot;retcode&quot;:0,&quot;retmsg&quot;:&quot;success&quot;&#125;</span><br><span class="line">[INFO] [2021-07-26 08:20:32,052] [1:140259369826048] - api_utils.py[line:131]: remote http api response: /v1/party/202107260820309976351/local/0/resource/apply &#123;&#x27;retcode&#x27;: 0, &#x27;retmsg&#x27;: &#x27;success&#x27;&#125;</span><br></pre></td></tr></table></figure></li>
<li>api_utils.py：返回response 给federated_scheduler.py，对应3</li>
<li>federated_scheduler.py： 根据response 结果输出日志到 ${job_log_dir}/fate_flow_schedule.log 中<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,053] [1:140259369826048] - federated_scheduler.py[line:42]: apply job 202107260820309976351 resource successfully</span><br></pre></td></tr></table></figure>
并返回资源分配结果给dag_scheduler.py，对应2</li>
<li>dag_scheduler.py：如资源申请成功，调用start_job启动job，<br>在${job_log_dir}/fate_flow_schedule.log 中输出日志<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,053] [1:140259369826048] - dag_scheduler.py[line:279]: try to start job 202107260820309976351 on initiator local 0</span><br></pre></td></tr></table></figure>
然后配置job_info 各项参数将status置为Runing，将tag置为end_waiting<br>在容器日志中也有<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">10.200.96.235 - - [26/Jul/2021 08:20:32] &quot;POST /v1/party/202107260820309976351/local/0/resource/apply HTTP/1.1&quot; 200 -</span><br></pre></td></tr></table></figure></li>
<li>dag_scheduler.py： 调用JobSaver.query_job查询db,确认该job在数据库中，就继续执行(骚操作，删db)<br>对应fate_flow/peewee.log 日志<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[DEBUG] [2021-07-26 08:20:32,058] [1:140259369826048] - peewee.py[line:2863]: (&#x27;SELECT `t1`.`f_create_time`, `t1`.`f_create_date`, `t1`.`f_update_time`, `t1`.`f_update_date`, `t1`.`f_user_id`, `t1`.`f_job_id`, `t1`.`f_name`, `t1`.`f_description`, `t1`.`f_tag`, `t1`.`f_dsl`, `t1`.`f_runtime_conf`, `t1`.`f_runtime_conf_on_party`, `t1`.`f_train_runtime_conf`, `t1`.`f_roles`, `t1`.`f_work_mode`, `t1`.`f_initiator_role`, `t1`.`f_initiator_party_id`, `t1`.`f_status`, `t1`.`f_status_code`, `t1`.`f_role`, `t1`.`f_party_id`, `t1`.`f_is_initiator`, `t1`.`f_progress`, `t1`.`f_ready_signal`, `t1`.`f_ready_time`, `t1`.`f_cancel_signal`, `t1`.`f_cancel_time`, `t1`.`f_rerun_signal`, `t1`.`f_end_scheduling_updates`, `t1`.`f_engine_name`, `t1`.`f_engine_type`, `t1`.`f_cores`, `t1`.`f_memory`, `t1`.`f_remaining_cores`, `t1`.`f_remaining_memory`, `t1`.`f_resource_in_use`, `t1`.`f_apply_resource_time`, `t1`.`f_return_resource_time`, `t1`.`f_start_time`, `t1`.`f_start_date`, `t1`.`f_end_time`, `t1`.`f_end_date`, `t1`.`f_elapsed` FROM `t_job` AS `t1` WHERE (((`t1`.`f_job_id` = %s) AND (`t1`.`f_role` = %s)) AND (`t1`.`f_party_id` = %s))&#x27;, [&#x27;202107260820309976351&#x27;, &#x27;local&#x27;, &#x27;0&#x27;])</span><br></pre></td></tr></table></figure>
这里对应的是submit阶段的insert语句。</li>
<li>dag_scheduler.py：调用FederatedScheduler.start_job() 启动任务</li>
<li>federated_scheduler.py：调用job_command  start job</li>
<li>federated_scheduler.py：由于是本地，向server发起http请求<br>类似第4步，在${job_log_dir}/fate_flow_audit.log中输出日志<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,063] [1:140259369826048] - api_utils.py[line:122]: remote http api request: http://10.200.96.235:9380/v1/party/202107260820309976351/local/0/start</span><br></pre></td></tr></table></figure></li>
<li>fate_flow_server.py：接收请求，因为endpoint 是party/<em>**</em>，通过flask机制，跳转至party_app通过Flask 机制，跳转至party_app</li>
<li>party_app.py： 调用JobController.start_job() 启动job</li>
<li>job_controller.py：打印日志<br>${job_log_dir}/fate_flow_schedule.log<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,053] [1:140259369826048] - dag_scheduler.py[line:279]: try to start job 202107260820309976351 on initiator local 0</span><br></pre></td></tr></table></figure>
并设置job_info，主要是将status 置为running,调用update_job_status 更新DB</li>
<li>job_controller.py：调用JobSaver.update_job_status</li>
<li>job_saver.py：执行update_job_status()<br>首先输出日志到${job_log_dir}/fate_flow_schedule.log<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,070] [1:140259119585024] - job_saver.py[line:45]: try to update job 202107260820309976351 status to running</span><br></pre></td></tr></table></figure>
调用update_status()，在update_status()内部，先查询db<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[DEBUG] [2021-07-26 08:20:32,074] [1:140259119585024] - peewee.py[line:2863]: (&#x27;SELECT `t1`.`f_create_time`, `t1`.`f_create_date`, `t1`.`f_update_time`, `t1`.`f_update_date`, `t1`.`f_user_id`, `t1`.`f_job_id`, `t1`.`f_name`, `t1`.`f_description`, `t1`.`f_tag`, `t1`.`f_dsl`, `t1`.`f_runtime_conf`, `t1`.`f_runtime_conf_on_party`, `t1`.`f_train_runtime_conf`, `t1`.`f_roles`, `t1`.`f_work_mode`, `t1`.`f_initiator_role`, `t1`.`f_initiator_party_id`, `t1`.`f_status`, `t1`.`f_status_code`, `t1`.`f_role`, `t1`.`f_party_id`, `t1`.`f_is_initiator`, `t1`.`f_progress`, `t1`.`f_ready_signal`, `t1`.`f_ready_time`, `t1`.`f_cancel_signal`, `t1`.`f_cancel_time`, `t1`.`f_rerun_signal`, `t1`.`f_end_scheduling_updates`, `t1`.`f_engine_name`, `t1`.`f_engine_type`, `t1`.`f_cores`, `t1`.`f_memory`, `t1`.`f_remaining_cores`, `t1`.`f_remaining_memory`, `t1`.`f_resource_in_use`, `t1`.`f_apply_resource_time`, `t1`.`f_return_resource_time`, `t1`.`f_start_time`, `t1`.`f_start_date`, `t1`.`f_end_time`, `t1`.`f_end_date`, `t1`.`f_elapsed` FROM `t_job` AS `t1` WHERE (((`t1`.`f_job_id` = %s) AND (`t1`.`f_role` = %s)) AND (`t1`.`f_party_id` = %s))&#x27;, [&#x27;202107260820309976351&#x27;, &#x27;local&#x27;, &#x27;0&#x27;])</span><br></pre></td></tr></table></figure>
根据获取信息，更新相关信息，然后调用execute_update()，执行sql更新DB<br>注意，在execute_update()这个方法里，会输出日志到${job_log_dir}/fate_flow_sql.log<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,081] [1:140259119585024] - job_saver.py[line:185]: UPDATE `t_job` SET `f_status` = &#x27;running&#x27; WHERE ((((`t_job`.`f_job_id` = &#x27;202107260820309976351&#x27;) AND (`t_job`.`f_role` = &#x27;local&#x27;)) AND (`t_job`.`f_party_id` = &#x27;0&#x27;)) AND (`t_job`.`f_status` = &#x27;waiting&#x27;))</span><br></pre></td></tr></table></figure>
对应的fate_flow/peewee.log 日志为<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[DEBUG] [2021-07-26 08:20:32,083] [1:140259119585024] - peewee.py[line:2863]: (&#x27;UPDATE `t_job` SET `f_status` = %s WHERE ((((`t_job`.`f_job_id` = %s) AND (`t_job`.`f_role` = %s)) AND (`t_job`.`f_party_id` = %s)) AND (`t_job`.`f_status` = %s))&#x27;, [&#x27;running&#x27;, &#x27;202107260820309976351&#x27;, &#x27;local&#x27;, &#x27;0&#x27;, &#x27;waiting&#x27;])</span><br></pre></td></tr></table></figure>
可以看到，就是将状态从waiting 更新为running。从时间戳上也能看出，先打sql.log，再底层调用peewee执行<br>update_status() 执行完毕，DB更新成功，会在${job_log_dir}/fate_flow_schedule.log中输出日志<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,088] [1:140259119585024] - job_saver.py[line:48]: update job 202107260820309976351 status successfully</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>注：若job的status属于EndStatus，会调用update_entity_table()</p>
<ol>
<li>job_saver.py：返回update_status()的执行状态给job_controllle.py 对应21</li>
<li><p>job_controlller.py：返回update_job_status()执行状态<br>注：若状态为true，会根据job status 判断是否回收资源，这个在start 阶段不用，但是后续finish 阶段会用</p>
</li>
<li><p>job_controlller.py： 调用update_job</p>
</li>
<li>job_controlller.py：调用JobSaver.update_job </li>
<li><p>job_saver.py：执行update_job()<br>输出日志到${job_log_dir}/fate_flow_schedule.log</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,089] [1:140259119585024] - job_saver.py[line:61]: try to update job 202107260820309976351</span><br></pre></td></tr></table></figure>
<p>调用update_entity_table，同22部分，先根据jobid 查现有更新，然后执行update</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[DEBUG] [2021-07-26 08:20:32,092] [1:140259119585024] - peewee.py[line:2863]: (&#x27;SELECT `t1`.`f_create_time`, `t1`.`f_create_date`, `t1`.`f_update_time`, `t1`.`f_update_date`, `t1`.`f_user_id`, `t1`.`f_job_id`, `t1`.`f_name`, `t1`.`f_description`, `t1`.`f_tag`, `t1`.`f_dsl`, `t1`.`f_runtime_conf`, `t1`.`f_runtime_conf_on_party`, `t1`.`f_train_runtime_conf`, `t1`.`f_roles`, `t1`.`f_work_mode`, `t1`.`f_initiator_role`, `t1`.`f_initiator_party_id`, `t1`.`f_status`, `t1`.`f_status_code`, `t1`.`f_role`, `t1`.`f_party_id`, `t1`.`f_is_initiator`, `t1`.`f_progress`, `t1`.`f_ready_signal`, `t1`.`f_ready_time`, `t1`.`f_cancel_signal`, `t1`.`f_cancel_time`, `t1`.`f_rerun_signal`, `t1`.`f_end_scheduling_updates`, `t1`.`f_engine_name`, `t1`.`f_engine_type`, `t1`.`f_cores`, `t1`.`f_memory`, `t1`.`f_remaining_cores`, `t1`.`f_remaining_memory`, `t1`.`f_resource_in_use`, `t1`.`f_apply_resource_time`, `t1`.`f_return_resource_time`, `t1`.`f_start_time`, `t1`.`f_start_date`, `t1`.`f_end_time`, `t1`.`f_end_date`, `t1`.`f_elapsed` FROM `t_job` AS `t1` WHERE (((`t1`.`f_job_id` = %s) AND (`t1`.`f_role` = %s)) AND (`t1`.`f_party_id` = %s))&#x27;, [&#x27;202107260820309976351&#x27;, &#x27;local&#x27;, &#x27;0&#x27;])</span><br><span class="line">[DEBUG] [2021-07-26 08:20:32,099] [1:140259119585024] - peewee.py[line:2863]: (&#x27;UPDATE `t_job` SET `f_start_time` = %s, `f_start_date` = %s WHERE (((`t_job`.`f_job_id` = %s) AND (`t_job`.`f_role` = %s)) AND (`t_job`.`f_party_id` = %s))&#x27;, [1627287632070, datetime.datetime(2021, 7, 26, 8, 20, 32), &#x27;202107260820309976351&#x27;, &#x27;local&#x27;, &#x27;0&#x27;])</span><br></pre></td></tr></table></figure>
<p>对应的${job_log_dir}/fate_flow_sql.log日志</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,097] [1:140259119585024] - job_saver.py[line:185]: UPDATE `t_job` SET `f_start_time` = 1627287632070, `f_start_date` = &#x27;2021-07-26 08:20:32&#x27; WHERE (((`t_job`.`f_job_id` = &#x27;202107260820309976351&#x27;) AND (`t_job`.`f_role` = &#x27;local&#x27;)) AND (`t_job`.`f_party_id` = &#x27;0&#x27;))</span><br></pre></td></tr></table></figure>
</li>
<li><p>job_saver.py：输出日志到${job_log_dir}/fate_flow_schedule.log</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,106] [1:140259119585024] - job_saver.py[line:64]: job 202107260820309976351 update successfully: &#123;&#x27;job_id&#x27;: &#x27;202107260820309976351&#x27;, &#x27;role&#x27;: &#x27;local&#x27;, &#x27;party_id&#x27;: 0, &#x27;status&#x27;: &#x27;running&#x27;, &#x27;start_time&#x27;: 1627287632070&#125;</span><br></pre></td></tr></table></figure>
<p>并返回update_status()的执行状态给job_controllle.py 对应26</p>
</li>
<li><p>job_controlller.py：start 结束， 输出 successfully 到${job_log_dir}/fate_flow_schedule.log</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,106] [1:140259119585024] - job_controller.py[line:250]: start job 202107260820309976351 on local 0 successfully</span><br></pre></td></tr></table></figure>
</li>
<li><p>dag_scheduler.py：若上述都无异常，在${job_log_dir}/fate_flow_schedule.log输出如下日志（对应13）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,110] [1:140259369826048] - dag_scheduler.py[line:292]: start job 202107260820309976351 on initiator local 0</span><br></pre></td></tr></table></figure>
</li>
<li><p>dag_scheduler.py：调用ready_singal，执行Job.updates，将ready_signal置为false<br>对应的fate_flow/peewee.log</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[DEBUG] [2021-07-26 08:20:32,111] [1:140259369826048] - peewee.py[line:2863]: (&#x27;UPDATE `t_job` SET `f_ready_signal` = %s, `f_ready_time` = %s WHERE ((`t_job`.`f_job_id` = %s) AND (`t_job`.`f_ready_signal` = %s))&#x27;, [False, None, &#x27;202107260820309976351&#x27;, True])</span><br></pre></td></tr></table></figure></li>
<li>dag_scheduler.py：在${job_log_dir}/fate_flow_schedule.log中打印31的执行结果<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:32,117] [1:140259369826048] - dag_scheduler.py[line:247]: reset job 202107260820309976351 ready signal True</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>至此，执行完毕，主要操作为更新DB中job的状态和resource的状态</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2023/08/20/fate-log-4-upload-submit/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="喵十八">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="喵十八の小窝">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/08/20/fate-log-4-upload-submit/" class="post-title-link" itemprop="url">FATE学习：跟着日志读源码（四）upload任务submit&create阶段</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2023-08-20 12:11:20 / Modified: 13:37:05" itemprop="dateCreated datePublished" datetime="2023-08-20T12:11:20+08:00">2023-08-20</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/FATE/" itemprop="url" rel="index"><span itemprop="name">FATE</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h1><p>本篇以upload任务的sumbit 和 create 阶段为例，结合产生的日志，说明该生命阶段代码的运行情况。</p>
<h1 id="执行步骤"><a href="#执行步骤" class="headerlink" title="执行步骤"></a>执行步骤</h1><p><img src="/image/fate/e3a8da03871b4ca5a7d5d35d09b2f1d2.png" alt="在这里插入图片描述"></p>
<p>为便于说明，画了下uml 时序图，结合图说下各步操作</p>
<ol>
<li>在CLI 用户执行命令 python fate_flow_client.py -f upload -c upload_guest.json</li>
<li><p>fate_flow_client.py:调用 call_fun()函数，向本地server 发起post请求</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">response = requests.post(&quot;/&quot;.join([server_url, &quot;data&quot;, func.replace(&#x27;_&#x27;, &#x27;/&#x27;)]), data=data,params=json.dumps(config_data),headers=&#123;&#x27;Content-Type&#x27;: data.content_type&#125;)</span><br></pre></td></tr></table></figure>
<p>这里会在容器日志中体现post请求日志</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">10.200.96.235 - - [26/Jul/2021 08:20:31] &quot;POST /v1/data/upload?%7B%22file%22:%20%22/data/projects/fate/examples/data/breast_hetero_guest.csv%22,%20%22head%22:%201,%20%22partition%22:%201,%20%22work_mode%22:%201,%20%22table_name%22:%20%22hetero_guest%22,%20%22namespace%22:%20%22cl%22,%20%22config%22:%20%22/data/projects/fate/cl/upload_guest.json%22,%20%22function%22:%20%22upload%22%7D HTTP/1.1&quot; 200 -</span><br></pre></td></tr></table></figure>
</li>
<li><p>fate_flow_server.py: 接受请求，Flask的调度，跳转至data_access_app.py</p>
</li>
<li>data_access_app.py: 执行download_upload() 函数，判断job_id 是否为空，若为空则调用job_utils.generate_job_id()</li>
<li>job_utils.py: 执行generate_job_id() 并返回ID</li>
<li>data_access_app.py: 根据request 中的相关参数，生成job_config</li>
<li>data_access_app.py: 调用detect_utils.check_config() 校验参数</li>
<li>detect_utils.py: 执行check_config() 如有异常抛出</li>
<li>data_access_app.py: 根据job_config，生成各项兼容性参数，如table_name,backnd 等</li>
<li>data_access_app.py: 初始化 StroageTableMeta并给data_table_meta赋值</li>
<li><em>table.py: 调用build 返回Meta，这里打出fate<em>flow/peewee.log 的第3行日志<br>这里在初始化 StorageTableMeta 的时候，会调用__new</em></em>() -&gt; query_table_meta() 从而在peewee 留下日志。<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[DEBUG] [2021-07-26 08:20:31,008] [1:140259119585024] - peewee.py[line:2863]: (&#x27;SELECT `t1`.`f_create_date`, `t1`.`f_update_date`, `t1`.`f_name`, `t1`.`f_namespace`, `t1`.`f_address`, `t1`.`f_engine`, `t1`.`f_type`, `t1`.`f_options`, `t1`.`f_partitions`, `t1`.`f_id_delimiter`, `t1`.`f_in_serialized`, `t1`.`f_have_head`, `t1`.`f_schema`, `t1`.`f_count`, `t1`.`f_part_of_data`, `t1`.`f_description`, `t1`.`f_create_time`, `t1`.`f_update_time` FROM `t_storage_table_meta` AS `t1` WHERE ((`t1`.`f_name` = %s) AND (`t1`.`f_namespace` = %s))&#x27;, [&#x27;hetero_guest&#x27;, &#x27;cl&#x27;])</span><br></pre></td></tr></table></figure></li>
<li>data_access_app.py: 校验tabel 不存在，或存在且drop 参数为1，调用gen_data_access_job_conf(job_config,access_module) 生成 job_dsl 和 job_runtime_conf</li>
<li>data_access_app.py: 调用DAGScheduler.submit({‘job_dsl’: job_dsl, job_runtime_conf’: job_runtime_conf}, job_id) 提交任务</li>
<li>dag_scheduler.py: 判断jobid 是否为空，空则重新生成</li>
<li><p>dag_scheduler.py: 打印出${job_log_dir}/fate_flow_schedule.log的第一行日志</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:31,014] [1:140259119585024] - dag_scheduler.py[line:40]: submit job, job_id 202107260820309976351, body &#123;&#x27;job_dsl&#x27;: &#123;&#x27;components&#x27;: &#123;&#x27;upload_0&#x27;: &#123;&#x27;module&#x27;: &#x27;Upload&#x27;&#125;&#125;&#125;, &#x27;job_runtime_conf&#x27;: &#123;&#x27;initiator&#x27;: &#123;&#x27;role&#x27;: &#x27;local&#x27;, &#x27;party_id&#x27;: 0&#125;, &#x27;job_parameters&#x27;: &#123;&#x27;common&#x27;: &#123;&#x27;backend&#x27;: 0, &#x27;work_mode&#x27;: 1&#125;&#125;, &#x27;role&#x27;: &#123;&#x27;local&#x27;: [0]&#125;, &#x27;component_parameters&#x27;: &#123;&#x27;role&#x27;: &#123;&#x27;local&#x27;: &#123;&#x27;0&#x27;: &#123;&#x27;upload_0&#x27;: &#123;&#x27;name&#x27;: &#x27;hetero_guest&#x27;, &#x27;head&#x27;: 1, &#x27;file&#x27;: &#x27;/data/projects/fate/jobs/202107260820309976351/fate_upload_tmp/breast_hetero_guest.csv&#x27;, &#x27;partition&#x27;: 1, &#x27;namespace&#x27;: &#x27;cl&#x27;, &#x27;destroy&#x27;: False&#125;&#125;&#125;&#125;&#125;, &#x27;dsl_version&#x27;: 2&#125;&#125;</span><br></pre></td></tr></table></figure>
<p>其中json部分是入参job_data，可以看到，整个body，包含两部分job_dsl 和 job_runtime_conf。job_dsl 定义了所使用的components,这里只有一个upload。<br>job_runtime_conf定义了整个job的相关参数，包括：     </p>
<ul>
<li>initiator: 调度者（或联邦任务的发起者）</li>
<li>job_parameters:job参数，主要是work_mode：是否为集群模式和 backend 计算引擎</li>
<li>role: 本次任务的参与角色</li>
<li>component_parameters: 位于各个role上的component 的参数，这里就是upload_0 的各项参数</li>
</ul>
</li>
<li><p>dag_scheduler.py: 从job_data中获取job_dsl 和 job_runtime_conf</p>
</li>
<li>dag_scheduler.py: 调用job_utils.check_job_config() 进行参数校验</li>
<li>job_utils.py: 确认各项必要参数都具备，且将party_id都强制转为int。如有异常，则raise。</li>
<li>dag_scheduler.py: 调用 authentication_utils.check_constraint() 校验约束</li>
<li>authentication_utils.py: 调用 check_component_constraint()，校验约束:不允许仅有arbiter 和guest，同在一个party_id, 除非host 也在该party_id</li>
<li>dag_scheduler.py: 配置initiator 和 conf_adapter</li>
<li>dag_scheduler.py: 根据job类型，配置train_runtime_conf</li>
<li>dag_scheduler.py: 新建job对象，并给对应参数赋值</li>
<li>dag_scheduler.py: 调用job_utils.save_job_conf() 生成job_dsl, job_runtime_conf,job_runtime_conf_on_party,train_runtime_conf,pipeline_dsl 各项的保存目录，并将相关参数以json格式落盘。</li>
<li>job_utils.py: 返回24中各项的目录</li>
<li>dag_scheduler.py: 校验initiator 在job_runtime_conf 列表里</li>
<li>dag_scheduler.py: 调用JobController.backend_compatibility() 设置计算引擎和是否集群模式</li>
<li>dag_scheduler.py: 调用JobController.adapt_job_parameters() 适配common_job_parameters</li>
<li>dag_scheduler.py: 使用28中的common_job_parameters 更新job.f_runtime_conf(job 为23 新建对象)</li>
<li>dag_scheduler.py: 调用schedule_utils.get_job_dsl_parser 解析参数</li>
<li>scheduler_utils.py: 根据配置选择dsl 版本（v1 或v2)</li>
<li>scheduler_utils.py: 调用dsl_parser.run() 解析参数</li>
<li>dsl_parser.py: 解析完毕</li>
<li>scheduler_utils.py: 返回dsl_parser 对象</li>
<li>dag_scheduler.py: 若为cluster  模式，且role 和 partyid 不为initiator（if role == job.f_initiator_role and party_id == job.f_initiator_party_id: continue），调用 JobController.initialize_tasks，故而后面36-41都不会执行</li>
<li>job_controller.py: 基于common_job_parameters, dsl_parser ,调用TaskController.create_task()，生成除initiator 外各个role &amp; party_id 上的task</li>
<li>task_controller.py: 设置task_info 的各项信息</li>
<li>task_controller.py:  调用job_utils.generate_task_id()</li>
<li>job_utils.py: 返回task_id</li>
<li>task_controller.py: 调用JobSaver.create_task()</li>
<li>job_saver.py: 调用create_job_family_entity将相关task信息入DB</li>
<li>dag_scheduler.py: 调用FederatedScheduler.create_job()</li>
<li>federated_scheduler.py: 调用job_command() </li>
<li>federated_scheduler.py: 调用api_utils.federated_api() 发起post请求。</li>
<li>api_utils.py: 由于upload 是在本地执行的，调用依次调用local_api()-&gt;federated_coordination_on_http(),${job_log_dir}/fate_flow_audit.log 的第一条日志从这里产生<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:31,028] [1:140259119585024] - api_utils.py[line:122]: remote http api request: http://10.200.96.235:9380/v1/party/202107260820309976351/local/0/create</span><br></pre></td></tr></table></figure></li>
<li>fate_flow_server.py: sever 端接受到如上请求，通过Flask，跳转至party_app.py</li>
<li>party_app.py: 调用create_job() </li>
<li>party_app.py: 调用JobController.create_job()</li>
<li>job_controller.py: 调用schedule_utils.get_job_dsl_parser 解析参数</li>
<li>scheduler_utils.py: 根据配置选择dsl 版本（v1 或v2)</li>
<li>scheduler_utils.py: 调用dsl_parser.run() 解析参数</li>
<li>dsl_parser.py: 解析完毕</li>
<li>job_controller.py: 打印出${job_log_dir}/fate_flow_schedule.log的第二行日志<br>schedule_logger(job_id).info(‘job parameters:{}’.format(job_parameters))<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 08:20:31,043] [1:140259111192320] - job_controller.py[line:51]: job parameters:&#123;&#x27;job_type&#x27;: &#x27;train&#x27;, &#x27;work_mode&#x27;: 1, &#x27;backend&#x27;: 0, &#x27;computing_engine&#x27;: &#x27;EGGROLL&#x27;, &#x27;federation_engine&#x27;: &#x27;EGGROLL&#x27;, &#x27;storage_engine&#x27;: &#x27;EGGROLL&#x27;, &#x27;engines_address&#x27;: &#123;&#125;, &#x27;federated_mode&#x27;: &#x27;MULTIPLE&#x27;, &#x27;task_parallelism&#x27;: 1, &#x27;computing_partitions&#x27;: 4, &#x27;federated_status_collect_type&#x27;: &#x27;PULL&#x27;, &#x27;model_id&#x27;: &#x27;local-0#model&#x27;, &#x27;model_version&#x27;: &#x27;202107260820309976351&#x27;, &#x27;eggroll_run&#x27;: &#123;&#125;, &#x27;spark_run&#x27;: &#123;&#125;, &#x27;rabbitmq_run&#x27;: &#123;&#125;, &#x27;adaptation_parameters&#x27;: &#123;&#x27;task_nodes&#x27;: 1, &#x27;task_cores_per_node&#x27;: 4, &#x27;task_memory_per_node&#x27;: 0, &#x27;request_task_cores&#x27;: 4, &#x27;if_initiator_baseline&#x27;: True&#125;&#125;</span><br></pre></td></tr></table></figure></li>
<li>job_controller.py: 更新job 参数信息，并进行参数校验</li>
<li>job_controller.py: 调用job_utils.save_job_conf() 生成job_dsl, job_runtime_conf,<br>job_runtime_conf_on_party,train_runtime_conf,pipeline_dsl 各项的保存目录，<br>并将相关参数以json格式落盘。这里和24 的操作一致，会入两次盘。</li>
<li>job_controller.py: 调用initialize_task, 初始化task。</li>
<li>job_controller.py: 基于common_job_parameters, dsl_parser ,调用TaskController.create_task()生成各个role &amp; party_id 上的task</li>
<li>task_controller.py: 设置task_info 的各项信息</li>
<li>task_controller.py: 调用JobSaver.create_task()</li>
<li><p>job_saver.py: 调用create_job_family_entity将相关task信息入DB<br>这里在fate_flow/peewee.log 中打出了日志</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[DEBUG] [2021-07-26 08:20:31,079] [1:140259111192320] - peewee.py[line:2863]: (&#x27;INSERT INTO `t_task` (`f_create_time`, `f_create_date`, `f_update_time`, `f_update_date`, `f_job_id`, `f_component_name`, `f_task_id`, `f_task_version`, `f_initiator_role`, `f_initiator_party_id`, `f_federated_mode`, `f_federated_status_collect_type`, `f_status`, `f_role`, `f_party_id`, `f_run_on_this_party`, `f_party_status`) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)&#x27;, [1627287631077, datetime.datetime(2021, 7, 26, 8, 20, 31), 1627287631078, datetime.datetime(2021, 7, 26, 8, 20, 31), &#x27;202107260820309976351&#x27;, &#x27;upload_0&#x27;, &#x27;202107260820309976351_upload_0&#x27;, 0, &#x27;local&#x27;, &#x27;0&#x27;, &#x27;MULTIPLE&#x27;, &#x27;PULL&#x27;, &#x27;waiting&#x27;, &#x27;local&#x27;, &#x27;0&#x27;, True, &#x27;waiting&#x27;])</span><br></pre></td></tr></table></figure>
</li>
<li><p>job_controller.py: 调用initialize_job_tracker，初始化tracker。</p>
</li>
<li>job_controller.py: 初始化 tracker,并调用tracker.log_job_view </li>
<li>job_tracker.py: 调用log_job_view 将相关信息写db，并在peewee.log 打日志<br>调用源码位于 python/fate_flow/operation/job_tracker.py 312行<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">@DB.connection_context()</span><br><span class="line">def bulk_insert_into_db(self, model, data_source):</span><br><span class="line">    try:</span><br><span class="line">        try:</span><br><span class="line">            DB.create_tables([model])</span><br><span class="line">        except Exception as e:</span><br><span class="line">            schedule_logger(self.job_id).exception(e)</span><br><span class="line">        batch_size = 50 if RuntimeConfig.USE_LOCAL_DATABASE else 1000</span><br><span class="line">        for i in range(0, len(data_source), batch_size):</span><br><span class="line">            with DB.atomic():</span><br><span class="line">                model.insert_many(data_source[i:i+batch_size]).execute()</span><br><span class="line">        return len(data_source)</span><br><span class="line">    except Exception as e:</span><br><span class="line">        schedule_logger(self.job_id).exception(e)</span><br><span class="line">        return 0</span><br></pre></td></tr></table></figure>
这里在fate_flow/peewee.log 中打出了日志如下：</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[DEBUG] [2021-07-26 08:20:31,122] [1:140259111192320] - peewee.py[line:2863]: (&#x27;SELECT table_name FROM information_schema.tables WHERE table_schema = DATABASE() AND table_type != %s ORDER BY table_name&#x27;, (&#x27;VIEW&#x27;,))</span><br><span class="line">[DEBUG] [2021-07-26 08:20:31,132] [1:140259111192320] - peewee.py[line:2863]: (&#x27;CREATE TABLE IF NOT EXISTS `t_tracking_metric_20210726` (`f_id` BIGINT AUTO_INCREMENT NOT NULL PRIMARY KEY, `f_create_time` BIGINT, `f_create_date` DATETIME, `f_update_time` BIGINT, `f_update_date` DATETIME, `f_job_id` VARCHAR(25) NOT NULL, `f_component_name` TEXT NOT NULL, `f_task_id` VARCHAR(100), `f_task_version` BIGINT, `f_role` VARCHAR(50) NOT NULL, `f_party_id` VARCHAR(10) NOT NULL, `f_metric_namespace` VARCHAR(180) NOT NULL, `f_metric_name` VARCHAR(180) NOT NULL, `f_key` VARCHAR(200) NOT NULL, `f_value` LONGTEXT NOT NULL, `f_type` INTEGER NOT NULL)&#x27;, [])</span><br><span class="line">[DEBUG] [2021-07-26 08:20:31,163] [1:140259111192320] - peewee.py[line:2863]: (&#x27;CREATE INDEX `trackingmetric_20210726_f_job_id` ON `t_tracking_metric_20210726` (`f_job_id`)&#x27;, [])</span><br><span class="line">[DEBUG] [2021-07-26 08:20:31,184] [1:140259111192320] - peewee.py[line:2863]: (&#x27;CREATE INDEX `trackingmetric_20210726_f_task_id` ON `t_tracking_metric_20210726` (`f_task_id`)&#x27;, [])</span><br><span class="line">[DEBUG] [2021-07-26 08:20:31,220] [1:140259111192320] - peewee.py[line:2863]: (&#x27;CREATE INDEX `trackingmetric_20210726_f_task_version` ON `t_tracking_metric_20210726` (`f_task_version`)&#x27;, [])</span><br><span class="line">[DEBUG] [2021-07-26 08:20:31,250] [1:140259111192320] - peewee.py[line:2863]: (&#x27;CREATE INDEX `trackingmetric_20210726_f_role` ON `t_tracking_metric_20210726` (`f_role`)&#x27;, [])</span><br><span class="line">[DEBUG] [2021-07-26 08:20:31,265] [1:140259111192320] - peewee.py[line:2863]: (&#x27;CREATE INDEX `trackingmetric_20210726_f_party_id` ON `t_tracking_metric_20210726` (`f_party_id`)&#x27;, [])</span><br><span class="line">[DEBUG] [2021-07-26 08:20:31,283] [1:140259111192320] - peewee.py[line:2863]: (&#x27;CREATE INDEX `trackingmetric_20210726_f_metric_namespace` ON `t_tracking_metric_20210726` (`f_metric_namespace`)&#x27;, [])</span><br><span class="line">[DEBUG] [2021-07-26 08:20:31,310] [1:140259111192320] - peewee.py[line:2863]: (&#x27;CREATE INDEX `trackingmetric_20210726_f_metric_name` ON `t_tracking_metric_20210726` (`f_metric_name`)&#x27;, [])</span><br><span class="line">[DEBUG] [2021-07-26 08:20:31,333] [1:140259111192320] - peewee.py[line:2863]: (&#x27;CREATE INDEX `trackingmetric_20210726_f_type` ON `t_tracking_metric_20210726` (`f_type`)&#x27;, [])</span><br><span class="line">[DEBUG] [2021-07-26 08:20:31,377] [1:140259111192320] - peewee.py[line:2863]: (&#x27;INSERT INTO `t_tracking_metric_20210726` (`f_create_time`, `f_job_id`, `f_component_name`, `f_task_id`, `f_task_version`, `f_role`, `f_party_id`, `f_metric_namespace`, `f_metric_name`, `f_key`, `f_value`, `f_type`) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s), (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s), (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)&#x27;, [1627287631122, &#x27;202107260820309976351&#x27;, &#x27;pipeline&#x27;, None, None, &#x27;local&#x27;, &#x27;0&#x27;, &#x27;job&#x27;, &#x27;job_view&#x27;, &#x27;gANYBwAAAHBhcnRuZXJxAC4=&#x27;, &#x27;gAN9cQAu&#x27;, 2, 1627287631122, &#x27;202107260820309976351&#x27;, &#x27;pipeline&#x27;, None, None, &#x27;local&#x27;, &#x27;0&#x27;, &#x27;job&#x27;, &#x27;job_view&#x27;, &#x27;gANYBwAAAGRhdGFzZXRxAC4=&#x27;, &#x27;gAN9cQBYBQAAAGxvY2FscQF9cQJLAH1xA3NzLg==&#x27;, 2, 1627287631122, &#x27;202107260820309976351&#x27;, &#x27;pipeline&#x27;, None, None, &#x27;local&#x27;, &#x27;0&#x27;, &#x27;job&#x27;, &#x27;job_view&#x27;, &#x27;gANYBQAAAHJvbGVzcQAu&#x27;, &#x27;gAN9cQBYBQAAAGxvY2FscQFdcQJLAGFzLg==&#x27;, 2])</span><br><span class="line">[DEBUG] [2021-07-26 08:20:31,381] [1:140259111192320] - pool.py[line:185]: Returning 140259386365264 to pool.</span><br></pre></td></tr></table></figure>
<ol>
<li>job_controller.py: 调用JobSaver.create_job() </li>
<li><p>job_saver.py: 调用create_job_family_entity将相关job信息 写db  fate_flow/peewee.log</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[DEBUG] [2021-07-26 08:20:31,385] [1:140259111192320] - peewee.py[line:2863]: (&#x27;INSERT INTO `t_job` (`f_create_time`, `f_create_date`, `f_update_time`, `f_update_date`, `f_job_id`, `f_name`, `f_description`, `f_tag`, `f_dsl`, `f_runtime_conf`, `f_runtime_conf_on_party`, `f_train_runtime_conf`, `f_roles`, `f_work_mode`, `f_initiator_role`, `f_initiator_party_id`, `f_status`, `f_role`, `f_party_id`, `f_is_initiator`, `f_progress`, `f_ready_signal`, `f_cancel_signal`, `f_rerun_signal`, `f_end_scheduling_updates`, `f_cores`, `f_memory`, `f_remaining_cores`, `f_remaining_memory`, `f_resource_in_use`) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)&#x27;, [1627287631382, datetime.datetime(2021, 7, 26, 8, 20, 31), 1627287631382, datetime.datetime(2021, 7, 26, 8, 20, 31), &#x27;202107260820309976351&#x27;, &#x27;&#x27;, &#x27;&#x27;, &#x27;&#x27;, &#x27;&#123;&quot;components&quot;: &#123;&quot;upload_0&quot;: &#123;&quot;module&quot;: &quot;Upload&quot;&#125;&#125;&#125;&#x27;, &#x27;&#123;&quot;initiator&quot;: &#123;&quot;role&quot;: &quot;local&quot;, &quot;party_id&quot;: 0&#125;, &quot;job_parameters&quot;: &#123;&quot;common&quot;: &#123;&quot;job_type&quot;: &quot;train&quot;, &quot;work_mode&quot;: 1, &quot;backend&quot;: 0, &quot;computing_engine&quot;: &quot;EGGROLL&quot;, &quot;federation_engine&quot;: &quot;EGGROLL&quot;, &quot;storage_engine&quot;: &quot;EGGROLL&quot;, &quot;engines_address&quot;: &#123;&#125;, &quot;federated_mode&quot;: &quot;MULTIPLE&quot;, &quot;task_parallelism&quot;: 1, &quot;computing_partitions&quot;: 4, &quot;federated_status_collect_type&quot;: &quot;PULL&quot;, &quot;model_id&quot;: &quot;local-0#model&quot;, &quot;model_version&quot;: &quot;202107260820309976351&quot;, &quot;eggroll_run&quot;: &#123;&#125;, &quot;spark_run&quot;: &#123;&#125;, &quot;rabbitmq_run&quot;: &#123;&#125;, &quot;adaptation_parameters&quot;: &#123;&quot;task_nodes&quot;: 1, &quot;task_cores_per_node&quot;: 4, &quot;task_memory_per_node&quot;: 0, &quot;request_task_cores&quot;: 4, &quot;if_initiator_baseline&quot;: true&#125;&#125;&#125;, &quot;role&quot;: &#123;&quot;local&quot;: [0]&#125;, &quot;component_parameters&quot;: &#123;&quot;role&quot;: &#123;&quot;local&quot;: &#123;&quot;0&quot;: &#123;&quot;upload_0&quot;: &#123;&quot;name&quot;: &quot;hetero_guest&quot;, &quot;head&quot;: 1, &quot;file&quot;: &quot;/data/projects/fate/jobs/202107260820309976351/fate_upload_tmp/breast_hetero_guest.csv&quot;, &quot;partition&quot;: 1, &quot;namespace&quot;: &quot;cl&quot;, &quot;destroy&quot;: false&#125;&#125;&#125;&#125;&#125;, &quot;dsl_version&quot;: 2&#125;&#x27;, &#x27;&#123;&quot;initiator&quot;: &#123;&quot;role&quot;: &quot;local&quot;, &quot;party_id&quot;: 0&#125;, &quot;job_parameters&quot;: &#123;&quot;job_type&quot;: &quot;train&quot;, &quot;work_mode&quot;: 1, &quot;backend&quot;: 0, &quot;computing_engine&quot;: &quot;EGGROLL&quot;, &quot;federation_engine&quot;: &quot;EGGROLL&quot;, &quot;storage_engine&quot;: &quot;EGGROLL&quot;, &quot;engines_address&quot;: &#123;&quot;computing&quot;: &#123;&quot;cores_per_node&quot;: 20, &quot;nodes&quot;: 1&#125;, &quot;federation&quot;: &#123;&quot;host&quot;: &quot;rollsite&quot;, &quot;port&quot;: 9370&#125;, &quot;storage&quot;: &#123;&quot;cores_per_node&quot;: 20, &quot;nodes&quot;: 1&#125;&#125;, &quot;federated_mode&quot;: &quot;MULTIPLE&quot;, &quot;task_parallelism&quot;: 1, &quot;computing_partitions&quot;: 4, &quot;federated_status_collect_type&quot;: &quot;PULL&quot;, &quot;model_id&quot;: &quot;local-0#model&quot;, &quot;model_version&quot;: &quot;202107260820309976351&quot;, &quot;eggroll_run&quot;: &#123;&quot;eggroll.session.processors.per.node&quot;: 4&#125;, &quot;spark_run&quot;: &#123;&#125;, &quot;rabbitmq_run&quot;: &#123;&#125;, &quot;adaptation_parameters&quot;: &#123;&quot;task_nodes&quot;: 1, &quot;task_cores_per_node&quot;: 4, &quot;task_memory_per_node&quot;: 0, &quot;request_task_cores&quot;: 4, &quot;if_initiator_baseline&quot;: false&#125;&#125;, &quot;role&quot;: &#123;&quot;local&quot;: [0]&#125;, &quot;component_parameters&quot;: &#123;&quot;role&quot;: &#123;&quot;local&quot;: &#123;&quot;0&quot;: &#123;&quot;upload_0&quot;: &#123;&quot;name&quot;: &quot;hetero_guest&quot;, &quot;head&quot;: 1, &quot;file&quot;: &quot;/data/projects/fate/jobs/202107260820309976351/fate_upload_tmp/breast_hetero_guest.csv&quot;, &quot;partition&quot;: 1, &quot;namespace&quot;: &quot;cl&quot;, &quot;destroy&quot;: false&#125;&#125;&#125;&#125;&#125;, &quot;dsl_version&quot;: 2&#125;&#x27;, &#x27;&#123;&#125;&#x27;, &#x27;&#123;&quot;local&quot;: [0]&#125;&#x27;, 1, &#x27;local&#x27;, &#x27;0&#x27;, &#x27;waiting&#x27;, &#x27;local&#x27;, &#x27;0&#x27;, True, 0, False, False, False, 0, 0, 0, 0, 0, False])</span><br></pre></td></tr></table></figure>
</li>
<li><p>party_app.py: 对应48，若以上步骤都未 raise 异常，则调用get_json_result, 生成返回信息 success 给。 否则抛出异常</p>
</li>
<li>party_app.py: 返回信息给server端，对应46</li>
<li>fate_flow_server.py: 返回response 给api_utils.py，对应45</li>
<li>api_utils.py: 返回response 给 federated_scheduler.py，对应44</li>
<li>federated_scheduler.py: 根据response生成federated_response </li>
<li>federated_scheduler.py: 返回federated_response  给dag_scheduler.py，对应42</li>
<li>dag_scheduler.py: 返回federated_response  给data_access_app.py，对应13</li>
<li>data_access_app.py: 返回federated_response 给fate_flow_server.py，对应3</li>
<li>fate_flow_server.py: 返回federated_response 给 fate_flow_client.py 对应2</li>
<li>fate_flow_client.py: 调用prettify 在console 打印日志</li>
</ol>
<p>至此，任务提交成功</p>
<h1 id="REF"><a href="#REF" class="headerlink" title="REF"></a>REF</h1><p><a href="">raise</a><br><a href="">peewee</a><br><a href="http://c.biancheng.net/view/5484.html">Python <strong>new</strong>()方法详解</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2023/08/20/fate-log-3-lifecycle/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="喵十八">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="喵十八の小窝">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/08/20/fate-log-3-lifecycle/" class="post-title-link" itemprop="url">FATE学习：跟着日志读源码（三）一般任务生命周期</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2023-08-20 12:10:55 / Modified: 13:30:33" itemprop="dateCreated datePublished" datetime="2023-08-20T12:10:55+08:00">2023-08-20</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/FATE/" itemprop="url" rel="index"><span itemprop="name">FATE</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h1><p>从联邦建模的角度去理解整个job的生命周期，就是一系列功能模块组成的DAG，可以参照fate官方的文档，<img src="/image/fate/3cd642864c473c272b55c0efde5026e8.png" alt="image"><br>其中，各个功能模块就具体实现而言，就是fate中的各个算法或数据组件。</p>
<p>不过结合源码，从日志的角度来看，可以将整个job的生命周期，按照不同阶段所做的操作，进行划分。具体如下图：<br><img src="/image/fate/084bab7a7de64bce9decdcf09c29e14d.png" alt="job life cycle"></p>
<ol>
<li>submit：提交 job </li>
<li>create：创建job 和该job下对应的 tasks（相当于元数据） ，这里创建好之后，所有状态都是waiting</li>
<li>job schedule：对于job，按照FIFO的顺序，轮询到waiting状态的job，为其申请资源并将状态置为running。</li>
<li>task scheduler：轮询到running状态的job，对其涉及的tasks，按照依赖关系依次调度</li>
<li>execute： 执行4中置为running状态的task</li>
<li>task finish：task 运行完毕，进行资源回收和环境清理</li>
<li>job finish：job 运行完毕，进行资源回收和环境清理</li>
<li>cancel：中止正在执行的job，cancel并不算一般的生命周期中的操作，可以发生在create之后的任何阶段，接受到cancel后，在polling下一次轮询时生效</li>
</ol>
<p>此外还有非生命周期中阶段polling： 是fate_flow_server上的轮询机制，探测到各种状态的job和task分别予以相对应的操作，严格而言，并不算是job的生命周期，只是一个轮询的角色。</p>
<h1 id="样例"><a href="#样例" class="headerlink" title="样例"></a>样例</h1><p>先从最简单的upload 开始，因为upload只是一个单方的job，在job schedule 至 job finish 阶段，不涉及多方的协作，日志都在单机上，分析较为简单。参照文档，在CLI 提交如下命令<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python fate_flow_client.py -f upload -c upload_guest.json</span><br></pre></td></tr></table></figure></p>
<h1 id="日志位置"><a href="#日志位置" class="headerlink" title="日志位置"></a>日志位置</h1><p>任务提交成功后，主要产生三部分日志：</p>
<h2 id="Console-日志"><a href="#Console-日志" class="headerlink" title="Console 日志"></a>Console 日志</h2><p>提交任务完成之后，打印在屏幕上的日志<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;data&quot;: &#123;</span><br><span class="line">        &quot;board_url&quot;: &quot;http://fateboard:8080/index.html#/dashboard?job_id=202107260820309976351&amp;role=local&amp;party_id=0&quot;,</span><br><span class="line">        &quot;job_dsl_path&quot;: &quot;/data/projects/fate/jobs/202107260820309976351/job_dsl.json&quot;,</span><br><span class="line">        &quot;job_id&quot;: &quot;202107260820309976351&quot;,</span><br><span class="line">        &quot;job_runtime_conf_on_party_path&quot;: &quot;/data/projects/fate/jobs/202107260820309976351/local/job_runtime_on_party_conf.json&quot;,</span><br><span class="line">        &quot;job_runtime_conf_path&quot;: &quot;/data/projects/fate/jobs/202107260820309976351/job_runtime_conf.json&quot;,</span><br><span class="line">        &quot;logs_directory&quot;: &quot;/data/projects/fate/logs/202107260820309976351&quot;,</span><br><span class="line">        &quot;model_info&quot;: &#123;</span><br><span class="line">            &quot;model_id&quot;: &quot;local-0#model&quot;,</span><br><span class="line">            &quot;model_version&quot;: &quot;202107260820309976351&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;namespace&quot;: &quot;cl&quot;,</span><br><span class="line">        &quot;pipeline_dsl_path&quot;: &quot;/data/projects/fate/jobs/202107260820309976351/pipeline_dsl.json&quot;,</span><br><span class="line">        &quot;table_name&quot;: &quot;hetero_guest&quot;,</span><br><span class="line">        &quot;train_runtime_conf_path&quot;: &quot;/data/projects/fate/jobs/202107260820309976351/train_runtime_conf.json&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;jobId&quot;: &quot;202107260820309976351&quot;,</span><br><span class="line">    &quot;retcode&quot;: 0,</span><br><span class="line">    &quot;retmsg&quot;: &quot;success&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="容器日志"><a href="#容器日志" class="headerlink" title="容器日志"></a>容器日志</h2><p>KubeFATE部署的话，查看名为python 的容器的日志，主要是POST请求和结果的日志。<br>其中和submit job 相关部分截取如下：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">1. 10.200.96.235 - - [26/Jul/2021 08:20:31] &quot;POST /v1/party/202107260820309976351/local/0/create HTTP/1.1&quot; 200 -</span><br><span class="line">2. 10.200.96.235 - - [26/Jul/2021 08:20:31] &quot;POST /v1/data/upload?%7B%22file%22:%20%22/data/projects/fate/examples/data/breast_hetero_guest.csv%22,%20%22head%22:%201,%20%22partition%22:%201,%20%22work_mode%22:%201,%20%22table_name%22:%20%22hetero_guest%22,%20%22namespace%22:%20%22cl%22,%20%22config%22:%20%22/data/projects/fate/cl/upload_guest.json%22,%20%22function%22:%20%22upload%22%7D HTTP/1.1&quot; 200 -</span><br><span class="line">3. 10.200.96.235 - - [26/Jul/2021 08:20:32] &quot;POST /v1/party/202107260820309976351/local/0/resource/apply HTTP/1.1&quot; 200 -</span><br><span class="line">4. 10.200.96.235 - - [26/Jul/2021 08:20:32] &quot;POST /v1/party/202107260820309976351/local/0/start HTTP/1.1&quot; 200 -</span><br><span class="line">5. 10.200.96.235 - - [26/Jul/2021 08:20:32] &quot;POST /v1/party/202107260820309976351/upload_0/202107260820309976351_upload_0/0/local/0/status/running HTTP/1.1&quot; 200 -</span><br><span class="line">6. 10.200.96.235 - - [26/Jul/2021 08:20:32] &quot;POST /v1/party/202107260820309976351/upload_0/202107260820309976351_upload_0/0/local/0/start HTTP/1.1&quot; 200 -</span><br><span class="line">7. 10.200.96.235 - - [26/Jul/2021 08:20:33] &quot;POST /v1/party/202107260820309976351/upload_0/202107260820309976351_upload_0/0/local/0/report HTTP/1.1&quot; 200 -</span><br><span class="line">8. 10.200.96.235 - - [26/Jul/2021 08:20:34] &quot;POST /v1/party/202107260820309976351/upload_0/202107260820309976351_upload_0/0/local/0/collect HTTP/1.1&quot; 200 -</span><br><span class="line">9. 10.200.96.235 - - [26/Jul/2021 08:20:36] &quot;POST /v1/party/202107260820309976351/upload_0/202107260820309976351_upload_0/0/local/0/collect HTTP/1.1&quot; 200 -</span><br><span class="line">10. 10.200.96.235 - - [26/Jul/2021 08:20:38] &quot;POST /v1/party/202107260820309976351/upload_0/202107260820309976351_upload_0/0/local/0/collect HTTP/1.1&quot; 200 -</span><br><span class="line">11. 10.200.96.235 - - [26/Jul/2021 08:20:40] &quot;POST /v1/party/202107260820309976351/upload_0/202107260820309976351_upload_0/0/local/0/collect HTTP/1.1&quot; 200 -</span><br><span class="line">12. 10.200.96.235 - - [26/Jul/2021 08:20:41] &quot;POST /v1/party/202107260820309976351/local/0/update HTTP/1.1&quot; 200 -</span><br><span class="line">13. 10.200.96.235 - - [26/Jul/2021 08:20:42] &quot;POST /v1/party/202107260820309976351/upload_0/202107260820309976351_upload_0/0/local/0/collect HTTP/1.1&quot; 200 -</span><br><span class="line">14. 10.200.96.235 - - [26/Jul/2021 08:20:43] &quot;POST /v1/tracker/202107260820309976351/upload_0/202107260820309976351_upload_0/0/local/0/output_data_info/save HTTP/1.1&quot; 200 -</span><br><span class="line">15. 10.200.96.235 - - [26/Jul/2021 08:20:43] &quot;POST /v1/tracker/202107260820309976351/upload_0/202107260820309976351_upload_0/0/local/0/metric_data/save HTTP/1.1&quot; 200 -</span><br><span class="line">16. 10.200.96.235 - - [26/Jul/2021 08:20:43] &quot;POST /v1/tracker/202107260820309976351/upload_0/202107260820309976351_upload_0/0/local/0/metric_meta/save HTTP/1.1&quot; 200 -</span><br><span class="line">17. 10.200.96.235 - - [26/Jul/2021 08:20:44] &quot;POST /v1/party/202107260820309976351/upload_0/202107260820309976351_upload_0/0/local/0/report HTTP/1.1&quot; 200 -</span><br><span class="line">18. 10.200.96.235 - - [26/Jul/2021 08:20:45] &quot;POST /v1/party/202107260820309976351/upload_0/202107260820309976351_upload_0/0/local/0/collect HTTP/1.1&quot; 200 -</span><br><span class="line">19. 10.200.96.235 - - [26/Jul/2021 08:20:45] &quot;POST /v1/party/202107260820309976351/upload_0/202107260820309976351_upload_0/0/local/0/report HTTP/1.1&quot; 200 -</span><br><span class="line">static conf path: /data/projects/fate/eggroll/conf/eggroll.properties</span><br><span class="line">20. 10.200.96.235 - - [26/Jul/2021 08:20:47] &quot;POST /v1/party/202107260820309976351/upload_0/202107260820309976351_upload_0/0/local/0/status/success HTTP/1.1&quot; 200 -</span><br><span class="line">21. 10.200.96.235 - - [26/Jul/2021 08:20:47] &quot;POST /v1/party/202107260820309976351/upload_0/202107260820309976351_upload_0/0/local/0/stop/success HTTP/1.1&quot; 200 -</span><br><span class="line">22. 10.200.96.235 - - [26/Jul/2021 08:20:47] &quot;POST /v1/party/202107260820309976351/local/0/model HTTP/1.1&quot; 200 -</span><br><span class="line">23. 10.200.96.235 - - [26/Jul/2021 08:20:47] &quot;POST /v1/party/202107260820309976351/local/0/status/success HTTP/1.1&quot; 200 -</span><br><span class="line">24. 10.200.96.235 - - [26/Jul/2021 08:20:47] &quot;POST /v1/party/202107260820309976351/local/0/stop/success HTTP/1.1&quot; 200 -</span><br><span class="line">25. 10.200.96.235 - - [26/Jul/2021 08:20:48] &quot;POST /v1/party/202107260820309976351/local/0/clean HTTP/1.1&quot; 200 -</span><br><span class="line">26. 10.200.96.235 - - [26/Jul/2021 08:20:48] &quot;POST /v1/party/202107260820309976351/local/0/stop/success HTTP/1.1&quot; 200 -</span><br><span class="line">27. 10.200.96.235 - - [26/Jul/2021 08:20:48] &quot;POST /v1/party/202107260820309976351/local/0/clean HTTP/1.1&quot; 200 -</span><br></pre></td></tr></table></figure><br>该生命周期中，各个阶段的情况，在这里都有体现。</p>
<h2 id="FATE-Flow-日志"><a href="#FATE-Flow-日志" class="headerlink" title="FATE Flow 日志"></a>FATE Flow 日志</h2><p>日志会生成在fate_flow 和 jobid 两个目录下，为了便于区分，分别会用fate_flow/xxx.log 和 $ {job_log_dir}/xxx.log 进行区分。<br>此外${job_log_dir}下还可进一步细分为如下几个目录：</p>
<ul>
<li>${job_log_dir}</li>
<li>$ {job_log_dir}/$ {role}/${party}</li>
<li>$ {job_log_dir}/$ {role}/$ {party}/${taskid} = ${task_log_dir}<h3 id="fate-flow-server-日志，位于-data-projects-fate-logs-fate-flow-目录下，说明见前文："><a href="#fate-flow-server-日志，位于-data-projects-fate-logs-fate-flow-目录下，说明见前文：" class="headerlink" title="fate_flow_server 日志，位于/data/projects/fate/logs/fate_flow 目录下，说明见前文："></a>fate_flow_server 日志，位于/data/projects/fate/logs/fate_flow 目录下，说明见前文：</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">- fate_flow_audit.log</span><br><span class="line">- fate_flow_detect.log</span><br><span class="line">- fate_flow_stat.log</span><br><span class="line">- fate_flow_schedule.log</span><br><span class="line">- peewee.log</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="job日志"><a href="#job日志" class="headerlink" title="job日志"></a>job日志</h3><p>位于/data/projects/fate/logs/${jobid} 下（此目录后文称之为job_log_dir），量级过大，这里只列下目录，每个日志的说明，见前文。<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">- fate_flow_audit.log：</span><br><span class="line">- fate_flow_schedule.log  </span><br><span class="line">- fate_flow_sql.log  </span><br><span class="line">+ $&#123;role&#125;</span><br><span class="line">    + $&#123;party&#125;</span><br><span class="line">        - DEBUG.log  </span><br><span class="line">        - INFO.log  </span><br><span class="line">        + $&#123;taskid&#125;</span><br><span class="line">            - DEBUG.log  </span><br><span class="line">            - fate_flow_schedule.log  </span><br><span class="line">            - INFO.log  </span><br><span class="line">            - peewee.log  </span><br><span class="line">            - PROFILING.log  </span><br><span class="line">            - stat.log  </span><br><span class="line">            - std.log</span><br></pre></td></tr></table></figure></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2023/08/20/fate-log-2-start-fateflow/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="喵十八">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="喵十八の小窝">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/08/20/fate-log-2-start-fateflow/" class="post-title-link" itemprop="url">FATE学习：跟着日志读源码（二）fate_flow server 启动</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2023-08-20 12:10:46 / Modified: 12:24:01" itemprop="dateCreated datePublished" datetime="2023-08-20T12:10:46+08:00">2023-08-20</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/FATE/" itemprop="url" rel="index"><span itemprop="name">FATE</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h1><ul>
<li>代码：python/fate_flow/fate_flow_server.py</li>
<li>web框架：Flask</li>
</ul>
<p>流程为：</p>
<ol>
<li>执行 python fate_flow_server.py ，这部分会在容器中打出相应日志。</li>
<li><p>fate_flow_server 执行启动过程。这里可以看下logs目录</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(app-root) bash-4.2# pwd</span><br><span class="line">/data/projects/fate/logs</span><br><span class="line">(app-root) bash-4.2# ls</span><br><span class="line">fate_flow</span><br><span class="line">(app-root) bash-4.2# ls fate_flow/</span><br><span class="line">DEBUG.log  fate_flow_detect.log  fate_flow_schedule.log  fate_flow_stat.log  INFO.log  peewee.log</span><br></pre></td></tr></table></figure>
<p>可以看到，由于没有提交任何任务，当前只有一个fate_flow的目录，这里记录的是fate_flow_server启动的日志。具体而言</p>
<ul>
<li>peewee.log：fate中操作db，使用了peewee，这里记录所有通过peewee操作数据库的日志</li>
<li>fate_flow_detect.log：探测器日志</li>
<li>fate_flow_schedule.log：调度器日志</li>
<li>fate_flow_stat.log：除以上3部分外的其余状态日志</li>
<li>DEBUG.log、INFO.log、WARNING.log、ERROR.log：各级别日志，会将以上各部分（除了fate_flow_detect.log，这个后续单独说明逻辑）中对应级别的日志收集。<br>因fate_flow_server 启动的日志，均输出在fate_flow 目录中，故本文所述的日志，均为fate_flow目录中的对应日志。</li>
</ul>
</li>
</ol>
<h1 id="执行-fate-flow-server-py"><a href="#执行-fate-flow-server-py" class="headerlink" title="执行 fate_flow_server.py"></a>执行 fate_flow_server.py</h1><p>由于是KubeFATE 部署，直接查看容器日志即可。<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+ mkdir -p /data/projects/fate/conf/</span><br><span class="line">+ cp /data/projects/fate/conf1/transfer_conf.yaml /data/projects/fate/conf/transfer_conf.yaml</span><br><span class="line">+ cp /data/projects/fate/conf1/service_conf.yaml /data/projects/fate/conf/service_conf.yaml</span><br><span class="line">+ cp /data/projects/fate/conf1/pipeline_conf.yaml /data/projects/fate/conf/pipeline_conf.yaml</span><br><span class="line">+ sed -i &#x27;s/host: fateflow/host: 10.200.96.237/g&#x27; /data/projects/fate/conf/service_conf.yaml</span><br><span class="line">+ sed -i &#x27;s/ip: fateflow/ip: 10.200.96.237/g&#x27; /data/projects/fate/conf/pipeline_conf.yaml</span><br><span class="line">+ cp -r /data/projects/fate/examples /data/projects/fate/examples-shared-for-client</span><br><span class="line">+ sleep 5</span><br><span class="line">+ python ./fate_flow/fate_flow_server.py</span><br><span class="line"> * Running on http://10.200.96.237:9380/ (Press CTRL+C to quit)</span><br></pre></td></tr></table></figure></p>
<p>从上述命令可以看出，涉及操作是创建目录-&gt;复制配置文件-&gt;启动 fate_flow_server.py。</p>
<h1 id="fate-flow-server-启动细节"><a href="#fate-flow-server-启动细节" class="headerlink" title="fate_flow_server 启动细节"></a>fate_flow_server 启动细节</h1><p><img src="/image/fate/a87d7d3622cc4f37a72b64b57de9577b.png" alt="fate_flow server 启动uml图"></p>
<ol>
<li><p>fate_flow_server.py：定义app变量，定义server能提供的服务，这一部分源码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">app = DispatcherMiddleware(</span><br><span class="line">    manager,</span><br><span class="line">    &#123;</span><br><span class="line">        &#x27;/&#123;&#125;/data&#x27;.format(API_VERSION): data_access_app_manager,</span><br><span class="line">        &#x27;/&#123;&#125;/model&#x27;.format(API_VERSION): model_app_manager,</span><br><span class="line">        &#x27;/&#123;&#125;/job&#x27;.format(API_VERSION): job_app_manager,</span><br><span class="line">        &#x27;/&#123;&#125;/table&#x27;.format(API_VERSION): table_app_manager,</span><br><span class="line">        &#x27;/&#123;&#125;/tracking&#x27;.format(API_VERSION): tracking_app_manager,</span><br><span class="line">        &#x27;/&#123;&#125;/pipeline&#x27;.format(API_VERSION): pipeline_app_manager,</span><br><span class="line">        &#x27;/&#123;&#125;/permission&#x27;.format(API_VERSION): permission_app_manager,</span><br><span class="line">        &#x27;/&#123;&#125;/version&#x27;.format(API_VERSION): version_app_manager,</span><br><span class="line">        &#x27;/&#123;&#125;/party&#x27;.format(API_VERSION): party_app_manager,</span><br><span class="line">        &#x27;/&#123;&#125;/initiator&#x27;.format(API_VERSION): initiator_app_manager,</span><br><span class="line">        &#x27;/&#123;&#125;/tracker&#x27;.format(API_VERSION): tracker_app_manager,</span><br><span class="line">        &#x27;/&#123;&#125;/forward&#x27;.format(API_VERSION): proxy_app_manager</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>不同的manager对应不同模块的功能。详细说明参照REF1，</p>
</li>
<li><p>fate_flow_server.py：调用db_models.init_database_tables()具体执行在 3 中。这里代码里是init_flow_db()，实际是db_models.init_database_tables()的别名。用来和init_arch_db()区分。</p>
</li>
<li><p>fate_flow/db/db_models.py： 初始化fate_flow相关表。注意，在类初始化时，这里会进行一次判断，cluster模式使用mysql，standalone 模式使用sqlite。这一部分日志会输出到fate_flow_stat.log 中：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 07:14:02,439] [1:140691673888576] - db_models.py[line:60]: init mysql database on cluster mode successfully</span><br></pre></td></tr></table></figure>
<p>这里涉及t_job、t_task、t_tracking_metric、t_tracking_output_data_info、t_machine_learning_model_info、t_model_tag、t_tags、t_component_summary、t_model_operation_log、t_engine_registry 等10张表。会执行建表和建索引的相关操作。各表具体字段可以查看源码。这部分日志会在peewee.log 中打出。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(&#x27;CREATE TABLE IF NOT EXISTS `componentsummary` (`f_id` BIGINT AUTO_INCREMENT NOT NULL PRIMARY KEY, `f_create_time` BIGINT, `f_create_date` DATETIME, `f_update_time` BIGINT, `f_update_date` DATETIME, `f_job_id` VARCHAR(25) NOT NULL, `f_role` VARCHAR(25) NOT NULL, `f_party_id` VARCHAR(10) NOT NULL, `f_component_name` TEXT NOT NULL, `f_task_id` VARCHAR(50), `f_task_version` VARCHAR(50), `f_summary` LONGTEXT NOT NULL)&#x27;, [])</span><br></pre></td></tr></table></figure>
<p><strong>注：本地debug时，默认为standalone模式，会在目录生成一个fate_flow_sqlite.db数据库文件</strong></p>
</li>
<li><p>fate_flow_server.py： 调用db_models.init_database_tables() 具体执行在5。</p>
</li>
<li>fate_arch/storage/metastore/db_models.py：初始化fate_arch相关表，和3类似，也会根据部署模式选择不同的数据库。<br>诶，然后这里没打日志。。。<br>这里涉及t_storage_table_meta、t_session_record 两张表。也会执行建表和建索引的相关操作。各表具体字段可以查看源码。这部分日志会在peewee.log 中紧接着3打出。</li>
<li>fate_flow_server.py： 使用argparse解析入参</li>
<li>fate_flow_server.py：调用RuntimeConfig.init_ent() 加载环境变量，调用RuntimeConfig.set_process_role() 设置为driver(?这里driver和executor 的区别？）</li>
<li>fate_flow_server.py：调用PrivilegeAuth.init() 进行鉴权模块初始化。</li>
<li>authentication_utils.py：根据配置项（默认是否）决定是否初始化各个role支持的component，这一部分日志会输出在fate_flow_stat.log中。</li>
<li>fate_flow_server.py：调用ServiceUtils.register() 注册服务</li>
<li>authentication_utils.py：根据配置项（默认是否），决定是否注册。如需注册，需要安装zookeeper，这一部分日志会输出在fate_flow_stat.log中。</li>
<li>fate_flow_server.py：调用ResourceManager.initialize() 初始化资源管理器</li>
<li>resource_manager.py：调用register_engine 初始化各项信息。<br>这里会根据配置文件python/fate_flow/settings.py 中的如下内容<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># Storage engine is used for component output data</span><br><span class="line">SUPPORT_BACKENDS_ENTRANCE = &#123;</span><br><span class="line">    &quot;fate_on_eggroll&quot;: &#123;</span><br><span class="line">        EngineType.COMPUTING: (ComputingEngine.EGGROLL, &quot;clustermanager&quot;),</span><br><span class="line">        EngineType.STORAGE: (StorageEngine.EGGROLL, &quot;clustermanager&quot;),</span><br><span class="line">        EngineType.FEDERATION: (FederationEngine.EGGROLL, &quot;rollsite&quot;),</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;fate_on_spark&quot;: &#123;</span><br><span class="line">        EngineType.COMPUTING: (ComputingEngine.SPARK, &quot;spark&quot;),</span><br><span class="line">        EngineType.STORAGE: (StorageEngine.HDFS, &quot;hdfs&quot;),</span><br><span class="line">        EngineType.FEDERATION: (FederationEngine.RABBITMQ, &quot;rabbitmq&quot;),</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
遍历不同的EngineType的engine_name，即COMPUTING、STORAGE、FEDERATION三个部分，创建相关记录。再将如上EngineType中各个engine_name替换为standalone，再遍历一次。<br>创建相关记录流程为先查询f_engine_entrance表中，是有有该f_engine_type和f_engine_name的值，如没有，执行insert 如有，则update 相关信息。故而针对如上配置的流程为：<ul>
<li>依此遍历fate_on_eggroll，fate_on_spark中 clustermanager，clustermanager，rollsite。因select 之后的结果都为空，故相关操作都是create信息，在peewee.log 中的日志为INSERT</li>
<li>fate_on_eggroll中，将engine_name替换为standalone后，再一次遍历。因select 之后的结果都为空，故相关操作都是create信息，在peewee.log 中的日志为INSERT。</li>
<li>fate_on_spark中，将engine_name替换为standalone后，再一次遍历。因上一步已经create相关记录，皆为update操作，在peewee.log 中的日志为UPDATE。</li>
</ul>
</li>
</ol>
<p>sql执行情况样例见 peewee.log<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[DEBUG] [2021-07-26 07:14:06,066] [1:140691673888576] - peewee.py[line:2863]: (&#x27;SELECT `t1`.`f_create_time`, `t1`.`f_create_date`, `t1`.`f_update_time`, `t1`.`f_update_date`, `t1`.`f_engine_type`, `t1`.`f_engine_name`, `t1`.`f_engine_entrance`, `t1`.`f_engine_config`, `t1`.`f_cores`, `t1`.`f_memory`, `t1`.`f_remaining_cores`, `t1`.`f_remaining_memory`, `t1`.`f_nodes` FROM `t_engine_registry` AS `t1` WHERE ((`t1`.`f_engine_type` = %s) AND (`t1`.`f_engine_name` = %s))&#x27;, [&#x27;computing&#x27;, &#x27;EGGROLL&#x27;])</span><br><span class="line">[DEBUG] [2021-07-26 07:14:06,072] [1:140691673888576] - peewee.py[line:2863]: (&#x27;INSERT INTO `t_engine_registry` (`f_create_time`, `f_create_date`, `f_update_time`, `f_update_date`, `f_engine_type`, `f_engine_name`, `f_engine_entrance`, `f_engine_config`, `f_cores`, `f_memory`, `f_remaining_cores`, `f_remaining_memory`, `f_nodes`) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)&#x27;, [1627283646068, datetime.datetime(2021, 7, 26, 7, 14, 6), 1627283646068, datetime.datetime(2021, 7, 26, 7, 14, 6), &#x27;computing&#x27;, &#x27;EGGROLL&#x27;, &#x27;clustermanager&#x27;, &#x27;&#123;&quot;cores_per_node&quot;: 20, &quot;nodes&quot;: 1&#125;&#x27;, 20, 0, 20, 0, 1])</span><br><span class="line"></span><br><span class="line">[DEBUG] [2021-07-26 07:14:06,242] [1:140691673888576] - peewee.py[line:2863]: (&#x27;UPDATE `t_engine_registry` SET `f_engine_config` = %s, `f_cores` = %s, `f_memory` = %s, `f_remaining_cores` = (`t_engine_registry`.`f_remaining_cores` + %s), `f_remaining_memory` = (`t_engine_registry`.`f_remaining_memory` + %s), `f_nodes` = %s WHERE ((`t_engine_registry`.`f_engine_type` = %s) AND (`t_engine_registry`.`f_engine_name` = %s))&#x27;, [&#x27;&#123;&quot;nodes&quot;: 1, &quot;cores_per_node&quot;: 20&#125;&#x27;, 20, 0, 0, 0, 1, &#x27;storage&#x27;, &#x27;STANDALONE&#x27;])</span><br></pre></td></tr></table></figure></p>
<p>fate_flow具体的日志，打在fate_flow_stat.log 中，如下：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 07:14:06,077] [1:140691673888576] - resource_manager.py[line:94]: create computing engine EGGROLL clustermanager registration information</span><br><span class="line">[INFO] [2021-07-26 07:14:06,097] [1:140691673888576] - resource_manager.py[line:94]: create storage engine EGGROLL clustermanager registration information</span><br><span class="line">[INFO] [2021-07-26 07:14:06,117] [1:140691673888576] - resource_manager.py[line:94]: create federation engine EGGROLL rollsite registration information</span><br><span class="line">[INFO] [2021-07-26 07:14:06,139] [1:140691673888576] - resource_manager.py[line:94]: create computing engine SPARK spark registration information</span><br><span class="line">[INFO] [2021-07-26 07:14:06,175] [1:140691673888576] - resource_manager.py[line:94]: create storage engine HDFS hdfs registration information</span><br><span class="line">[INFO] [2021-07-26 07:14:06,199] [1:140691673888576] - resource_manager.py[line:94]: create federation engine RABBITMQ rabbitmq registration information</span><br><span class="line">[INFO] [2021-07-26 07:14:06,207] [1:140691673888576] - resource_manager.py[line:94]: create computing engine STANDALONE fateflow registration information</span><br><span class="line">[INFO] [2021-07-26 07:14:06,216] [1:140691673888576] - resource_manager.py[line:94]: create storage engine STANDALONE fateflow registration information</span><br><span class="line">[INFO] [2021-07-26 07:14:06,227] [1:140691673888576] - resource_manager.py[line:94]: create federation engine STANDALONE fateflow registration information</span><br><span class="line">[INFO] [2021-07-26 07:14:06,236] [1:140691673888576] - resource_manager.py[line:76]: update computing engine STANDALONE fateflow registration information takes no effect</span><br><span class="line">[INFO] [2021-07-26 07:14:06,243] [1:140691673888576] - resource_manager.py[line:76]: update storage engine STANDALONE fateflow registration information takes no effect</span><br><span class="line">[INFO] [2021-07-26 07:14:06,253] [1:140691673888576] - resource_manager.py[line:76]: update federation engine STANDALONE fateflow registration information takes no effect</span><br></pre></td></tr></table></figure></p>
<ol>
<li>fate_flow_server.py：启动探测器， Detecor().start，每隔5秒轮询</li>
<li>detect.py： 一次探测如下四个类型的任务running_task，running_job，resource_record，expired_session，探测到之后，执行相关操作。日志包括两部分，查询DB的日志记录在peewee.log，fate_flow的日志记录在fate_flow/fate_flow_detect.log 中<ul>
<li>running_task：查询db获取所有处于running的TASK的信息-&gt;遍历每个TASK的pid，查看是否存在(通过kill(pid,0)检查）。如不存在（不在运行），则存入stop_job_ids-&gt; 遍历stop_job_ids，发起stop请求(此处仍为异步)-&gt;打印出running状态的task数量。</li>
<li>running_job：查询db获取所有处于running的JOB的信息-&gt;遍历每个JOB判断是否超时（默认超时限制为3天）-&gt; 对超时的job发起stop请求。</li>
<li>resource_record：回收资源，查询DB，获取所有资源处于使用中，且任务状态已经结束，并且申请资源时间超过600s的任务，依次遍历任务回收资源。</li>
<li>expired_session：查询过期(超过5小时)session ，依次遍历并终止</li>
</ul>
</li>
</ol>
<p>注意，这里打日志，是调用log.py 中 detect_log() 方法，而不是 settings.py 中的LoggerFactory.getLogger(“fate_flow_detect”)<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 07:14:11,255] [1:140691103205120] - detector.py[line:38]: start to detect running task..</span><br><span class="line">[INFO] [2021-07-26 07:14:11,264] [1:140691103205120] - detector.py[line:70]: finish detect 0 running task</span><br><span class="line">[INFO] [2021-07-26 07:14:11,264] [1:140691103205120] - detector.py[line:74]: start detect running job</span><br><span class="line">[INFO] [2021-07-26 07:14:11,272] [1:140691103205120] - detector.py[line:88]: finish detect running job</span><br><span class="line">[INFO] [2021-07-26 07:14:11,273] [1:140691103205120] - detector.py[line:93]: start detect resource recycle</span><br><span class="line">[INFO] [2021-07-26 07:14:11,280] [1:140691103205120] - detector.py[line:116]: finish detect resource recycle</span><br><span class="line">[INFO] [2021-07-26 07:14:11,280] [1:140691103205120] - detector.py[line:120]: start detect expired session</span><br></pre></td></tr></table></figure></p>
<ol>
<li>fate_flow_server.py：启动DagScheduler，每隔2秒轮询，调度处于waiting状态的任务</li>
<li>dag_scheduler.py：调度job，依次调度如下五部分</li>
</ol>
<ul>
<li>调度处于waiting 状态的job:schedule_waiting_jobs(job=job)，调用start_job 启动任务 ，调度的日志输出在fate_flow/fate_flow_scheduler.log 中，start_job 中，再调用FederatedScheduler.start_job(job=job)的日志，就输出到jobid/fate_flow_scheduler.log 中了。<br>这里每次只能调度一个处于waiting 状态的任务，通过order_by=”create_time” 和  job = jobs[0] 实现</li>
<li>调度处于running 状态的job:，不同于waiting，因为是异步提交，考虑到资源问题，每次只调度一个，running状态的调度，主要检查状态等，故从DB中获取到所有running状态的任务后，都会依次遍历，调用schedule_running_job(job=job)，是判断任务是否被取消，任务状态为结束时，保存模型。并调用FederatedScheduler.sync_job_status(job=job)同步任务状态。</li>
<li>调度ready状态的job:schedule_ready_job(job=job)</li>
<li>调度rerun任务的job:schedule_rerun_job(job=job)</li>
<li>更新已经为endstatus的job的status：end_scheduling_updates(job_id=job.f_job_id)</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 07:14:08,324] [1:140259369826048] - dag_scheduler.py[line:134]: start schedule waiting jobs</span><br><span class="line">[INFO] [2021-07-26 07:14:08,339] [1:140259369826048] - dag_scheduler.py[line:136]: have 0 waiting jobs</span><br><span class="line">[INFO] [2021-07-26 07:14:08,339] [1:140259369826048] - dag_scheduler.py[line:146]: schedule waiting jobs finished</span><br><span class="line">[INFO] [2021-07-26 07:14:08,339] [1:140259369826048] - dag_scheduler.py[line:148]: start schedule running jobs</span><br><span class="line">[INFO] [2021-07-26 07:14:08,348] [1:140259369826048] - dag_scheduler.py[line:150]: have 0 running jobs</span><br><span class="line">[INFO] [2021-07-26 07:14:08,349] [1:140259369826048] - dag_scheduler.py[line:158]: schedule running jobs finished</span><br><span class="line">[INFO] [2021-07-26 07:14:08,349] [1:140259369826048] - dag_scheduler.py[line:161]: start schedule ready jobs</span><br><span class="line">[INFO] [2021-07-26 07:14:08,359] [1:140259369826048] - dag_scheduler.py[line:163]: have 0 ready jobs</span><br><span class="line">[INFO] [2021-07-26 07:14:08,359] [1:140259369826048] - dag_scheduler.py[line:171]: schedule ready jobs finished</span><br><span class="line">[INFO] [2021-07-26 07:14:08,359] [1:140259369826048] - dag_scheduler.py[line:173]: start schedule rerun jobs</span><br><span class="line">[INFO] [2021-07-26 07:14:08,367] [1:140259369826048] - dag_scheduler.py[line:175]: have 0 rerun jobs</span><br><span class="line">[INFO] [2021-07-26 07:14:08,367] [1:140259369826048] - dag_scheduler.py[line:183]: schedule rerun jobs finished</span><br><span class="line">[INFO] [2021-07-26 07:14:08,368] [1:140259369826048] - dag_scheduler.py[line:185]: start schedule end status jobs to update status</span><br><span class="line">[INFO] [2021-07-26 07:14:08,375] [1:140259369826048] - dag_scheduler.py[line:187]: have 0 end status jobs</span><br><span class="line">[INFO] [2021-07-26 07:14:08,376] [1:140259369826048] - dag_scheduler.py[line:199]: schedule end status jobs finished</span><br></pre></td></tr></table></figure>
<p>每次查询各状态的任务时，都会操作db，对应的peewee.log 日志类似<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[DEBUG] [2021-07-26 07:14:08,260] [1:140691094812416] - peewee.py[line:2863]: (&#x27;SELECT `t1`.`f_create_time`, `t1`.`f_create_date`, `t1`.`f_update_time`, `t1`.`f_update_date`, `t1`.`f_user_id`, `t1`.`f_job_id`, `t1`.`f_name`, `t1`.`f_description`, `t1`.`f_tag`, `t1`.`f_dsl`, `t1`.`f_runtime_conf`, `t1`.`f_runtime_conf_on_party`, `t1`.`f_train_runtime_conf`, `t1`.`f_roles`, `t1`.`f_work_mode`, `t1`.`f_initiator_role`, `t1`.`f_initiator_party_id`, `t1`.`f_status`, `t1`.`f_status_code`, `t1`.`f_role`, `t1`.`f_party_id`, `t1`.`f_is_initiator`, `t1`.`f_progress`, `t1`.`f_ready_signal`, `t1`.`f_ready_time`, `t1`.`f_cancel_signal`, `t1`.`f_cancel_time`, `t1`.`f_rerun_signal`, `t1`.`f_end_scheduling_updates`, `t1`.`f_engine_name`, `t1`.`f_engine_type`, `t1`.`f_cores`, `t1`.`f_memory`, `t1`.`f_remaining_cores`, `t1`.`f_remaining_memory`, `t1`.`f_resource_in_use`, `t1`.`f_apply_resource_time`, `t1`.`f_return_resource_time`, `t1`.`f_start_time`, `t1`.`f_start_date`, `t1`.`f_end_time`, `t1`.`f_end_date`, `t1`.`f_elapsed` FROM `t_job` AS `t1` WHERE ((`t1`.`f_is_initiator` = %s) AND (`t1`.`f_status` = %s)) ORDER BY `t1`.`f_create_time` ASC&#x27;, [True, &#x27;waiting&#x27;])</span><br></pre></td></tr></table></figure></p>
<ol>
<li>fate_flow_server.py：启动grpc server服务，用于不同的rollsite 通信<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 07:14:06,255] [1:140691673888576] - fate_flow_server.py[line:107]: start grpc server thread pool by 40 max workers</span><br><span class="line">[INFO] [2021-07-26 07:14:06,268] [1:140691673888576] - fate_flow_server.py[line:115]: FATE Flow grpc server start successfully</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>fate_flow_server.py：启动http server服务，用于处理本地fate_flow_client 和 fate_flow_server 之间的通信。<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 07:14:06,269] [1:140691673888576] - fate_flow_server.py[line:118]: FATE Flow http server start...</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h1 id="REF"><a href="#REF" class="headerlink" title="REF"></a>REF</h1><p><a href="https://flask.palletsprojects.com/en/2.0.x/">Flask</a><br><a href="https://os.51cto.com/art/202105/664200.htm">os.kill(pid,0)</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2023/08/20/fate-log-1-dir-struct/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="喵十八">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="喵十八の小窝">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/08/20/fate-log-1-dir-struct/" class="post-title-link" itemprop="url">FATE学习：跟着日志读源码（一）日志目录结构</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2023-08-20 12:10:35 / Modified: 12:18:49" itemprop="dateCreated datePublished" datetime="2023-08-20T12:10:35+08:00">2023-08-20</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/FATE/" itemprop="url" rel="index"><span itemprop="name">FATE</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h1><p>介绍fate运行过程中产生的各个类型日志</p>
<h1 id="版本说明"><a href="#版本说明" class="headerlink" title="版本说明"></a>版本说明</h1><p>部署方式为KubeFATE。相关版本为：</p>
<ul>
<li>FATE: 1.5.1</li>
<li>KubeFATE: v1.3.0</li>
</ul>
<h1 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h1><ul>
<li>job：一个完整的调度单元，可以拆分成不同的task。</li>
<li>task：最小的执行单元。</li>
<li>component：预先定义的不同组件，具体的task执行，就是运行不同的组件。</li>
</ul>
<h1 id="FATE框架"><a href="#FATE框架" class="headerlink" title="FATE框架"></a>FATE框架</h1><p>根据官方的架构图，在单个Party的上FATE-Flow 的架构如下<br><img src="/image/fate/fea45d92e84d4785acd417e65dc420d7.png" alt=""></p>
<p>整体分成6大块：</p>
<ul>
<li>FATE-Flow Client: 客户端，提供给用户的交互接口，日常提交的命令<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python fate_flow_client.py -f submit_job -c examples/test_hetero_lr_job_conf.json -d examples/test_hetero_lr_job_dsl.json</span><br></pre></td></tr></table></figure>
就是调用了客户端。</li>
<li>FATE-Flow Server: 服务端，用于处理Client 提交的各类请求。</li>
<li>FATE-Board: Web UI，用于监控FATE 任务的运行情况。</li>
<li>Transfer : 用来处理多方联邦时的交互和通信</li>
<li>Metadata Service: 对联邦过程，产生的各类Metadata进行存储、管理并提供查询</li>
<li>Local FileSystem: 本地文件系统，进行持久化</li>
<li>Storage &amp; Computing: 底层存储和计算引擎</li>
</ul>
<p>其中 FATE-Flow Server 最为复杂，</p>
<ul>
<li>API Service: 处理Client 提交的各类请求</li>
<li>Job Queue: 异步调度的方式，所有提交的job 先进入job queue （见后文，所有job创建完毕，都是waiting状态）,根据资源情况，进行下发（调度策略是啥？？）</li>
<li>DAG Scheduler: 每个job 会根据涉及的计算内容，解析为一个DAG（有向无环图），图中的每个节点为一个task。针对每个job，通过一个DAG Scheduler 进行该job下涉及task 的调度。</li>
<li>Task Scheduler: task 在本Party的调度，涉及到跨多个Party 会调用Federated Task Scheduler 进行调度。</li>
<li>Federated Task Scheduler:涉及到跨多个Party 的Task 的调度</li>
<li>Executor: 联邦任务执行节点，支持不同的 Operator 容器。这里会调用Framework </li>
<li>Framework: 计算框架抽象层，根据backend 的设置 调用底层的Storage &amp; Computing，同时若为联邦任务，则和Transfer 进行通信。</li>
<li>DSL Parser: 是调度的核心，通过 DSL parser 解析到一个计算任务的上下游关系及依赖等。</li>
<li>Controller: 任务的控制器</li>
<li>Model Registry: 模型管理器</li>
<li>Tracking Manager：任务输入输出的实时追踪，包括每个 task 输出的 data 和 model。</li>
</ul>
<h1 id="日志分类"><a href="#日志分类" class="headerlink" title="日志分类"></a>日志分类</h1><p>从FATE的框架，可以看出FATE 涉及大量的http/grpc 请求， 以及DB操作，因而按照日志的操作来源分，主要有一下三类：</p>
<ul>
<li>http/grpc 请求日志</li>
<li>操作DB的日志</li>
<li>fate 框架运行时的日志</li>
</ul>
<p>而从日志输出的位置来看，则可以分为如下三类：</p>
<ul>
<li>Console 日志</li>
<li>容器日志</li>
<li>FATE Flow 日志</li>
</ul>
<p>鉴于按照位置来看，可以更好的体现出整个执行的时序，下文会按照这个维度，来进行解析</p>
<h1 id="Console-日志"><a href="#Console-日志" class="headerlink" title="Console 日志"></a>Console 日志</h1><p>控制台部分，也是就正常提交任务之后会产生的日志，例如 执行命令<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python fate_flow_client.py -f upload -c examples/upload_guest.json</span><br></pre></td></tr></table></figure></p>
<p>返回的日志为<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;data&quot;: &#123;</span><br><span class="line">        &quot;board_url&quot;: &quot;http://fateboard:8080/index.html#/dashboard?job_id=20210720163310959474309&amp;role=local&amp;party_id=0&quot;,</span><br><span class="line">        &quot;job_dsl_path&quot;: &quot;/data/projects/fate/jobs/20210720163310959474309/job_dsl.json&quot;,</span><br><span class="line">        &quot;job_id&quot;: &quot;20210720163310959474309&quot;,</span><br><span class="line">        &quot;job_runtime_conf_on_party_path&quot;: &quot;/data/projects/fate/jobs/20210720163310959474309/local/job_runtime_on_party_conf.json&quot;,</span><br><span class="line">        &quot;job_runtime_conf_path&quot;: &quot;/data/projects/fate/jobs/20210720163310959474309/job_runtime_conf.json&quot;,</span><br><span class="line">        &quot;logs_directory&quot;: &quot;/data/projects/fate/logs/20210720163310959474309&quot;,</span><br><span class="line">        &quot;model_info&quot;: &#123;</span><br><span class="line">            &quot;model_id&quot;: &quot;local-0#model&quot;,</span><br><span class="line">            &quot;model_version&quot;: &quot;20210720163310959474309&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;namespace&quot;: &quot;cl&quot;,</span><br><span class="line">        &quot;pipeline_dsl_path&quot;: &quot;/data/projects/fate/jobs/20210720163310959474309/pipeline_dsl.json&quot;,</span><br><span class="line">        &quot;table_name&quot;: &quot;bgame3342_20210719&quot;,</span><br><span class="line">        &quot;train_runtime_conf_path&quot;: &quot;/data/projects/fate/jobs/20210720163310959474309/train_runtime_conf.json&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;jobId&quot;: &quot;20210720163310959474309&quot;,</span><br><span class="line">    &quot;retcode&quot;: 0,</span><br><span class="line">    &quot;retmsg&quot;: &quot;success&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>包含4个部分：</p>
<ul>
<li>data: 提交任务的相关参数</li>
<li>jobId: FATE为该job分配的id</li>
<li>retcode: 提交请求的返回状态码</li>
<li>retmsg: 提交请求返回的状态信息。如不成功，会有相关报错。<br>这里需要关注的日志信息，只有retmsg。</li>
</ul>
<h1 id="Server日志"><a href="#Server日志" class="headerlink" title="Server日志"></a>Server日志</h1><p>这一部分日志，可以在部署fate_flow server 的容器中看到。<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">10.200.224.54 - - [20/Jul/2021 16:33:12] &quot;POST /v1/party/20210720163310959474309/local/0/create HTTP/1.1&quot; 200 -</span><br><span class="line">10.200.224.54 - - [20/Jul/2021 16:33:12] &quot;POST /v1/data/upload?%7B%22file%22:%20%22/data/projects/fate/python/cl/fc/data/predict_data/20210719/bgame3342_20210719.csv%22,%20%22head%22:%201,%20%22partition%22:%201,%20%22work_mode%22:%201,%20%22table_name%22:%20%22bgame3342_20210719%22,%20%22namespace%22:%20%22cl%22,%20%22count%22:%20752361,%20%22config%22:%20%22/data/projects/fate/python/cl/fc/config/20210719/upload_20210719_bgame3342.json%22,%20%22function%22:%20%22upload%22,%20%22drop%22:%20%221%22%7D HTTP/1.1&quot; 200 -</span><br><span class="line">10.200.224.54 - - [20/Jul/2021 16:33:14] &quot;POST /v1/party/20210720163310959474309/local/0/resource/apply HTTP/1.1&quot; 200 -</span><br><span class="line">10.200.224.54 - - [20/Jul/2021 16:33:14] &quot;POST /v1/party/20210720163310959474309/local/0/start HTTP/1.1&quot; 200 -</span><br></pre></td></tr></table></figure><br>记录了 FATE-Flow Server 收到的POST请求。<br>这一部分主要是federate_scheduler.py 向server 发起的POST 请求。</p>
<h1 id="FATE-Flow-日志"><a href="#FATE-Flow-日志" class="headerlink" title="FATE Flow 日志"></a>FATE Flow 日志</h1><p>这一部分是fate运行过程中的主要产生的日志，用来排查fate job、task的运行状况。目录位于kubefate 的容器中， /data/projects/fate/logs/<br>其中 fate_flow 目录记录了fate 启动以来的总体的日志。 各个${jobid} 目录下，记录了各个job详细的日志。<br>相关处理类：python/fate_arch/common/log.py</p>
<h2 id="fate-flow-目录下日志分类"><a href="#fate-flow-目录下日志分类" class="headerlink" title="fate_flow 目录下日志分类"></a>fate_flow 目录下日志分类</h2><p>总的结构类似，分成如下几个类型的日志</p>
<ul>
<li>DEBUG.log</li>
<li>INFO.log</li>
<li>ERROR.log</li>
<li>WARNING.log</li>
<li>peewee.log</li>
<li>stat.log</li>
<li>fate_flow_audit.log </li>
<li>fate_flow_detect.log</li>
<li>fate_flow_stat.log</li>
<li>fate_flow_schedule.log </li>
</ul>
<h3 id="DEBUG、INFO、ERROR、WARNING"><a href="#DEBUG、INFO、ERROR、WARNING" class="headerlink" title="DEBUG、INFO、ERROR、WARNING"></a>DEBUG、INFO、ERROR、WARNING</h3><p>正常四个级别的日志输出</p>
<h3 id="peewee-log"><a href="#peewee-log" class="headerlink" title="peewee.log"></a>peewee.log</h3><p>fate 操作数据是使用peewee 这一ORM 实现的，peewee.log 记录了相关操作的日志。关于peewee 可以参考<a href="http://docs.peewee-orm.com/en/latest/index.html">官方文档</a></p>
<h3 id="stat-log"><a href="#stat-log" class="headerlink" title="stat.log"></a>stat.log</h3><p>记录了各param 的check ，感觉可有可无，不知道单独放一个日志的设计意图。</p>
<p>样例<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[DEBUG] [2021-07-26 15:26:37,778] [1150092:140485761271616] - intersect_param.py[line:61]: Finish encode parameter check!</span><br><span class="line">[DEBUG] [2021-07-26 15:26:37,778] [1150092:140485761271616] - intersect_param.py[line:184]: Finish intersect parameter check!</span><br><span class="line">[DEBUG] [2021-07-26 15:26:37,778] [1150092:140485761271616] - intersect_param.py[line:61]: Finish encode parameter check!</span><br><span class="line">[DEBUG] [2021-07-26 15:26:37,778] [1150092:140485761271616] - intersect_param.py[line:184]: Finish intersect parameter check!</span><br><span class="line">[DEBUG] [2021-07-26 15:26:37,780] [1150092:140485761271616] - encrypt_param.py[line:71]: Finish encrypt parameter check!</span><br><span class="line">[DEBUG] [2021-07-26 15:26:37,780] [1150092:140485761271616] - predict_param.py[line:44]: Finish predict parameter check!</span><br><span class="line">[DEBUG] [2021-07-26 15:26:37,781] [1150092:140485761271616] - encrypt_param.py[line:71]: Finish encrypt parameter check!</span><br><span class="line">[DEBUG] [2021-07-26 15:26:37,781] [1150092:140485761271616] - predict_param.py[line:44]: Finish predict parameter check!</span><br><span class="line">[DEBUG] [2021-07-26 15:26:37,782] [1150092:140485761271616] - encrypt_param.py[line:71]: Finish encrypt parameter check!</span><br><span class="line">[DEBUG] [2021-07-26 15:26:37,782] [1150092:140485761271616] - predict_param.py[line:44]: Finish predict parameter check!</span><br></pre></td></tr></table></figure></p>
<h3 id="fate-flow-audit-log"><a href="#fate-flow-audit-log" class="headerlink" title="fate_flow_audit.log"></a>fate_flow_audit.log</h3><p>记录发起的grpc 请求。函数为 audit_logger。有调用的地方在<br>python/fate_flow/utils/api_utils.py 中的<br>federated_coordination_on_grpc<br>federated_coordination_on_http<br>forward_api</p>
<p>和 python/fate_flow/utils/grpc_utils.py 中的<br>unaryCall</p>
<p>日志样例：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-05-18 08:00:26,886] [1:140215515608832] - grpc_utils.py[line:93]: rpc receive: header &#123;</span><br><span class="line">  task &#123;</span><br><span class="line">    taskId: &quot;2021051808002667189911&quot;</span><br><span class="line">  &#125;</span><br><span class="line">  src &#123;</span><br><span class="line">    name: &quot;2021051808002667189911&quot;</span><br><span class="line">    partyId: &quot;9999&quot;</span><br><span class="line">    role: &quot;fateflow&quot;</span><br><span class="line">    callback &#123;</span><br><span class="line">      ip: &quot;10.200.96.205&quot;</span><br><span class="line">      port: 9360</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  dst &#123;</span><br><span class="line">    name: &quot;2021051808002667189911&quot;</span><br><span class="line">    partyId: &quot;10000&quot;</span><br><span class="line">    role: &quot;fateflow&quot;</span><br><span class="line">  &#125;</span><br><span class="line">  command &#123;</span><br><span class="line">    name: &quot;fateflow&quot;</span><br><span class="line">  &#125;</span><br><span class="line">  operator: &quot;POST&quot;</span><br><span class="line">  conf &#123;</span><br><span class="line">    overallTimeout: 30000</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">body &#123;</span><br><span class="line">  key: &quot;/v1/party/2021051808002667189911/host/10000/create&quot;</span><br><span class="line">  value: &quot;&#123;\&quot;name\&quot;: \&quot;\&quot;, \&quot;description\&quot;: \&quot;\&quot;, \&quot;tag\&quot;: \&quot;\&quot;, \&quot;is_initiator\&quot;: false, \&quot;progress\&quot;: 0, \&quot;ready_signal\&quot;: false, \&quot;cancel_signal\&quot;: false, \&quot;rerun_signal\&quot;: false, \&quot;end_scheduling_updates\&quot;: 0, \&quot;cores\&quot;: 0, \&quot;memory\&quot;: 0, \&quot;remaining_cores\&quot;: 0, \&quot;remaining_memory\&quot;: 0, \&quot;resource_in_use\&quot;: false, \&quot;job_id\&quot;: \&quot;2021051808002667189911\&quot;, \&quot;dsl\&quot;: &#123;\&quot;components\&quot;: &#123;\&quot;dataio_0\&quot;: &#123;\&quot;module\&quot;: \&quot;DataIO\&quot;, \&quot;input\&quot;: &#123;\&quot;model\&quot;: [\&quot;pipeline.dataio_0.dataio\&quot;], \&quot;data\&quot;: &#123;\&quot;data\&quot;: [\&quot;args.eval_data\&quot;]&#125;&#125;, \&quot;output\&quot;: &#123;\&quot;data\&quot;: [\&quot;train\&quot;]&#125;, \&quot;CodePath\&quot;: \&quot;federatedml/util/data_io.py/DataIO\&quot;&#125;, \&quot;intersection_0\&quot;: &#123;\&quot;module\&quot;: \&quot;Intersection\&quot;, \&quot;output\&quot;: &#123;\&quot;data\&quot;: [\&quot;train\&quot;]&#125;, \&quot;input\&quot;: &#123;\&quot;data\&quot;: &#123;\&quot;data\&quot;: [\&quot;dataio_0.train\&quot;]&#125;&#125;, \&quot;CodePath\&quot;: \&quot;federatedml/statistic/intersect/intersect_model.py/IntersectGuest\&quot;&#125;, \&quot;secureboost_0\&quot;: &#123;\&quot;module\&quot;: \&quot;HeteroSecureBoost\&quot;, \&quot;input\&quot;: &#123;\&quot;model\&quot;: [\&quot;pipeline.secureboost_0.train\&quot;], \&quot;data\&quot;: &#123;\&quot;test_data\&quot;: [\&quot;intersection_0.train\&quot;]&#125;&#125;, \&quot;output\&quot;: &#123;\&quot;data\&quot;: [\&quot;train\&quot;]&#125;, \&quot;CodePath\&quot;: \&quot;federatedml/ensemble/boosting/hetero/hetero_secureboost_guest.py/HeteroSecureBoostingTreeGuest\&quot;&#125;&#125;&#125;, \&quot;train_runtime_conf\&quot;: &#123;\&quot;initiator\&quot;: &#123;\&quot;role\&quot;: \&quot;guest\&quot;, \&quot;party_id\&quot;: 9999&#125;, \&quot;job_parameters\&quot;: &#123;\&quot;job_type\&quot;: \&quot;train\&quot;, \&quot;work_mode\&quot;: 1, \&quot;backend\&quot;: 0, \&quot;computing_engine\&quot;: \&quot;EGGROLL\&quot;, \&quot;federation_engine\&quot;: \&quot;EGGROLL\&quot;, \&quot;storage_engine\&quot;: \&quot;EGGROLL\&quot;, \&quot;engines_address\&quot;: &#123;&#125;, \&quot;federated_mode\&quot;: \&quot;MULTIPLE\&quot;, \&quot;task_cores\&quot;: 2, \&quot;task_parallelism\&quot;: 1, \&quot;computing_partitions\&quot;: 2, \&quot;federated_status_collect_type\&quot;: \&quot;PULL\&quot;, \&quot;model_id\&quot;: \&quot;guest-9999#host-10000#model\&quot;, \&quot;model_version\&quot;: \&quot;2021051807524507002410\&quot;, \&quot;eggroll_run\&quot;: &#123;&#125;, \&quot;spark_run\&quot;: &#123;&#125;, \&quot;rabbitmq_run\&quot;: &#123;&#125;, \&quot;adaptation_parameters\&quot;: &#123;\&quot;task_nodes\&quot;: 1, \&quot;task_cores_per_node\&quot;: 2, \&quot;task_memory_per_node\&quot;: 0, \&quot;request_task_cores\&quot;: 2, \&quot;if_initiator_baseline\&quot;: true&#125;&#125;, \&quot;role\&quot;: &#123;\&quot;guest\&quot;: [9999], \&quot;host\&quot;: [10000]&#125;, \&quot;role_parameters\&quot;: &#123;\&quot;guest\&quot;: &#123;\&quot;args\&quot;: &#123;\&quot;data\&quot;: &#123;\&quot;train_data\&quot;: [&#123;\&quot;name\&quot;: \&quot;breast_hetero_guest\&quot;, \&quot;namespace\&quot;: \&quot;experiment\&quot;&#125;], \&quot;eval_data\&quot;: [&#123;\&quot;name\&quot;: \&quot;breast_hetero_guest\&quot;, \&quot;namespace\&quot;: \&quot;experiment\&quot;&#125;]&#125;&#125;, \&quot;dataio_0\&quot;: &#123;\&quot;with_label\&quot;: [true], \&quot;label_name\&quot;: [\&quot;y\&quot;], \&quot;label_type\&quot;: [\&quot;int\&quot;], \&quot;output_format\&quot;: [\&quot;dense\&quot;]&#125;&#125;, \&quot;host\&quot;: &#123;\&quot;args\&quot;: &#123;\&quot;data\&quot;: &#123;\&quot;train_data\&quot;: [&#123;\&quot;name\&quot;: \&quot;breast_hetero_host\&quot;, \&quot;namespace\&quot;: \&quot;experiment\&quot;&#125;], \&quot;eval_data\&quot;: [&#123;\&quot;name\&quot;: \&quot;breast_hetero_host\&quot;, \&quot;namespace\&quot;: \&quot;experiment\&quot;&#125;]&#125;&#125;, \&quot;dataio_0\&quot;: &#123;\&quot;with_label\&quot;: [false], \&quot;output_format\&quot;: [\&quot;dense\&quot;]&#125;&#125;&#125;, \&quot;algorithm_parameters\&quot;: &#123;\&quot;secureboost_0\&quot;: &#123;\&quot;task_type\&quot;: \&quot;classification\&quot;, \&quot;learning_rate\&quot;: 0.1, \&quot;num_trees\&quot;: 5, \&quot;subsample_feature_rate\&quot;: 1, \&quot;n_iter_no_change\&quot;: false, \&quot;tol\&quot;: 0.0001, \&quot;bin_num\&quot;: 50, \&quot;objective_param\&quot;: &#123;\&quot;objective\&quot;: \&quot;cross_entropy\&quot;&#125;, \&quot;encrypt_param\&quot;: &#123;\&quot;method\&quot;: \&quot;paillier\&quot;&#125;, \&quot;predict_param\&quot;: &#123;\&quot;threshold\&quot;: 0.5&#125;, \&quot;cv_param\&quot;: &#123;\&quot;n_splits\&quot;: 5, \&quot;shuffle\&quot;: false, \&quot;random_seed\&quot;: 103, \&quot;need_cv\&quot;: false&#125;, \&quot;validation_freqs\&quot;: 1&#125;, \&quot;evaluation_0\&quot;: &#123;\&quot;eval_type\&quot;: \&quot;binary\&quot;&#125;&#125;, \&quot;config\&quot;: \&quot;/data/projects/fate/examples/min_test_task/test/submit_job_guest.config_1621324363_9498\&quot;, \&quot;dsl\&quot;: \&quot;/data/projects/fate/examples/min_test_task/config/test_secureboost_train_dsl.json\&quot;, \&quot;function\&quot;: \&quot;submit_job\&quot;&#125;, \&quot;roles\&quot;: &#123;\&quot;guest\&quot;: [9999], \&quot;host\&quot;: [10000], \&quot;arbiter\&quot;: []&#125;, \&quot;work_mode\&quot;: 1, \&quot;initiator_role\&quot;: \&quot;guest\&quot;, \&quot;initiator_party_id\&quot;: 9999, \&quot;role\&quot;: \&quot;guest\&quot;, \&quot;party_id\&quot;: 9999, \&quot;runtime_conf\&quot;: &#123;\&quot;initiator\&quot;: &#123;\&quot;role\&quot;: \&quot;guest\&quot;, \&quot;party_id\&quot;: 9999&#125;, \&quot;job_parameters\&quot;: &#123;\&quot;job_type\&quot;: \&quot;predict\&quot;, \&quot;work_mode\&quot;: 1, \&quot;backend\&quot;: 0, \&quot;computing_engine\&quot;: \&quot;EGGROLL\&quot;, \&quot;federation_engine\&quot;: \&quot;EGGROLL\&quot;, \&quot;storage_engine\&quot;: \&quot;EGGROLL\&quot;, \&quot;engines_address\&quot;: &#123;&#125;, \&quot;federated_mode\&quot;: \&quot;MULTIPLE\&quot;, \&quot;task_parallelism\&quot;: 1, \&quot;computing_partitions\&quot;: 4, \&quot;federated_status_collect_type\&quot;: \&quot;PULL\&quot;, \&quot;model_id\&quot;: \&quot;guest-9999#host-10000#model\&quot;, \&quot;model_version\&quot;: \&quot;2021051807524507002410\&quot;, \&quot;eggroll_run\&quot;: &#123;&#125;, \&quot;spark_run\&quot;: &#123;&#125;, \&quot;rabbitmq_run\&quot;: &#123;&#125;, \&quot;adaptation_parameters\&quot;: &#123;\&quot;task_nodes\&quot;: 1, \&quot;task_cores_per_node\&quot;: 4, \&quot;task_memory_per_node\&quot;: 0, \&quot;request_task_cores\&quot;: 4, \&quot;if_initiator_baseline\&quot;: true&#125;&#125;, \&quot;role\&quot;: &#123;\&quot;guest\&quot;: [9999], \&quot;host\&quot;: [10000], \&quot;arbiter\&quot;: []&#125;, \&quot;role_parameters\&quot;: &#123;\&quot;guest\&quot;: &#123;\&quot;args\&quot;: &#123;\&quot;data\&quot;: &#123;\&quot;eval_data\&quot;: [&#123;\&quot;name\&quot;: \&quot;breast_hetero_guest\&quot;, \&quot;namespace\&quot;: \&quot;experiment\&quot;&#125;]&#125;&#125;&#125;, \&quot;host\&quot;: &#123;\&quot;args\&quot;: &#123;\&quot;data\&quot;: &#123;\&quot;eval_data\&quot;: [&#123;\&quot;name\&quot;: \&quot;breast_hetero_host\&quot;, \&quot;namespace\&quot;: \&quot;experiment\&quot;&#125;]&#125;&#125;&#125;&#125;, \&quot;config\&quot;: \&quot;/data/projects/fate/examples/min_test_task/test/submit_job_guest.config_1621324825_2041\&quot;, \&quot;function\&quot;: \&quot;submit_job\&quot;&#125;, \&quot;runtime_conf_on_party\&quot;: &#123;\&quot;initiator\&quot;: &#123;\&quot;role\&quot;: \&quot;guest\&quot;, \&quot;party_id\&quot;: 9999&#125;, \&quot;job_parameters\&quot;: &#123;\&quot;job_type\&quot;: \&quot;predict\&quot;, \&quot;work_mode\&quot;: 1, \&quot;backend\&quot;: 0, \&quot;computing_engine\&quot;: \&quot;EGGROLL\&quot;, \&quot;federation_engine\&quot;: \&quot;EGGROLL\&quot;, \&quot;storage_engine\&quot;: \&quot;EGGROLL\&quot;, \&quot;engines_address\&quot;: &#123;&#125;, \&quot;federated_mode\&quot;: \&quot;MULTIPLE\&quot;, \&quot;task_parallelism\&quot;: 1, \&quot;computing_partitions\&quot;: 4, \&quot;federated_status_collect_type\&quot;: \&quot;PULL\&quot;, \&quot;model_id\&quot;: \&quot;guest-9999#host-10000#model\&quot;, \&quot;model_version\&quot;: \&quot;2021051807524507002410\&quot;, \&quot;eggroll_run\&quot;: &#123;&#125;, \&quot;spark_run\&quot;: &#123;&#125;, \&quot;rabbitmq_run\&quot;: &#123;&#125;, \&quot;adaptation_parameters\&quot;: &#123;\&quot;task_nodes\&quot;: 1, \&quot;task_cores_per_node\&quot;: 4, \&quot;task_memory_per_node\&quot;: 0, \&quot;request_task_cores\&quot;: 4, \&quot;if_initiator_baseline\&quot;: true&#125;&#125;, \&quot;role\&quot;: &#123;\&quot;guest\&quot;: [9999], \&quot;host\&quot;: [10000], \&quot;arbiter\&quot;: []&#125;, \&quot;role_parameters\&quot;: &#123;\&quot;guest\&quot;: &#123;\&quot;args\&quot;: &#123;\&quot;data\&quot;: &#123;\&quot;eval_data\&quot;: [&#123;\&quot;name\&quot;: \&quot;breast_hetero_guest\&quot;, \&quot;namespace\&quot;: \&quot;experiment\&quot;&#125;]&#125;&#125;&#125;, \&quot;host\&quot;: &#123;\&quot;args\&quot;: &#123;\&quot;data\&quot;: &#123;\&quot;eval_data\&quot;: [&#123;\&quot;name\&quot;: \&quot;breast_hetero_host\&quot;, \&quot;namespace\&quot;: \&quot;experiment\&quot;&#125;]&#125;&#125;&#125;&#125;, \&quot;config\&quot;: \&quot;/data/projects/fate/examples/min_test_task/test/submit_job_guest.config_1621324825_2041\&quot;, \&quot;function\&quot;: \&quot;submit_job\&quot;&#125;, \&quot;src_role\&quot;: \&quot;guest\&quot;, \&quot;src_party_id\&quot;: 9999&#125;&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="fate-flow-detect-log"><a href="#fate-flow-detect-log" class="headerlink" title="fate_flow_detect.log"></a>fate_flow_detect.log</h3><p>detector.py 产生的日志，涉及如下函数</p>
<ul>
<li>detect_running_task：探测正在运行的task</li>
<li>detect_running_job：探测正在运行的job</li>
<li>detect_resource_record：探测资源使用记录</li>
<li>detect_expired_session：探测是否有过期的session，有的话，调用request_stop_jobs进行stop<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 10:01:34,230] [1:139736070596352] - detector.py[line:38]: start to detect running task..</span><br><span class="line">[INFO] [2021-07-26 10:01:34,241] [1:139736070596352] - detector.py[line:70]: finish detect 1 running task</span><br><span class="line">[INFO] [2021-07-26 10:01:34,241] [1:139736070596352] - detector.py[line:74]: start detect running job</span><br><span class="line">[INFO] [2021-07-26 10:01:34,251] [1:139736070596352] - detector.py[line:88]: finish detect running job</span><br><span class="line">[INFO] [2021-07-26 10:01:34,251] [1:139736070596352] - detector.py[line:93]: start detect resource recycle</span><br><span class="line">[INFO] [2021-07-26 10:01:34,255] [1:139736070596352] - detector.py[line:116]: finish detect resource recycle</span><br><span class="line">[INFO] [2021-07-26 10:01:34,255] [1:139736070596352] - detector.py[line:120]: start detect expired session</span><br><span class="line">[INFO] [2021-07-26 10:01:39,260] [1:139736070596352] - detector.py[line:38]: start to detect running task..</span><br><span class="line">[INFO] [2021-07-26 10:01:39,270] [1:139736070596352] - detector.py[line:70]: finish detect 1 running task</span><br><span class="line">[INFO] [2021-07-26 10:01:39,270] [1:139736070596352] - detector.py[line:74]: start detect running job</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="fate-flow-scheduler-log"><a href="#fate-flow-scheduler-log" class="headerlink" title="fate_flow_scheduler.log"></a>fate_flow_scheduler.log</h3><p>这里和${jobid} 目录下的fate_flow_scheduler.log 记录具体job的调度细节不同，只记录最外层的job级别的调度。<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-26 10:02:14,169] [1:139736062203648] - dag_scheduler.py[line:152]: schedule running job 202107260958386097411239</span><br><span class="line">[INFO] [2021-07-26 10:02:55,598] [1:139736062203648] - dag_scheduler.py[line:152]: schedule running job 202107260959086853751240</span><br><span class="line">[INFO] [2021-07-26 10:03:46,124] [1:139736062203648] - dag_scheduler.py[line:152]: schedule running job 202107261000016603571241</span><br><span class="line">[INFO] [2021-07-26 10:04:37,635] [1:139736062203648] - dag_scheduler.py[line:158]: schedule running jobs finished</span><br><span class="line">[INFO] [2021-07-26 10:04:37,635] [1:139736062203648] - dag_scheduler.py[line:161]: start schedule ready jobs</span><br><span class="line">[INFO] [2021-07-26 10:04:37,645] [1:139736062203648] - dag_scheduler.py[line:163]: have 0 ready jobs</span><br><span class="line">[INFO] [2021-07-26 10:04:37,645] [1:139736062203648] - dag_scheduler.py[line:171]: schedule ready jobs finished</span><br><span class="line">[INFO] [2021-07-26 10:04:37,645] [1:139736062203648] - dag_scheduler.py[line:173]: start schedule rerun jobs</span><br><span class="line">[INFO] [2021-07-26 10:04:37,652] [1:139736062203648] - dag_scheduler.py[line:175]: have 0 rerun jobs</span><br><span class="line">[INFO] [2021-07-26 10:04:37,652] [1:139736062203648] - dag_scheduler.py[line:183]: schedule rerun jobs finished</span><br><span class="line">[INFO] [2021-07-26 10:04:37,652] [1:139736062203648] - dag_scheduler.py[line:185]: start schedule end status jobs to update status</span><br><span class="line">[INFO] [2021-07-26 10:04:37,673] [1:139736062203648] - dag_scheduler.py[line:187]: have 5 end status jobs</span><br><span class="line">[INFO] [2021-07-26 10:04:37,674] [1:139736062203648] - dag_scheduler.py[line:189]: schedule end status job 202107260956208255521237</span><br><span class="line">[INFO] [2021-07-26 10:04:37,829] [1:139736062203648] - dag_scheduler.py[line:189]: schedule end status job 202107260956590660001238</span><br><span class="line">[INFO] [2021-07-26 10:04:37,956] [1:139736062203648] - dag_scheduler.py[line:189]: schedule end status job 202107260958386097411239</span><br><span class="line">[INFO] [2021-07-26 10:04:38,088] [1:139736062203648] - dag_scheduler.py[line:189]: schedule end status job 202107260959086853751240</span><br><span class="line">[INFO] [2021-07-26 10:04:38,225] [1:139736062203648] - dag_scheduler.py[line:189]: schedule end status job 202107261000016603571241</span><br></pre></td></tr></table></figure></p>
<h3 id="fate-flow-stat-log"><a href="#fate-flow-stat-log" class="headerlink" title="fate_flow_stat.log"></a>fate_flow_stat.log</h3><p>记录各个进程的状态<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-07-25 17:20:50,581] [1:139738990319424] - job_utils.py[line:345]: child process 676448 exit with exitcode 0</span><br><span class="line">[INFO] [2021-07-25 17:20:50,582] [1:139738990319424] - job_utils.py[line:342]: no child process was immediately available</span><br><span class="line">[INFO] [2021-07-25 17:20:56,256] [1:139738990319424] - job_utils.py[line:345]: child process 676533 exit with exitcode 0</span><br><span class="line">[INFO] [2021-07-25 17:20:56,256] [1:139738990319424] - job_utils.py[line:342]: no child process was immediately available</span><br><span class="line">[INFO] [2021-07-25 17:21:02,272] [1:139738990319424] - job_utils.py[line:345]: child process 676692 exit with exitcode 0</span><br><span class="line">[INFO] [2021-07-25 17:21:02,273] [1:139738990319424] - job_utils.py[line:342]: no child process was immediately available</span><br><span class="line">[INFO] [2021-07-25 17:21:09,043] [1:139738990319424] - job_utils.py[line:345]: child process 676378 exit with exitcode 0</span><br><span class="line">[INFO] [2021-07-25 17:21:09,043] [1:139738990319424] - job_utils.py[line:342]: no child process was immediately available</span><br><span class="line">[INFO] [2021-07-25 17:21:15,706] [1:139738990319424] - job_utils.py[line:345]: child process 676612 exit with exitcode 0</span><br><span class="line">[WARNING] [2021-07-25 17:21:15,707] [1:139738990319424] - job_utils.py[line:348]: current process has no existing unwaited-for child processes.</span><br></pre></td></tr></table></figure></p>
<h2 id="jobid-目录下日志"><a href="#jobid-目录下日志" class="headerlink" title="${jobid} 目录下日志"></a>${jobid} 目录下日志</h2><p>${jobid} 目录下的日志与 fate_flow 的日志不完全一致，主要分成如下几个部分</p>
<ul>
<li>fate_flow_audit.log  </li>
<li>fate_flow_schedule.log  </li>
<li>fate_flow_sql.log</li>
<li>${role}/${party_id}/</li>
</ul>
<h3 id="fate-flow-audit-log-1"><a href="#fate-flow-audit-log-1" class="headerlink" title="fate_flow_audit.log"></a>fate_flow_audit.log</h3><p>同fate_flow 目录下的fate_flow_audit.log，fate_flow 下的fate_flow_audit.log 是各个${jobid} 中fate_flow_audit.log 的集合</p>
<h3 id="fate-flow-schedule-log"><a href="#fate-flow-schedule-log" class="headerlink" title="fate_flow_schedule.log"></a>fate_flow_schedule.log</h3><p>记录fate调度的主要日志，函数为schedule_logger。不同于fate_flow 下的fate_flow_schedule.log，只记录dag_scheduler.py 的日志，调度过程中，记录了众多调用模块的日志，后文详细说明。</p>
<h3 id="fate-flow-sql-log"><a href="#fate-flow-sql-log" class="headerlink" title="fate_flow_sql.log"></a>fate_flow_sql.log</h3><p>记录操作DB的sql 日志。函数为sql_logger，FATE中操作DB的日志，调用在 job_saver.py：将job的各项metrics 写DB 和 save_model_info.py 将模型的相关信息写DB</p>
<p>python/fate_flow/operation/job_saver.py<br>create_job_family_entity 函数，创建job时，将job初始化信息写DB<br>execute_update 函数：更新job相关信息</p>
<p>FATE-1.5.1/python/fate_flow/utils/model_utils.py 中<br>save_model_info 函数，将模型相关信息写DB</p>
<h3 id="role-party-id"><a href="#role-party-id" class="headerlink" title="${role}/${party_id}/"></a>${role}/${party_id}/</h3><p>记录了各个role &amp; party_id 下具体task执行的日志<br>具体目录结构为</p>
<ul>
<li>DEBUG.log</li>
<li>INFO.log</li>
<li>ERROR.log</li>
<li>WARNING.log</li>
<li>${taskName}/DEBUG.log</li>
<li>${taskName}/INFO.log</li>
<li>${taskName}/ERROR.log</li>
<li>${taskName}/WARNING.log</li>
<li>${taskName}/peewee.log</li>
<li>${taskName}/fate_flow_schedule.log</li>
<li>${taskName}/stat.log</li>
<li>${taskName}/std.log</li>
</ul>
<p>其中 DEBUG、INFO、WARNING、ERROR 是各个${taskName} 下对应日志的合集。</p>
<p>${taskName}/fate_flow_schedule.log 记录了task_executor.py 执行该task的日志<br>${taskName}/peewee.log 是该task 通过peewee 操作DB的记录<br>${taskName}/stat.log 记录了该task的相关统计值<br>${taskName}/std.log std 输出</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://wdxtub.com/flt/flt-c1/2021/07/02/">【联邦学习之旅】C1 FATE Flow 源码解析</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2023/08/20/fedai-overview/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="喵十八">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="喵十八の小窝">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/08/20/fedai-overview/" class="post-title-link" itemprop="url">联邦学习哪家强，中国山东找saber</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2023-08-20 11:46:42 / Modified: 12:06:59" itemprop="dateCreated datePublished" datetime="2023-08-20T11:46:42+08:00">2023-08-20</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/FATE/" itemprop="url" rel="index"><span itemprop="name">FATE</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><strong>ERROR：本文要素过多，包含开车、粗鄙之语、魔法、表情包等。但不包含联邦学习的具体说明，相关内容，文末给出传送门，请自行参阅。</strong></p>
<h1 id="先吐槽"><a href="#先吐槽" class="headerlink" title="先吐槽"></a>先吐槽</h1><p>人呐就都不知道，自己不可以预料，一个人的职业发展啊，当然要靠自我奋斗，但是也要考虑技术的演进。我绝对不知道，我作为一个搞大数据的，怎么开始搞联邦学习了？</p>
<p><img src="/image/fate/0d5a5e318d7f8e52bbfa2f45ffa42349.jpg" alt=""></p>
<p>所以当领导给我安排任务，说“公司都决定了，你来调研联邦学习吧”。</p>
<p>我说另请高明吧，我实在也不是谦虚。我一个玩大数据的，一年正经代码写不了几行，到是sql和shell 写的飞起的咸鱼，怎么就能研究联邦学习了呢？但是，领导说“公司已经研究决定了”。后来我就装了两虚拟机。</p>
<p>一台主机名是gou，一台主机名是qi，提醒我，工作再多，也得有健康的身体去享受</p>
<p><img src="/image/fate/6be630a085ee553471e954f804ec2e78.png" alt=""></p>
<h1 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h1><p>说到联邦学习，最近概念有点活，相关在大神遍地的知乎，随便一搜就是了。当然，我有个朋友对此很是不屑，评价到</p>
<blockquote>
<p>联邦学习反映了大数据在面对隐私时既想当xx又想立xx的复杂心情 —by 我的一个朋友</p>
</blockquote>
<p>当然，无论我朋友说啥，反正他又不发我工资，该调研FATE，还是继续调研哈。</p>
<p><img src="/image/fate/d7811ed7c9a0ec4f3e1d9b5455a7bc55.png" alt=""></p>
<p>然后本着博（不）采（务）众（正）长（业）的态度，想看看除了FATE，其他框架的情况，以及好久没写python，就想写个爬虫，练练手的，即复习了python，又能增加装逼素材</p>
<p><img src="/image/fate/143340550b2dd66479ff3a34f1769640.png" alt=""></p>
<p>于是，在github， 使用federated learning关键词搜索，爬取了star数 top30的repository，以及之前有了解的京东的9nfl和 字节跳动的Fedlearner ，一共32个项目的数据。</p>
<p>然而，像腾讯的AngelFL，这种未开源，只见文章的框架，暂时还无法统计。</p>
<p>因为时间精力有限，无法走读所有项目的源码，只能爬取git上的数据，进行一些简单的分析（吹逼），给有志于从事联邦学习的小伙伴在选择入门框架上一（带）些（到）建（坑）议（里）。</p>
<p><img src="/image/fate/c0b77fe777fbd6d6221af27f74d21581.png" alt=""></p>
<h2 id="repository的类型"><a href="#repository的类型" class="headerlink" title="repository的类型"></a>repository的类型</h2><p>我们先从repository 的类型，开始吹起，粗略分了一下类，大致为三分天下，一类是Framework Or System 即联邦学习的框架或者系统，一类是Research Collections即学习资料收集，如研究类文献、paper和最新的idea的收集整理，还有一类是配套的设施，支持，如K8s 解决方案方案 KubeFATE，分布式机器学习 eggroll 等。</p>
<p><img src="/image/fate/4ceb3991d18b35a14529065b3bbfd80f.png" alt=""></p>
<p>总的来看Framework Or System 独占7成。毕竟是个新兴的领域，Research 也不少。而相关的配套就比较少了，只有可怜的6.25%</p>
<h2 id="活跃度"><a href="#活跃度" class="headerlink" title="活跃度"></a>活跃度</h2><p>其次，从issue、commit、pr等指标来看，分别统计了各项指标的排列前三的项目，可以发现FATE 和 tensorflow federated(缩写TFF） 双雄争霸，是社区活跃度最高的两个项目。</p>
<p><img src="/image/fate/c14af37c2188b47e09f9a3b4ae2ceddf.png" alt=""></p>
<p>TFF自不必说，背靠Google这颗大树好乘凉，又有TensorFlow 强大的算法库支持。整个项目规划清晰，文档完善，除了tutorials，连design，openmined2020 都给你贴出来了。假如你之前就是个TensorFlow玩家，给我一个不用TFF 入门的理由？</p>
<p>而FATE</p>
<p><img src="/image/fate/e7fa568f70e6b63871f77d1f6d33cf17.png" alt=""></p>
<p>不好意思，放错图了，是这个</p>
<p><img src="/image/fate/98b49a10bdc993bc245288fa7ce586db.png" alt=""></p>
<p>号称工业级的开源联邦学习框架，在项目的完整性和易用性上，完全不虚TFF，各种文档一应俱全，还有热心网友编写教程，手把手教你怎么玩。项目1.5重构后，整体规划也更加清晰。同时还支持双语教学，对于英语不好的同学来说，简直就是福音。</p>
<p><img src="/image/fate/7735f07951812f4f2cf5ca97c4c830f6.png" alt=""></p>
<p>唯一美中不足的，可能是进化太快了？（看看这个release数，看看这个commit数），一些老的文档已经跟不上最新的版本了。</p>
<p><img src="/image/fate/749b372183589fd41e67e2484d62e4df.png" alt=""></p>
<p>字节跳动的 Fedlearner凭着极其简洁的文档和天马行空的代码结构，能够在开源不久就获得如此之高的star数，看来凭借推荐算法起家的字节跳动，其能力都得到了大家的认可。（访问github 双击鼠标不加star吧？）</p>
<p><img src="/image/fate/53a9e0713e1d6c24cf7e67dd92d4ab4f.png" alt=""></p>
<p>百度的PaddleFL虽然人气比不过上面几个，不过依旧默默努力。加上有源于产业实践的开源深度学习平台 飞桨的加持，愿天道酬勤吧？</p>
<p>FedML 则是一个更加学术性的项目，查了下貌似没啥大公司的加成，更加偏向于一些paper和 idea 的实践和论证，是一个不错的学习和拓宽思路的项目。</p>
<p>xaynet 第一次看到xaynetwork 这个名字，我是陌生的，后来一查，居然是一家德国的公司，作为一个 潜在的巴伐利亚拖拉机主，不禁心头狂喜（deploy 这个项目的时候，需要在机柜边上放个油纸包么？） 的确，欧洲一直以来，没什么让人亮眼的互联网公司，但随着GDPR的出台，欧洲可能也在谋求些什么。</p>
<p><img src="/image/fate/d92f7328044b4a58cb60b631e4a69187.png" alt=""></p>
<p>Train on the Edge with Federated Learning，这个巨大的title，仿佛就是Google 最初联邦学习想法的实现。</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>虽然选了top30 的，但从活跃度的绝对数量上看，头部聚集效应明显，无论是commits on master/main 的数量，还是pr 和 issues 的数量，头部的几位都把后面的远远甩在了身后。</p>
<p><img src="/image/fate/be914640aedcd715d8491f68de188b9d.png" alt=""></p>
<h2 id="使用语言"><a href="#使用语言" class="headerlink" title="使用语言"></a>使用语言</h2><p><img src="/image/fate/f091be675bf0c77cd5d712c4291e294e.png" alt=""></p>
<p>再从开发的语言上来看看，python 一马当先，一骑绝尘，一夫当关，一统天下</p>
<p>有志于此的小伙伴，还是好好学python 吧。</p>
<p><img src="/image/fate/f4d146cc8d2463ee9fb74e043930dff4.png" alt=""></p>
<p>当然仔细分析一下，像FATE 依赖的分布式机器学习引擎eggroll，也涉及了scala 和java。 还有TTF依赖的TensorFlow，也是C++ 写的，也就是在引擎层面，python 还是未够班啊。</p>
<p><img src="/image/fate/9e13bdd07be8bd7dbe20f3ddb73bc0fc.png" alt=""></p>
<h2 id="公司-amp-组织参与度"><a href="#公司-amp-组织参与度" class="headerlink" title="公司&amp;组织参与度"></a>公司&amp;组织参与度</h2><p>从公司参与度上 ，微众银行、IBM、GOOGLE 位列三甲。</p>
<p><img src="/image/fate/a2c71f164052ea76a79d0bbf81758d0e.png" alt=""></p>
<p>当然，FATE也并非微众一家之功，从现有的信息来看，VMare也在FATE的实现中，出了不少力 。</p>
<p>GOOGLE作为科技巨头，上榜也一点也不意外。</p>
<p>到是看到了蓝色巨人IBM，蓝色巨人的底蕴还是深厚的<br>[图片上传失败…(image-b0cc3b-1607480144280)]</p>
<p>不禁想起了我的又一个朋友，身为前IBM员工，一直感叹：</p>
<blockquote>
<p>老东家在云计算、大数据方向，IBM都是起了个大早，赶了个晚集 ！</p>
</blockquote>
<p>如果联邦学习会成为下个趋势的话，期望看到蓝色巨人有所作为吧。</p>
<h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>以上，就是基于一个不靠谱的爬虫获取的不靠谱的分析 。</p>
<p>如果非要有什么结论，或者在选型上有所建议的话，个人的建议是 双持FATE和TTF。</p>
<p><img src="/image/fate/3b952bd5eef7f17ff3db63d10e738e71.png" alt=""></p>
<p>TTF背靠大树，以及有着TensorFlow 这么丰富的宝库，必定后劲绵长。而FATE，尤其完善的配套（可以去看下FATE下的repo），活跃的社区，中文的支持。学习起来，必然事半功倍。</p>
<p>当然，除了以上两个，本着open source 的精神，还是要博采众长。尤其是一些走在前沿，带着极强research性质的框架，可能功能不是那么完善和强大，但是很多思路确可以借鉴。作为一个技术人员，切不可固步自封，而应该更加的开放，不是么？</p>
<p><img src="/image/fate/d0c304da9af60fb9424373c4fa125f77.png" alt=""></p>
<h1 id="假装必须有的FAQ"><a href="#假装必须有的FAQ" class="headerlink" title="假装必须有的FAQ"></a>假装必须有的FAQ</h1><h2 id="Q1：-为什么开头是ERROR-？"><a href="#Q1：-为什么开头是ERROR-？" class="headerlink" title="Q1： 为什么开头是ERROR ？"></a>Q1： 为什么开头是ERROR ？</h2><p>A: WARNNING你会看？</p>
<h2 id="Q2-为什么是山东"><a href="#Q2-为什么是山东" class="headerlink" title="Q2: 为什么是山东?"></a>Q2: 为什么是山东?</h2><p><img src="/image/fate/444088ba625e3d41b24131aa5ca3ff51.png" alt=""></p>
<p>A:即便诸葛村夫的广告你没看过， 喜马拉雅山以东啊，懂？</p>
<h2 id="Q3-学好了FATE-可以补魔么？"><a href="#Q3-学好了FATE-可以补魔么？" class="headerlink" title="Q3: 学好了FATE 可以补魔么？"></a>Q3: 学好了FATE 可以补魔么？</h2><p>A:不用FATE，给你自己new 一个对象，def 一个补魔的function，配合一个while(ture) 你就可以永动补魔到OOM了</p>
<p><img src="/image/fate/9a5a82a041930e346697ace5e76185c6.png" alt=""></p>
<h2 id="Q4：为什么是潜在巴伐利亚拖拉机主，不是别的？"><a href="#Q4：为什么是潜在巴伐利亚拖拉机主，不是别的？" class="headerlink" title="Q4：为什么是潜在巴伐利亚拖拉机主，不是别的？"></a>Q4：为什么是潜在巴伐利亚拖拉机主，不是别的？</h2><p>A: 人呢，要知足，我觉得有拖拉机开就不错了，不奢求开破二手车 更不奢望压金砖 况且，拖拉机和我的气质更配</p>
<h2 id="Q5-你说的那么多朋友，真的是朋友？"><a href="#Q5-你说的那么多朋友，真的是朋友？" class="headerlink" title="Q5: 你说的那么多朋友，真的是朋友？"></a>Q5: 你说的那么多朋友，真的是朋友？</h2><p>A:</p>
<p><img src="/image/fate/a2aa239b5fb920c7a384dcaf349dcc8a.jpg" alt=""></p>
<h1 id="REF"><a href="#REF" class="headerlink" title="REF"></a>REF</h1><ul>
<li><a href="http://news.yesky.com/hotnews/363/714746363.shtml">9nfl</a></li>
<li><a href="https://mp.weixin.qq.com/s/MHUpJT1jr71Rt93BhPCvvg">Fedlearner</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/112983993">AngelFL</a> </li>
<li><a href="https://github.com/PaddlePaddle/PaddleFL">PaddleFL</a></li>
<li><a href="https://tensorflow.google.cn/">TensorFlow</a></li>
<li><a href="https://tensorflow.google.cn/federated">TFF</a></li>
<li><a href="https://fedml.ai/">FedML</a></li>
<li><a href="https://github.com/xaynetwork">xaynet</a></li>
<li><a href="https://gdpr-info.eu/">GDPR</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/101644082">GOOGLE 联邦学习漫画</a></li>
<li><a href="https://github.com/WeBankFinTech/eggroll">eggroll</a></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/06/postpmml4LR/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="喵十八">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="喵十八の小窝">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/08/06/postpmml4LR/" class="post-title-link" itemprop="url">机器学习不只是调包--通过PMML解析线性回归和逻辑回归</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2018-08-06 21:45:18 / Modified: 21:47:03" itemprop="dateCreated datePublished" datetime="2018-08-06T21:45:18+08:00">2018-08-06</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/pmml/" itemprop="url" rel="index"><span itemprop="name">pmml</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h1><ul>
<li>线性回归介绍</li>
<li>线性回归pmml 介绍</li>
<li>线性回归模型结构</li>
<li>如何手动写一个java类表述线性回归</li>
</ul>
<p>很多时候，我们在看机器学习的算法的时候，看到的都是一些列的公式推导。那么这些公式推导出来的结果是什么？最后又是如何组织的？作为一个码农，更关心的是，这些公式最后又是如何变为代码的？<br>本系列将借助PMML这一工具，可以用来解析模型的结构，了解各种模型中都有那些元素？这些元素又是通过何种组合方式，计算公式得到最后的结果的？<br>本文针对偏工程人员，不涉及具体的模型优化求解问题，我们关注的是模型实质的结构，以及根据这些信息，如何实现跨平台的使用模型。至于模型的求解过程，前人已经总结完备，不过多赘述，在文中会给出地址，如有兴趣，可以自行查阅推导。</p>
<h1 id="先从简单的线性回归开始"><a href="#先从简单的线性回归开始" class="headerlink" title="先从简单的线性回归开始"></a>先从简单的线性回归开始</h1><h2 id="什么是线性回归"><a href="#什么是线性回归" class="headerlink" title="什么是线性回归"></a>什么是线性回归</h2><p>定义：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在统计学中，线性回归(Linear Regression)是利用称为线性回归方程的最小平方函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。这种函数是一个或多个称为回归系数的模型参数的线性组合。只有一个自变量的情况称为简单回归,大于一个自变量情况的叫做多元回归。</span><br></pre></td></tr></table></figure><br>线性回归是数据挖掘中的基础算法之一，从某种意义上来说，在学习函数的时候已经开始接触线性回归了，只不过那时候并没有涉及到误差项。线性回归的思想其实就是解一组方程，得到回归函数，不过在出现误差项之后，方程的解法就存在了改变，一般使用最小二乘法进行计算。</p>
<h3 id="解决问题流程"><a href="#解决问题流程" class="headerlink" title="解决问题流程"></a>解决问题流程</h3><p>Step1. 选择一个模型函数h<br>Step2. 为h找到适应数据的最优解，即找出最优解下的h的参数。</p>
<h3 id="函数模型"><a href="#函数模型" class="headerlink" title="函数模型"></a>函数模型</h3><p><img src="https://images0.cnblogs.com/blog2015/633472/201503/262037556613399.jpg" alt="线性回归函数模型"><br>其中h()是一个线性函数，所有的自变量构成一个一维向量X，所有参数构成一个一维向量W，就可以将第一行的公式改写为第二行的形式。</p>
<p>假设存在训练数据集<br><img src="https://images0.cnblogs.com/blog2015/633472/201503/262041198028564.jpg" alt=""><br>为了方便，可以改写为矩阵的形式<br><img src="https://images0.cnblogs.com/blog2015/633472/201503/262042295678545.jpg" alt=""></p>
<p>其中x可以看成特征，theater看成是权重。我们的目标就是找出所有的权重，进而出现新的x值时，可以对函数的输出进行估计。那我们如何求得使函数输出最接近样本的值呢？函数输出最接近样本值就意味着二者之差尽可能的小。我们假设输入的特征为，对应的样本值为，我们用模型估计出的值为，估计值与真实值之间的误差表示为<br>成为损失函数，损失函数的自变量为，所以我们需要找到最小时的取值。</p>
<p>在机器学习中我们采用梯度下降算法求解该方程，</p>
<p>一般的求解方法有梯度下降法，牛顿法，共轭梯度法，启发式优化方法等，在这篇<a href="http://www.cnblogs.com/maybe2030/p/4751804.html">博文</a>中介绍的比较详细。再次不多赘述。</p>
<p>OK,通过求解损失函数最小化，我们会得到一组参数，这就是线性回归的解，根据这个解，就有了我们的模型。</p>
<h2 id="操作实例"><a href="#操作实例" class="headerlink" title="操作实例"></a>操作实例</h2><h3 id="sklearn-实现线性回归"><a href="#sklearn-实现线性回归" class="headerlink" title="sklearn 实现线性回归"></a>sklearn 实现线性回归</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from sklearn2pmml import PMMLPipeline</span><br><span class="line">from sklearn2pmml import sklearn2pmml</span><br><span class="line">from sklearn import linear_model</span><br><span class="line">clf = linear_model.LinearRegression()</span><br><span class="line">X = [[0,0],[1,1],[2,2]]</span><br><span class="line">y = [0,1,2]</span><br><span class="line">pipeline = PMMLPipeline([(&quot;classifier&quot;, clf)])</span><br><span class="line">pipeline.fit(X,y)</span><br></pre></td></tr></table></figure>
<p>以上是希望拟合y=0.5 <em> x1 + 0.5 </em> x2。<br>测试下<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pipeline.predict([[1.0,2.0]])</span><br></pre></td></tr></table></figure><br>结果为：1.5 符合预期</p>
<p>将其导出为pmml<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn2pmml(pipeline,&quot;linearregression.pmml&quot;,with_repr = True)  </span><br></pre></td></tr></table></figure></p>
<h3 id="线性回归的PMML及结构"><a href="#线性回归的PMML及结构" class="headerlink" title="线性回归的PMML及结构"></a>线性回归的PMML及结构</h3><p>上一节中的模型，导出PMML文件的主要部分如下：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&lt;DataDictionary&gt;</span><br><span class="line">	&lt;DataField name=&quot;y&quot; optype=&quot;continuous&quot; dataType=&quot;double&quot;/&gt;</span><br><span class="line">	&lt;DataField name=&quot;x1&quot; optype=&quot;continuous&quot; dataType=&quot;double&quot;/&gt;</span><br><span class="line">	&lt;DataField name=&quot;x2&quot; optype=&quot;continuous&quot; dataType=&quot;double&quot;/&gt;</span><br><span class="line">&lt;/DataDictionary&gt;</span><br><span class="line">&lt;RegressionModel functionName=&quot;regression&quot;&gt;</span><br><span class="line">	&lt;MiningSchema&gt;</span><br><span class="line">		&lt;MiningField name=&quot;y&quot; usageType=&quot;target&quot;/&gt;</span><br><span class="line">		&lt;MiningField name=&quot;x1&quot;/&gt;</span><br><span class="line">		&lt;MiningField name=&quot;x2&quot;/&gt;</span><br><span class="line">	&lt;/MiningSchema&gt;</span><br><span class="line">	&lt;RegressionTable intercept=&quot;2.220446049250313E-16&quot;&gt;</span><br><span class="line">		&lt;NumericPredictor name=&quot;x1&quot; coefficient=&quot;0.4999999999999999&quot;/&gt;</span><br><span class="line">		&lt;NumericPredictor name=&quot;x2&quot; coefficient=&quot;0.49999999999999983&quot;/&gt;</span><br><span class="line">	&lt;/RegressionTable&gt;</span><br><span class="line">&lt;/RegressionModel&gt;</span><br></pre></td></tr></table></figure><br>模型一共两个输入 x1 x2,一个目标输出y<br>x1，x2 对应的参数分别为 0.4999999999999999 和 0.49999999999999983 （注意，这里不是0.5，是因为拟合的误差原因）整个的截距是2.220446049250313E-16</p>
<p>只要有了这几个参数，你就有了训练好的模型。针对任意的输入的x1，x2，你都能直接输入一个拟合好的y，掉包 不存在的。</p>
<h3 id="用Scala实现线性回归的预测"><a href="#用Scala实现线性回归的预测" class="headerlink" title="用Scala实现线性回归的预测"></a>用Scala实现线性回归的预测</h3><p>这里，参考了Spark MLlib中的源码，简单写了几个类，主要用于说明线性回归的结构，以及预测的逻辑。没有涉及到模型的训练。<br>从上一节中，可以看出线性回归模型中，其实就两个参数，1个权重向量，1个截距。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">class LinerRegressionModel(</span><br><span class="line">   val weights: Array[Double],</span><br><span class="line">   val intercept: Double</span><br><span class="line">   ) &#123;</span><br><span class="line"></span><br><span class="line">  def predict(testData:Array[Double]): Double =&#123;</span><br><span class="line">    predictPoint(testData,weights,intercept)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def predictPoint(</span><br><span class="line">    dataMatrix: Array[Double],</span><br><span class="line">    weightMatrix: Array[Double],</span><br><span class="line">    intercept: Double): Double = &#123;</span><br><span class="line">    DenseVector(dataMatrix).dot(DenseVector(weightMatrix)) + intercept</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>定义线性回归模型，预测结果是输入向量和权重向量点积加上截距。<br>权重向量就是上边每个入参对应的参数。<br>测试类如下：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def testLinerRegression(): Unit =&#123;</span><br><span class="line">  val weight = Array(0.4999999999999999, 0.49999999999999983)</span><br><span class="line">  val intercept = 2.220446049250313E-16</span><br><span class="line">  val model = new LinerRegressionModel(weight,intercept)</span><br><span class="line">  val x = Array(1.0, 2.0)</span><br><span class="line">  val y = model.predict(x)</span><br><span class="line">  println(f&quot;The Result Of Model is $&#123;y&#125;&quot;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>预测(1.0, 2.0)结果为：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">The Result Of Model is 1.4999999999999998</span><br></pre></td></tr></table></figure><br>四舍五入和python结果一致。</p>
<h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><p>根据上文的叙述，线性回归的模型是求出输出特征向量Y和输入样本矩阵X之间的线性关系系数theater,使其满足Y=theater X，如果的Y是连续的，所以是回归模型。如果我们想要Y是离散的话，怎么办呢？一个可以想到的办法是，我们对于这个Y再做一次函数转换，变为g(Y)。如果我们令g(Y)的值在某个实数区间的时候是类别A，在另一个实数区间的时候是类别B，就可以得到一个分类模型。<br>在逻辑回归中，这个函数就是sigmoid函数<br><img src="https://gss3.bdstatic.com/-Po3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D99/sign=a46bd6f1dd33c895a27e9472d01340df/0df3d7ca7bcb0a4659502a5f6f63f6246b60af62.jpg" alt=""></p>
<p>下图展示了将分布函数变形的过程。<br><img src="https://img-blog.csdn.net/20171005175521991?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY3lteTAwMQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""></p>
<p>具体的求解过程，可以参见这篇文章的叙述，本文不做赘述<a href="http://www.aboutyun.com/thread-10650-1-1.html">http://www.aboutyun.com/thread-10650-1-1.html</a></p>
<h2 id="二元逻辑回归"><a href="#二元逻辑回归" class="headerlink" title="二元逻辑回归"></a>二元逻辑回归</h2><p>如果结果的类别只有两种，那么就是一个二元分类模型了。<br>二元逻辑回归的预测值由下式求得<br><img src="https://img-blog.csdn.net/20141209123917993" alt=""></p>
<p>因此，逻辑回归分类器的解就是一组权值向量，和线性回归是一致的。</p>
<h3 id="二元逻辑回归python实现"><a href="#二元逻辑回归python实现" class="headerlink" title="二元逻辑回归python实现"></a>二元逻辑回归python实现</h3><p>使用自带的iris数据集进行操作，iris中包含3个分类，为了体现二分类的特性，删除了一个分类。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn2pmml <span class="keyword">import</span> PMMLPipeline</span><br><span class="line"><span class="keyword">from</span> sklearn2pmml <span class="keyword">import</span> sklearn2pmml</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">lr = LogisticRegression()</span><br><span class="line"><span class="comment"># 导入数据，为了后续方便，将三类中的一类去除，使之变为二分类问题。</span></span><br><span class="line">iris = load_iris()</span><br><span class="line">df = pd.DataFrame(iris.data)</span><br><span class="line">df[<span class="string">&quot;class&quot;</span>] = iris.target</span><br><span class="line">df.columns=[<span class="string">&#x27;V0&#x27;</span>,<span class="string">&#x27;V1&#x27;</span>,<span class="string">&#x27;V2&#x27;</span>,<span class="string">&#x27;V3&#x27;</span>,<span class="string">&#x27;class&#x27;</span>]</span><br><span class="line">df = df[df[<span class="string">&#x27;class&#x27;</span>] &lt; <span class="number">2</span>]</span><br><span class="line">df.describe()</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(df[df.columns.difference([<span class="string">&#x27;class&#x27;</span>])], df[<span class="string">&#x27;class&#x27;</span>], test_size=<span class="number">0.5</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">pipeline = PMMLPipeline([(<span class="string">&quot;classifier&quot;</span>, lr)])</span><br><span class="line">pipeline.fit(X_train,y_train)</span><br></pre></td></tr></table></figure></p>
<p>然后将训练好的模型导出为PMML<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from sklearn2pmml import sklearn2pmml</span><br><span class="line">sklearn2pmml(pipeline, &quot;LRbin.pmml&quot;, with_repr = True)</span><br></pre></td></tr></table></figure></p>
<h3 id="二元逻辑回归PMML分析"><a href="#二元逻辑回归PMML分析" class="headerlink" title="二元逻辑回归PMML分析"></a>二元逻辑回归PMML分析</h3><p>上一节代码生成的PMML中的主要部分如下：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">&lt;DataDictionary&gt;</span><br><span class="line">		&lt;DataField name=&quot;class&quot; optype=&quot;categorical&quot; dataType=&quot;integer&quot;&gt;</span><br><span class="line">			&lt;Value value=&quot;0&quot;/&gt;</span><br><span class="line">			&lt;Value value=&quot;1&quot;/&gt;</span><br><span class="line">		&lt;/DataField&gt;</span><br><span class="line">		&lt;DataField name=&quot;V0&quot; optype=&quot;continuous&quot; dataType=&quot;double&quot;/&gt;</span><br><span class="line">		&lt;DataField name=&quot;V1&quot; optype=&quot;continuous&quot; dataType=&quot;double&quot;/&gt;</span><br><span class="line">		&lt;DataField name=&quot;V2&quot; optype=&quot;continuous&quot; dataType=&quot;double&quot;/&gt;</span><br><span class="line">		&lt;DataField name=&quot;V3&quot; optype=&quot;continuous&quot; dataType=&quot;double&quot;/&gt;</span><br><span class="line">	&lt;/DataDictionary&gt;</span><br><span class="line">	&lt;RegressionModel functionName=&quot;classification&quot; normalizationMethod=&quot;logit&quot;&gt;</span><br><span class="line">		&lt;MiningSchema&gt;</span><br><span class="line">			&lt;MiningField name=&quot;class&quot; usageType=&quot;target&quot;/&gt;</span><br><span class="line">			&lt;MiningField name=&quot;V0&quot;/&gt;</span><br><span class="line">			&lt;MiningField name=&quot;V1&quot;/&gt;</span><br><span class="line">			&lt;MiningField name=&quot;V2&quot;/&gt;</span><br><span class="line">			&lt;MiningField name=&quot;V3&quot;/&gt;</span><br><span class="line">		&lt;/MiningSchema&gt;</span><br><span class="line">		&lt;Output&gt;</span><br><span class="line">			&lt;OutputField name=&quot;probability(0)&quot; optype=&quot;continuous&quot; dataType=&quot;double&quot; feature=&quot;probability&quot; value=&quot;0&quot;/&gt;</span><br><span class="line">			&lt;OutputField name=&quot;probability(1)&quot; optype=&quot;continuous&quot; dataType=&quot;double&quot; feature=&quot;probability&quot; value=&quot;1&quot;/&gt;</span><br><span class="line">		&lt;/Output&gt;</span><br><span class="line">		&lt;RegressionTable intercept=&quot;-0.24391923532173168&quot; targetCategory=&quot;1&quot;&gt;</span><br><span class="line">			&lt;NumericPredictor name=&quot;V0&quot; coefficient=&quot;-0.31738779611631857&quot;/&gt;</span><br><span class="line">			&lt;NumericPredictor name=&quot;V1&quot; coefficient=&quot;-1.2346299390640323&quot;/&gt;</span><br><span class="line">			&lt;NumericPredictor name=&quot;V2&quot; coefficient=&quot;1.920449906205768&quot;/&gt;</span><br><span class="line">			&lt;NumericPredictor name=&quot;V3&quot; coefficient=&quot;0.8753093733469249&quot;/&gt;</span><br><span class="line">		&lt;/RegressionTable&gt;</span><br><span class="line">		&lt;RegressionTable intercept=&quot;0.0&quot; targetCategory=&quot;0&quot;/&gt;</span><br><span class="line">	&lt;/RegressionModel&gt;</span><br></pre></td></tr></table></figure><br>一共四个入参，V0，V1，V2，V4 都为double类型。目标输出为0，1分类。<br>模型是一个<code>classification</code>，标准化使用的是<code>logit</code>函数，也就是sigmod函数。<br>权重矩阵为(-0.31738779611631857,-1.2346299390640323,1.920449906205768,0.8753093733469249),截距为-0.24391923532173168。<br>这里需要注意，二分类只需要一个RegressionTable就能满足分类的需求，后续的多分类会涉及到多个RegressionTable。<br>这里就能发现，二分类的LR，其实就是对输入求了一次线性回归的值，然后对这个值再求其sigmod解。因而，有了权重矩阵和截距，我们就能求出逻辑回归的预测值。</p>
<h3 id="scala简单实现"><a href="#scala简单实现" class="headerlink" title="scala简单实现"></a>scala简单实现</h3><p>定义一个用于二元逻辑回归计算预测概率值的类。<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">class LRBinModel(</span><br><span class="line">    val weights: Array[Double],</span><br><span class="line">    val intercept: Double) &#123;</span><br><span class="line"></span><br><span class="line">  def predict(testData:Array[Double]): Double =&#123;</span><br><span class="line">    predictPoint(testData,weights,intercept)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def predictPoint(</span><br><span class="line">                    dataMatrix: Array[Double],</span><br><span class="line">                    weightMatrix: Array[Double],</span><br><span class="line">                    intercept: Double): Double = &#123;</span><br><span class="line">    val margin = DenseVector(dataMatrix).dot(DenseVector(weightMatrix)) + intercept</span><br><span class="line">    val score = 1.0 / (1.0 + math.exp(-margin))</span><br><span class="line">    score</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>取测试集的第一条记录，进行测试<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X_t=X_test.head(1)</span><br><span class="line">print(X_t)</span><br><span class="line">y_t=y_test.head(1)</span><br><span class="line">print(y_t)</span><br><span class="line">pipeline.predict_proba(X_t)</span><br></pre></td></tr></table></figure><br>可以看到，第一条记录的数据为<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">6.0  2.7  5.1  1.6</span><br></pre></td></tr></table></figure><br>实际类别为1<br>预测结果为<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([[ 0.00329174,  0.99670826]])</span><br></pre></td></tr></table></figure><br>意为属于类别0的概率为0.0032917，属于类别1的概率为0.99670826，两者的和正好为1。故而，求出为类别1的概率之后，用1减去该值就为类别0的概率。</p>
<p>编写一个简单的测试方法，测试改组数据<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def testLRbin(): Unit =&#123;</span><br><span class="line">  val weight = Array(-0.31738779611631857,-1.2346299390640323,1.920449906205768,0.8753093733469249)</span><br><span class="line">  val intercept = -0.24391923532173168</span><br><span class="line">  val model = new LRBinModel(weight, intercept)</span><br><span class="line">  val x = Array(6.0, 2.7, 5.1, 1.6)</span><br><span class="line">  val y = model.predict(x)</span><br><span class="line">  println(f&quot;The Probability Of Class 1 is $&#123;y&#125;&quot;)</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><br>结果为：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">The Probability Of Class 1 is 0.9967082628300301</span><br></pre></td></tr></table></figure><br>属于类别1的概率为0.9967082628300301 和python 的预测结果基本一致。<br>然后就可以根据设定的阈值，判别属于哪一类了。</p>
<h3 id="模型中的调优参数"><a href="#模型中的调优参数" class="headerlink" title="模型中的调优参数"></a>模型中的调优参数</h3><p>对于逻辑回归模型，会有一些参数需要调节，比如<code>C</code>，<code>max_iter</code>，<code>penalty</code>等，这几个值在模型求解的过程中生效，在已经求解的模型中，并无体现。</p>
<h2 id="多元逻辑回归分析"><a href="#多元逻辑回归分析" class="headerlink" title="多元逻辑回归分析"></a>多元逻辑回归分析</h2><p>多元逻辑回归，和二元类似，分别计算属于每个类别的概率，选取其中的最大值作为预测值。具体叙述参见<a href="https://blog.csdn.net/quiet_girl/article/details/70216899"></a></p>
<h3 id="多元逻辑回归python实现"><a href="#多元逻辑回归python实现" class="headerlink" title="多元逻辑回归python实现"></a>多元逻辑回归python实现</h3><p>我们还是使用iris数据集进行示例，这次不用删除类别了。<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from sklearn2pmml import PMMLPipeline</span><br><span class="line">from sklearn.datasets import load_iris</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line">iris = load_iris()</span><br><span class="line">clf = LogisticRegression()</span><br><span class="line">pipeline = PMMLPipeline([(&quot;classifier&quot;, clf)])</span><br><span class="line">pipeline.fit(iris.data, iris.target)</span><br><span class="line"></span><br><span class="line"># 导出为PMML</span><br><span class="line">from sklearn2pmml import sklearn2pmml</span><br><span class="line">sklearn2pmml(pipeline, &quot;LR.pmml&quot;, with_repr = True)</span><br></pre></td></tr></table></figure></p>
<h3 id="多元逻辑回归PMML分析"><a href="#多元逻辑回归PMML分析" class="headerlink" title="多元逻辑回归PMML分析"></a>多元逻辑回归PMML分析</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">&lt;DataDictionary&gt;</span><br><span class="line">	&lt;DataField name=&quot;y&quot; optype=&quot;categorical&quot; dataType=&quot;integer&quot;&gt;</span><br><span class="line">		&lt;Value value=&quot;0&quot;/&gt;</span><br><span class="line">		&lt;Value value=&quot;1&quot;/&gt;</span><br><span class="line">		&lt;Value value=&quot;2&quot;/&gt;</span><br><span class="line">	&lt;/DataField&gt;</span><br><span class="line">	&lt;DataField name=&quot;x1&quot; optype=&quot;continuous&quot; dataType=&quot;double&quot;/&gt;</span><br><span class="line">	&lt;DataField name=&quot;x2&quot; optype=&quot;continuous&quot; dataType=&quot;double&quot;/&gt;</span><br><span class="line">	&lt;DataField name=&quot;x3&quot; optype=&quot;continuous&quot; dataType=&quot;double&quot;/&gt;</span><br><span class="line">	&lt;DataField name=&quot;x4&quot; optype=&quot;continuous&quot; dataType=&quot;double&quot;/&gt;</span><br><span class="line">&lt;/DataDictionary&gt;</span><br><span class="line">&lt;RegressionModel functionName=&quot;classification&quot; normalizationMethod=&quot;logit&quot;&gt;</span><br><span class="line">	&lt;MiningSchema&gt;</span><br><span class="line">		&lt;MiningField name=&quot;y&quot; usageType=&quot;target&quot;/&gt;</span><br><span class="line">		&lt;MiningField name=&quot;x1&quot;/&gt;</span><br><span class="line">		&lt;MiningField name=&quot;x2&quot;/&gt;</span><br><span class="line">		&lt;MiningField name=&quot;x3&quot;/&gt;</span><br><span class="line">		&lt;MiningField name=&quot;x4&quot;/&gt;</span><br><span class="line">	&lt;/MiningSchema&gt;</span><br><span class="line">	&lt;Output&gt;</span><br><span class="line">		&lt;OutputField name=&quot;probability(0)&quot; optype=&quot;continuous&quot; dataType=&quot;double&quot; feature=&quot;probability&quot; value=&quot;0&quot;/&gt;</span><br><span class="line">		&lt;OutputField name=&quot;probability(1)&quot; optype=&quot;continuous&quot; dataType=&quot;double&quot; feature=&quot;probability&quot; value=&quot;1&quot;/&gt;</span><br><span class="line">		&lt;OutputField name=&quot;probability(2)&quot; optype=&quot;continuous&quot; dataType=&quot;double&quot; feature=&quot;probability&quot; value=&quot;2&quot;/&gt;</span><br><span class="line">	&lt;/Output&gt;</span><br><span class="line">	&lt;RegressionTable intercept=&quot;0.26560616797551695&quot; targetCategory=&quot;0&quot;&gt;</span><br><span class="line">		&lt;NumericPredictor name=&quot;x1&quot; coefficient=&quot;0.4149883282957013&quot;/&gt;</span><br><span class="line">		&lt;NumericPredictor name=&quot;x2&quot; coefficient=&quot;1.4612973885622267&quot;/&gt;</span><br><span class="line">		&lt;NumericPredictor name=&quot;x3&quot; coefficient=&quot;-2.2621411772020728&quot;/&gt;</span><br><span class="line">		&lt;NumericPredictor name=&quot;x4&quot; coefficient=&quot;-1.02909509924489&quot;/&gt;</span><br><span class="line">	&lt;/RegressionTable&gt;</span><br><span class="line">	&lt;RegressionTable intercept=&quot;1.0854237423889572&quot; targetCategory=&quot;1&quot;&gt;</span><br><span class="line">		&lt;NumericPredictor name=&quot;x1&quot; coefficient=&quot;0.41663968559520786&quot;/&gt;</span><br><span class="line">		&lt;NumericPredictor name=&quot;x2&quot; coefficient=&quot;-1.6008331852575897&quot;/&gt;</span><br><span class="line">		&lt;NumericPredictor name=&quot;x3&quot; coefficient=&quot;0.5776576286775582&quot;/&gt;</span><br><span class="line">		&lt;NumericPredictor name=&quot;x4&quot; coefficient=&quot;-1.3855384286634223&quot;/&gt;</span><br><span class="line">	&lt;/RegressionTable&gt;</span><br><span class="line">	&lt;RegressionTable intercept=&quot;-1.2147145780786366&quot; targetCategory=&quot;2&quot;&gt;</span><br><span class="line">		&lt;NumericPredictor name=&quot;x1&quot; coefficient=&quot;-1.7075251538239047&quot;/&gt;</span><br><span class="line">		&lt;NumericPredictor name=&quot;x2&quot; coefficient=&quot;-1.5342683399889876&quot;/&gt;</span><br><span class="line">		&lt;NumericPredictor name=&quot;x3&quot; coefficient=&quot;2.4709716807720206&quot;/&gt;</span><br><span class="line">		&lt;NumericPredictor name=&quot;x4&quot; coefficient=&quot;2.5553821129820884&quot;/&gt;</span><br><span class="line">	&lt;/RegressionTable&gt;</span><br><span class="line">&lt;/RegressionModel&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>基本上和二元逻辑回归类似，只是RegressionTable 有三个，这是因为一共有三个类别，需要三组权重矩阵和截距，分别计算属于当前类别的概率值，因而对于多元逻辑回归而言，有多少元，输出的概率值就有多少个。<br>然后再针对所有输出的概率，求和，然后计算每个概率和求和概率的比值。这是为了保证形式上的统一。</p>
<h3 id="Scala实现"><a href="#Scala实现" class="headerlink" title="Scala实现"></a>Scala实现</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">class LRMultiModel(</span><br><span class="line">   val weights: Array[Array[Double]],</span><br><span class="line">   val intercept: Array[Double]</span><br><span class="line">                          ) &#123;</span><br><span class="line"></span><br><span class="line">  def predict(testData:Array[Double]): Array[Double] =&#123;</span><br><span class="line">    val classNum = weights.size</span><br><span class="line">    val pro = new ArrayBuffer[Double]</span><br><span class="line">    for (i &lt;- 0 until classNum)&#123;</span><br><span class="line">      pro.append(predictPoint(testData,weights(i),intercept(i)))</span><br><span class="line">    &#125;</span><br><span class="line">    val sum = pro.sum</span><br><span class="line">    val result = pro.toArray.map(x=&gt;(x/sum))</span><br><span class="line">    result</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def predictPoint(</span><br><span class="line">                    dataMatrix: Array[Double],</span><br><span class="line">                    weightMatrix: Array[Double],</span><br><span class="line">                    intercept: Double): Double = &#123;</span><br><span class="line">    val margin = DenseVector(dataMatrix).dot(DenseVector(weightMatrix)) + intercept</span><br><span class="line">    val score = 1.0 / (1.0 + math.exp(-margin))</span><br><span class="line">    score</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对于python训练的模型，选取head(1)进行测试<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X_t=X_test.head(1)</span><br><span class="line">print(X_t)</span><br><span class="line">y_t=y_test.head(1)</span><br><span class="line">print(y_t)</span><br><span class="line">pipeline.predict_proba(X_t)</span><br></pre></td></tr></table></figure><br>结果如下：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[ 0.00122453  0.39920192  0.59957355]]</span><br></pre></td></tr></table></figure></p>
<p>编写测试方法测试<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">def testLRMul(): Unit =&#123;</span><br><span class="line">   val weight = Array(</span><br><span class="line">     Array(0.4149883282957013,1.4612973885622267,-2.2621411772020728,-1.02909509924489),</span><br><span class="line">     Array(0.41663968559520786,-1.6008331852575897,0.5776576286775582,-1.3855384286634223),</span><br><span class="line">     Array(-1.7075251538239047,-1.5342683399889876,2.4709716807720206,2.5553821129820884)</span><br><span class="line">   )</span><br><span class="line"></span><br><span class="line">   val intercept = Array(0.26560616797551695, 1.0854237423889572, -1.2147145780786366)</span><br><span class="line">   val model = new LRMultiModel(weight, intercept)</span><br><span class="line">   val x = Array(6.0, 2.7, 5.1, 1.6)</span><br><span class="line">   val y = model.predict(x)</span><br><span class="line">   var mclass = 0</span><br><span class="line">   var max = 0.0</span><br><span class="line">   for (i &lt;- 0 until y.size)&#123;</span><br><span class="line">     println(f&quot;The Probability Of Class $&#123;i&#125; is $&#123;y(i)&#125;&quot;)</span><br><span class="line">     if (y(i) &gt; max)&#123;</span><br><span class="line">       max = y(i)</span><br><span class="line">       mclass = i</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   println(s&quot;The Class May Be $&#123;mclass&#125;&quot;)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><br>结果如下：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">The Probability Of Class 0 is 0.0012245308619260621</span><br><span class="line">The Probability Of Class 1 is 0.39920191920408243</span><br><span class="line">The Probability Of Class 2 is 0.5995735499339914</span><br><span class="line">The Class May Be 2</span><br></pre></td></tr></table></figure></p>
<p>两者完全一致。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本文借助了PMML，解析了简单线性回归和逻辑回归的结构。介绍了这两种模型是如何实现预测的。其实所有看起来，或者听起来“高大上”的模型，在码农的眼里，最终的呈现都是一系列的“参数”而已。<br>通过不同的方式将这些参数组合起来，便可实现一些神奇的功能。</p>
<h1 id="REF"><a href="#REF" class="headerlink" title="REF"></a>REF</h1><p><a href="https://blog.csdn.net/fleurdalis/article/details/54931721">https://blog.csdn.net/fleurdalis/article/details/54931721</a><br>李航 统计学习方法</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/05/postCLT/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="喵十八">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="喵十八の小窝">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/08/05/postCLT/" class="post-title-link" itemprop="url">【翻译活动】面向数据科学的概率论-14.中心极限定律</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2018-08-05 19:29:19 / Modified: 19:33:20" itemprop="dateCreated datePublished" datetime="2018-08-05T19:29:19+08:00">2018-08-05</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%BF%BB%E8%AF%91/" itemprop="url" rel="index"><span itemprop="name">翻译</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="十四、-中心极限定律"><a href="#十四、-中心极限定律" class="headerlink" title="十四、 中心极限定律"></a>十四、 中心极限定律</h1><blockquote>
<p>原文：<a href="https://nbviewer.jupyter.org/github/prob140/textbook/blob/gh-pages/notebooks/Chapter_14/">prob140/textbook/notebooks/ch_14</a></p>
<p>译者：<a href="https://github.com/Yao544303">喵十八</a></p>
<p>协议：<a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></p>
<p>自豪地采用<a href="https://translate.google.cn/">谷歌翻译</a></p>
</blockquote>
<h1 id="本章依赖的python"><a href="#本章依赖的python" class="headerlink" title="本章依赖的python"></a>本章依赖的python</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># HIDDEN</span></span><br><span class="line"><span class="keyword">from</span> datascience <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> prob140 <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.style.use(<span class="string">&#x27;fivethirtyeight&#x27;</span>)</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> stats</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># HIDDEN</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dist_sum</span>(<span class="params">n, probs_0_through_N</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Return the distribution of S_n,</span></span><br><span class="line"><span class="string">    the sum of n i.i.d. copies</span></span><br><span class="line"><span class="string">    of a random variable with distribution probs_0_through_N</span></span><br><span class="line"><span class="string">    on the integers 0, 1, 2, ..., N&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Find the possible values of S_n</span></span><br><span class="line">    N = <span class="built_in">len</span>(probs_0_through_N) - <span class="number">1</span>   </span><br><span class="line">    values_Sn = np.arange(n*N + <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Find the probailities of those values</span></span><br><span class="line">    coeffs_X1 = np.flipud(probs_0_through_N)</span><br><span class="line">    pgf_X1 = np.poly1d(coeffs_X1)</span><br><span class="line">    pgf_Sn = pgf_X1**n</span><br><span class="line">    coeffs_Sn = pgf_Sn.c</span><br><span class="line">    probs_Sn = np.flipud(coeffs_Sn)</span><br><span class="line">    </span><br><span class="line">    t = Table().with_columns(</span><br><span class="line">        <span class="string">&#x27;Value&#x27;</span>, values_Sn,</span><br><span class="line">        <span class="string">&#x27;Probability&#x27;</span>, probs_Sn</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> t</span><br></pre></td></tr></table></figure>
<h1 id="中心极限定理"><a href="#中心极限定理" class="headerlink" title="中心极限定理"></a>中心极限定理</h1><p>标准差是广为流传的衡量标准之一，此外，还有很多其他的衡量标准。为什么使用标准差？主要的原因是标准差和<a href="https://baike.baidu.com/item/%E6%AD%A3%E6%80%81%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83/7246669?fr=aladdin">正态曲线</a>之间的联系。为何正态曲线如此重要？本章将回答该问题。</p>
<p>我们将从分析一个独立同分布的概率和开始。并已知其均值和标准差。在本章中，我们将研究分布的形状：当我们可以计算它的精确形状时，我们计算它，当数据量过大时，我们计算一个其近似值。</p>
<h2 id="精确分布"><a href="#精确分布" class="headerlink" title="精确分布"></a>精确分布</h2><p>我们已经知道如何找到任意两个离散随机变量之和的分布。</p>
<script type="math/tex; mode=display">
P(X+Y = k) = \sum_j P(X=j, Y=k-j)</script><p>如果$X$和$Y$是独立的，这简化为成为<em>离散卷积公式</em>：</p>
<script type="math/tex; mode=display">
P(X+Y = k) = \sum_j P(X=j)P(Y=k-j)</script><p>通过归纳，我们可以将其扩展到任何有限数量的自变量的和。</p>
<p>所以原则上讲，我们知道如何找到$n$个（$n &gt; 1$）独立随机变量的概率分布之和。但是，对于很大的$n$，这种方式很难实现。</p>
<p>在本节中，我们将研究另一种分布求和的方法。正如您将看到的，它更容易实现自动化，但最终也会遇到计算障碍。</p>
<h3 id="概率生成函数"><a href="#概率生成函数" class="headerlink" title="概率生成函数"></a>概率生成函数</h3><p>设$X$是一个随机变量，对于给定的$N$，其可能取值为$0, 1, 2, \ldots, N$。<br>为简洁起见，令$p_k = P(X = k)$，其中$k$的取值范围为0到$N$。</p>
<p>定义$X$的<em>概率生成函数</em>(pgf)为</p>
<script type="math/tex; mode=display">
G_X(s) ~  = ~ \sum_{k=0}^N p_ks^k, ~~~ -\infty < s < \infty</script><p>对于具有无限多个非负整数随机变量的扩展，请参阅本节末尾的技术说明。</p>
<p>上面的定义表明对任何$s$，有</p>
<script type="math/tex; mode=display">
G_X(s) ~ = ~ p_0 + p_1s + p_2s^2 + p_3s^3 + \cdots + p_Ns^N</script><p>你可以看到$G_X$是一个$N$次多项式，并且$s^k$的系数是$p_k = P(X=k)$。</p>
<p>因此，如果给你一个随机变量的pgf，你可以通过简单地列出所有的权重和相应的系数来计算出随机变量的分布。</p>
<p>要了解这如何帮助我们找到总和的分布，请观察每一个$s$，$G_X(s)$的期望为</p>
<script type="math/tex; mode=display">
G_X(s) ~ = ~ \sum_{k=0}^N s^kP(X=k) ~ = ~ E(s^X)</script><p>因此，如果$X$和$Y$是独立的非负整数随机变量，那么对于每个$s$有</p>
<script type="math/tex; mode=display">
G_{X+Y}(s) ~ = ~ E(s^{X+Y}) ~ = ~ E(s^X s^Y) ~ = ~ E(s^X)E(s^Y)
~ = ~ G_X(s)G_Y(s)</script><p>我们已经使用了这样的事实：对于独立的随机变量，其相乘的期望是期望的相乘。</p>
<p>结果表明两个独立随机变量之和的pgf是两个pgf的乘积。这很容易扩展到两个以上的随机变量，并为独立同分布变量之和的pgf产生一个简单的公式。</p>
<h3 id="一个独立同分布样本分布之和的PGF"><a href="#一个独立同分布样本分布之和的PGF" class="headerlink" title="一个独立同分布样本分布之和的PGF"></a>一个独立同分布样本分布之和的PGF</h3><p>设$X_1, X_2, \ldots, X_n$是分布在$0, 1, 2, \ldots, N$上的独立同分布事件。令$S_n = X_1 + X_2 + \cdots + X_n$，那么$S_n$的pgf为：</p>
<script type="math/tex; mode=display">
G_{S_n}(s) ~ = ~ \big{(}G_{X_1}(s)\big{)}^n, ~~~ -\infty < s < \infty</script><p>因为$G<em>{X_1}$是一个$N$次多项式，$G</em>{S_n}$也是一个$nN$次多项式。与任何pgf一样，$s^k$的系数是$k$的概率。也就是说，对于每一个在0到$nN$范围内的$k$有</p>
<script type="math/tex; mode=display">
P(S_n = k) = \text{coefficient of } s^k \text{ in } G_{S_n}(s)</script><p>我们现在有一个查找$S_n$分布的算法。</p>
<ul>
<li>从$X_1$的pgf开始。</li>
<li>增加幂至$n$。也就是$S_n$的pgf。</li>
<li>读取$S_n$的pgf.</li>
</ul>
<p>精彩！我们完成了！除了实际上这样做涉及将多项式升幂。当数很大时，这是一项艰巨的任务。</p>
<p>幸运的是，正如您将在下一节中看到的那样，<code>NumPy</code>使用一组多项式方法来解决问题。</p>
<p><em>技术说明。</em>我们已经为具有有限多个非负整数值的随机变量定义了概率生成函数。该定义可以扩展到具有无限多个非负整数值的随机变量。但在这种情况下，pgf是一个无限系列，我们必须小心收敛。通常，pdf是的值域 $|s| \le 1$，这样它就会收敛。 </p>
<h2 id="NumPy中的PGF"><a href="#NumPy中的PGF" class="headerlink" title="NumPy中的PGF"></a>NumPy中的PGF</h2><p>回忆一下，我们找到$S_n$分布的算法。</p>
<ul>
<li>从$X_1$的pgf开始。</li>
<li>增加幂至$n$。也就是$S_n$的pgf。</li>
<li>读取$S_n$的pgf.</li>
</ul>
<p>在本节中，我们将使用<code>NumPy</code>实践此算法。</p>
<p>假设$X_1$的分布为$p_0 = 0.1$，$p_1 = 0.5$，$p_2 = 0.4$。令数组<code>probs_X1</code>包含了0,1,2的概率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">probs_X1 = make_array(<span class="number">0.1</span>, <span class="number">0.5</span>, <span class="number">0.4</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dist_X1 = Table().values(np.arange(<span class="number">3</span>)).probability(probs_X1)</span><br><span class="line">Plot(dist_X1)</span><br></pre></td></tr></table></figure>
<p><img src="/image/prob140/14_02_output_5_0.png" alt="png"></p>
<p>$X_1$的pgf是：</p>
<script type="math/tex; mode=display">
0.1 + 0.5s + 0.4s^2</script><p><code>NumPy</code> 以标准的数学方式表示这个多项式，以最高次项开始：</p>
<script type="math/tex; mode=display">
0.4s^2 + 0.5s + 0.1</script><p>方法<code>np.flipud</code>将概率数组反转为与该系数的顺序一致。<code>ud</code>代表“up down”。NumPy正在考虑将数组作为一个列。<code>NumPy</code>考虑将该数组转为一列。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">coeffs_X1 = np.flipud(probs_X1)</span><br><span class="line">coeffs_X1</span><br></pre></td></tr></table></figure>
<pre><code>array([ 0.4,  0.5,  0.1])
</code></pre><p>方法<code>np.poly1d</code>以系数数组为参数，构造多项式。方法名中的<code>1d</code>代表”一维”。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pgf_X1 = np.poly1d(coeffs_X1)</span><br><span class="line"><span class="built_in">print</span>(pgf_X1)</span><br></pre></td></tr></table></figure>
<pre><code>     2
0.4 x + 0.5 x + 0.1
</code></pre><p>调用<code>print</code>方法，打印出该多项式。在$s$的位置，用$x$代替表示。请记住，最后一项是$x^0$的系数。</p>
<p>现在假设$S_3$是三个$X_1$副本的和。$S_3$的pgf是$X_1$pgf的三次方，并且可以按照您的希望计算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pgf_S3 = pgf_X1**<span class="number">3</span></span><br><span class="line"><span class="built_in">print</span>(pgf_S3)</span><br></pre></td></tr></table></figure>
<pre><code>       6        5         4         3         2
0.064 x + 0.24 x + 0.348 x + 0.245 x + 0.087 x + 0.015 x + 0.001
</code></pre><p>$S_3$中多项式的幂为从0到6，因为$S_3$是三个幂从0到2的值的副本的和。系数是$S_3$分布的概率。</p>
<p>你可以使用属性<code>c</code>输出其“系数”。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">coeffs_S3 = pgf_S3.c</span><br><span class="line">coeffs_S3</span><br></pre></td></tr></table></figure>
<pre><code>array([ 0.064,  0.24 ,  0.348,  0.245,  0.087,  0.015,  0.001])
</code></pre><p>这些是从6次到0次项的系数。在概率中，更习惯从低到高的顺序来看，所以再使用一次<code>np.flipud</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">probs_S3 = np.flipud(coeffs_S3)</span><br><span class="line">probs_S3</span><br></pre></td></tr></table></figure>
<pre><code>array([ 0.001,  0.015,  0.087,  0.245,  0.348,  0.24 ,  0.064])
</code></pre><p>您现在拥有绘制$S_3$的概率直方图所需的输入了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dist_S3 = Table().values(np.arange(<span class="number">7</span>)).probability(probs_S3)</span><br><span class="line">Plot(dist_S3)</span><br></pre></td></tr></table></figure>
<p><img src="/image/prob140/14_02_output_17_0.png" alt="png"></p>
<h3 id="计算分布-S-n-的函数"><a href="#计算分布-S-n-的函数" class="headerlink" title="计算分布$S_n$的函数"></a>计算分布$S_n$的函数</h3><p>我们将结合上面的步骤来创建一个函数<code>dist_sum</code>，入参为副本个数$n$和$X_1$的分布，返回值为$n$个$X_1$的副本的和的分布。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">dist_sum</span>(<span class="params">n, probs_0_through_N</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Return the distribution of S_n,</span></span><br><span class="line"><span class="string">    the sum of n i.i.d. copies</span></span><br><span class="line"><span class="string">    of a random variable with distribution probs_0_through_N</span></span><br><span class="line"><span class="string">    on the integers 0, 1, 2, ..., N&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Find the possible values of S_n</span></span><br><span class="line">    N = <span class="built_in">len</span>(probs_0_through_N) - <span class="number">1</span>   </span><br><span class="line">    values_Sn = np.arange(n*N + <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Find the probailities of those values</span></span><br><span class="line">    coeffs_X1 = np.flipud(probs_0_through_N)</span><br><span class="line">    pgf_X1 = np.poly1d(coeffs_X1)</span><br><span class="line">    pgf_Sn = pgf_X1**n</span><br><span class="line">    coeffs_Sn = pgf_Sn.c</span><br><span class="line">    probs_Sn = np.flipud(coeffs_Sn)</span><br><span class="line">    </span><br><span class="line">    t = Table().with_columns(</span><br><span class="line">        <span class="string">&#x27;Value&#x27;</span>, values_Sn,</span><br><span class="line">        <span class="string">&#x27;Probability&#x27;</span>, probs_Sn</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> t</span><br></pre></td></tr></table></figure>
<h3 id="n-次掷骰子游戏的和"><a href="#n-次掷骰子游戏的和" class="headerlink" title="$n$次掷骰子游戏的和"></a>$n$次掷骰子游戏的和</h3><p>在第3章中，我们通过列出所有的$6^5$种可能情况，并计算他们从而找到了5次掷骰子游戏的和的分布。这种方法难以处理大数据量的情况。让我们看看我们的新方法是否可以找到10个骰子总和的分布。</p>
<p>我们必须从单个筛子的分布开始，为此重要的是要记住包含0作为0个点的概率。否则pgf将是错误的，因为<code>NumPy</code>不知道它不应该包括0次项。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">die = np.append(<span class="number">0</span>, (<span class="number">1</span>/<span class="number">6</span>)*np.ones(<span class="number">6</span>))</span><br><span class="line">die</span><br></pre></td></tr></table></figure>
<pre><code>array([ 0.        ,  0.16666667,  0.16666667,  0.16666667,  0.16666667,
        0.16666667,  0.16666667])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Plot(dist_sum(<span class="number">1</span>, die))</span><br></pre></td></tr></table></figure>
<p><img src="/image/prob140/14_02_output_22_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Plot(dist_sum(<span class="number">10</span>, die))</span><br></pre></td></tr></table></figure>
<p><img src="/image/prob140/14_02_output_23_0.png" alt="png"></p>
<h3 id="制作波"><a href="#制作波" class="headerlink" title="制作波"></a>制作波</h3><p>10个筛子和的分布，看上去很符合正态分布。是所有的和都这样么？</p>
<p>要探索这个问题，让$X_1$的分布为$p_1 = p_2 = p_9 = 1/3$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">probs_X1 = make_array(<span class="number">0</span>, <span class="number">1</span>/<span class="number">3</span>, <span class="number">1</span>/<span class="number">3</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>/<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>这是$X_1$的分布。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Plot(dist_sum(<span class="number">1</span>, probs_X1))</span><br></pre></td></tr></table></figure>
<p><img src="/image/prob140/14_02_output_27_0.png" alt="png"></p>
<p>$S_{10}$的概率直方图表明“和”并不总是具有平滑的分布。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Plot(dist_sum(<span class="number">10</span>, probs_X1))</span><br></pre></td></tr></table></figure>
<p><img src="/image/prob140/14_02_output_29_0.png" alt="png"></p>
<p>$S_{30}$的分布看上去头发乱糟糟的剑龙。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Plot(dist_sum(<span class="number">30</span>, probs_X1))</span><br></pre></td></tr></table></figure>
<p><img src="/image/prob140/14_02_output_31_0.png" alt="png"></p>
<p>$S_{100}$的分布是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Plot(dist_sum(<span class="number">100</span>, probs_X1))</span><br></pre></td></tr></table></figure>
<p><img src="/image/prob140/14_02_output_33_0.png" alt="png"></p>
<p>… 非常正态分布了. </p>
<p>这看起来好像这里有什么定理。在本章的其余部分，我们将研究该定理，该定理是关于大量独立同分布样本之和的近似分布。</p>
<p>请记住，只要<code>NumPy</code>能够处理计算,我们的pgf方法就能求出有限多个非负整数上的独立同分布样本分布总和的<em>精确分布</em>。在上面的例子中， $S_{100}$的pgf是一个最高次项为900的多项式。<code>NumPy</code>处理得很好。</p>
<h2 id="中心极限定理-1"><a href="#中心极限定理-1" class="headerlink" title="中心极限定理"></a>中心极限定理</h2><p>顾名思义，这个定理是概率，统计和数据科学领域的核心。它解释了上一节中出现的正态曲线。</p>
<p>在我们得到定理之前，让我们回顾一下数据8和本课程前面的一些事实。</p>
<h3 id="标准单位"><a href="#标准单位" class="headerlink" title="标准单位"></a>标准单位</h3><p>正如我们之前看到的，随机变量 $X$转换为<em>标准单位</em>如下</p>
<script type="math/tex; mode=display">
Z = \frac{X - \mu_X}{\sigma_X}</script><p>$Z$以标准差为单位，衡量了$X$离开均值的距离（如，均值4，标准差1,5就离开均值1个标准差的距离）。换句话说$Z$表示了$X$高于均值的标准差的数。</p>
<p>按线性函数规则，无论$X$的分布是什么，都有</p>
<script type="math/tex; mode=display">
E(Z) = 0 ~~~ \text{and} ~~~ SD(Z) = 1</script><h3 id="标准正态曲线"><a href="#标准正态曲线" class="headerlink" title="标准正态曲线"></a>标准正态曲线</h3><p>回顾数据8，标准正常曲线由通常用小写希腊字母phi，$\phi$表示的函数定义，。</p>
<script type="math/tex; mode=display">
\phi(z) = \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}z^2}, ~~~ -\infty < z < \infty</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># HIDDEN</span></span><br><span class="line">Plot_norm(x_limits=(-<span class="number">4</span>, <span class="number">4</span>), mu=<span class="number">0</span>, sigma=<span class="number">1</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;$z$&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;$\phi(z)$&#x27;</span>, rotation=<span class="number">0</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Standard Normal Curve&#x27;</span>);</span><br></pre></td></tr></table></figure>
<p><img src="/image/prob140/14_03_output_5_0.png" alt="png"></p>
<p>该曲线关于0对称。其拐点在$z=-1$和$z=1$。您在Data 8中观察到了这一点并且可以通过微积分证明它。</p>
<p><strong>术语</strong> 我们将说曲线具有<em>位置参数</em> 0 和<em>比例</em> 参数 1。我们还将使用术语<em>平均值</em>来表示位置和<em>标准差</em>来表示比例，类似于标准单位中随机变量的均值和标准差。在本课程的后面，我们将证明这与具有连续值的随机变量的均值和标准差的定义一致。</p>
<p>曲线下的总面积为1。这需要一些工作来证明。您可能已经在微积分课中看到过它。我们将在课程的后期使用概率方法证明它。</p>
<p>如果是随机变量$X$的分布大致是钟形，那么标准化变量$Z$的分布大致遵循上面的标准正态曲线。</p>
<p>请注意，几乎没有概率落在范围$(-3, 3)之外。回顾一下数据8中的一下数据：</p>
<ul>
<li>介于-1和1之间的面积：约68％</li>
<li>介于-2和2之间的面积：约95％</li>
<li>介于-3和3之间的面积：约99.73％</li>
</ul>
<h3 id="正态曲线"><a href="#正态曲线" class="headerlink" title="正态曲线"></a>正态曲线</h3><p>标准正态曲线是这样一<em>类</em>正态曲线，由其位置和比例参数参数，及均值和标准差确定。</p>
<p>均值为$\mu$，方差为$\sigma$的正态曲线定义如下：</p>
<script type="math/tex; mode=display">
f(x) ~ = ~ \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}, ~~~ -\infty < x < \infty</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># HIDDEN</span></span><br><span class="line">Plot_norm(x_limits=(-<span class="number">4</span>, <span class="number">4</span>), mu=<span class="number">0</span>, sigma=<span class="number">1</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;$x$&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;$f(x)$&#x27;</span>, rotation=<span class="number">0</span>)</span><br><span class="line">plt.yticks(np.arange(<span class="number">0</span>, <span class="number">0.401</span>, <span class="number">0.05</span>), np.array(<span class="number">7</span>*[<span class="string">&#x27;&#x27;</span>]))</span><br><span class="line">plt.xticks(np.arange(-<span class="number">4</span>, <span class="number">4.1</span>), [<span class="string">&#x27;&#x27;</span>,<span class="string">&#x27;&#x27;</span>,<span class="string">&#x27;$\mu-2\sigma$&#x27;</span>, <span class="string">&#x27;$\mu - \sigma$&#x27;</span>, <span class="string">&#x27;$\mu$&#x27;</span>, <span class="string">&#x27;$\mu+\sigma$&#x27;</span>,<span class="string">&#x27;$\mu+2\sigma$&#x27;</span>,<span class="string">&#x27;&#x27;</span>])</span><br><span class="line">plt.title(<span class="string">&#x27;Normal Curve, mean $\mu$, SD $\sigma$&#x27;</span>);</span><br></pre></td></tr></table></figure>
<p><img src="/image/prob140/14_03_output_8_0.png" alt="png"></p>
<p>形状看起来与标准正常曲线完全相同。唯一的区别在于轴上的测量尺度。中心现在是$\mu$而不是0，并且拐点远离中心距离是以$\sigma$为单位而不是1。</p>
<p>现在给出正态曲线的重要性：</p>
<h3 id="中心极限定理-2"><a href="#中心极限定理-2" class="headerlink" title="中心极限定理"></a>中心极限定理</h3><p>设 $X_1, X_2, \ldots$ 是i.i.d., 每一个都有均值$\mu$和标准差$\sigma$。令$S_n = X_1 + X_2 + \cdots + X_n$，则有</p>
<script type="math/tex; mode=display">
E(S_n) = n\mu ~~~~~~~~~~ SD(S_n) = \sqrt{n}\sigma</script><p>我们还不知道$S_n$的分布的形状。<em>中心极限定理</em>（CLT）告诉我们，当$n$很大时，曲线会很平滑。</p>
<h4 id="定理"><a href="#定理" class="headerlink" title="定理"></a>定理</h4><p>当$n$很大时，标准分布的和为</p>
<script type="math/tex; mode=display">
\frac{S_n - n\mu}{\sqrt{n}\sigma}</script><p>无论$X_i$的分布如何，最终将大致遵循标准正态分布。</p>
<p>换言之，当$n$很大时，无论$X_i$的分布如何，$S_n$的分布与均值$n\mu$和标准差$\sqrt{n}\sigma$有关</p>
<p>中心极限定理是使用标准差对分布进行衡量的主要原因。</p>
<p>究竟当$n$多大时，估计值能够有一个较好的结果？这取决于$X_i$的分布。我们稍后会详细说明。现在，假设我们使用的样本大小足够大，以使正态估计合理。</p>
<p>该定理的证明超出了本课程的范围。但是你已经在数据8中进行的模拟以及前一节中计算的总和的精确分布中看到了大量的证据。</p>
<h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h4><p>假设一个样本为100人的重量，是独立同分布的，其均值为150磅，标准差为15磅。然后所采样的人的总重量的均值为$100 \times 150 = 15,000$，标准差为$\sqrt{100} \times 15 = 150$。</p>
<p>谁在乎一群随机人的总重量？询问那些建造体育馆，电梯和飞机的人。</p>
<p>您可以使用<code>prob140</code>方法绘制此分布<code>Plot_norm</code>。参数是您希望绘制曲线的间隔，平均值和标准差。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">n = <span class="number">100</span></span><br><span class="line">mu = <span class="number">150</span></span><br><span class="line">sigma = <span class="number">15</span></span><br><span class="line"></span><br><span class="line">mean = n*mu</span><br><span class="line">sd = (n**<span class="number">0.5</span>)*sigma</span><br><span class="line"></span><br><span class="line">plot_interval = make_array(mean-<span class="number">4</span>*sd, mean+<span class="number">4</span>*sd)</span><br><span class="line"></span><br><span class="line">Plot_norm(plot_interval, mean, sd)</span><br></pre></td></tr></table></figure>
<p><img src="/image/prob140/14_03_output_12_0.png" alt="png"></p>
<h3 id="正态曲线下的概率"><a href="#正态曲线下的概率" class="headerlink" title="正态曲线下的概率"></a>正态曲线下的概率</h3><p>假设我们想要找到抽样人员的总重量小于15,100磅的概率。这大约是下面的黄金区域。这是使用正态曲线的一个估计。</p>
<p>请注意参数<code>right_end=15100</code>。这告诉<code>Plot_norm</code>其右边界。如果没有指定左端，则最左端视为其左边界。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Plot_norm(plot_interval, mean, sd, right_end=<span class="number">15100</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/image/prob140/14_03_output_14_0.png" alt="png"></p>
<p>和以前一样，返回点左边所有概率的函数称为分布的<em>累积分布函数</em>（cdf）。<code>stats.norm.cdf</code>使用适当的参数，概率不到75％。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stats.norm.cdf(<span class="number">15100</span>, mean, sd)</span><br></pre></td></tr></table></figure>
<pre><code>0.74750746245307709
</code></pre><p>为了估计总重量在14,800磅到15,100磅之间的概率率是多少？现在我们指定两个参数<code>left_end</code>和<code>right_end</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Plot_norm(plot_interval, mean, sd, left_end=<span class="number">14800</span>, right_end=<span class="number">15100</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/image/prob140/14_03_output_18_0.png" alt="png"></p>
<p>阴影面积约为65.6％.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stats.norm.cdf(<span class="number">15100</span>, mean, sd) - stats.norm.cdf(<span class="number">14800</span>, mean, sd)</span><br></pre></td></tr></table></figure>
<pre><code>0.65629624272720921
</code></pre><h3 id="标准正态-CDF-Phi"><a href="#标准正态-CDF-Phi" class="headerlink" title="标准正态 CDF $\Phi$"></a>标准正态 CDF $\Phi$</h3><p>实际上只有一条正常曲线很重要 - 标准正态曲线。所有其他的都是标准正态曲线通过水平轴的线性变换获得的。因此，通过标准化，可以根据标准正态cdf完成上述所有计算，如下所述。</p>
<p>要找到总重量小于15,100磅的大概几率，首先将重量标准化，然后使用标准正态cdf。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">z_right = (<span class="number">15100</span> - mean)/sd</span><br><span class="line"></span><br><span class="line">stats.norm.cdf(z_right)  <span class="comment"># The standard curve is the default</span></span><br></pre></td></tr></table></figure>
<pre><code>0.74750746245307709
</code></pre><p>要找到总重量在14,800磅到15,100磅之间的概率：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">z_left = (<span class="number">14800</span> - mean)/sd</span><br><span class="line"></span><br><span class="line">stats.norm.cdf(z_right) - stats.norm.cdf(z_left)</span><br></pre></td></tr></table></figure>
<pre><code>0.65629624272720921
</code></pre><p>标准正态cdf的常用符号是大写字母$\Phi$，因为它是$\phi$的积分：</p>
<script type="math/tex; mode=display">
\Phi(x) = \int_{-\infty}^x \phi(z)dz, ~~~~ -\infty < x < \infty</script><p>这个积分虽然是有限的，但没有封闭形式的公式，可以用算术运算，幂，三角函数，指数和对数函数以及组合来改写。它必须通过数值积分来求近似值。这就是为什么每个统计系统都有内置功能，例如<code>stats.norm.cdf</code>提供出色的近似值功能。</p>
<p>标准化标准正态累积分布函数$\Phi$为所有正态曲线下的面积值提供了紧凑的表示法。我们不必对不同的参数使用不同的函数。</p>
<p>在CLT的假设下，对于大的值$n$我们有近似值</p>
<script type="math/tex; mode=display">
P(S_n \le x) ~ \approx ~ \Phi \big{(} \frac{x - n\mu}{\sqrt{n}\sigma} \big{)} ~~~ \text{for all } x</script><p>正如您在数据8中看到的那样，近似值通常在分布的尾部中表现不佳。如果使用CLT来逼近尾部区域的概率，请注意近似值可能非常粗糙。</p>
<h3 id="二项分布-n-p-的估计"><a href="#二项分布-n-p-的估计" class="headerlink" title="二项分布 $(n, p)$ 的估计"></a>二项分布 $(n, p)$ 的估计</h3><p>一个二项随机分布$(n, p)$ 是$n$个i.i.d.分布的和。CLT表明，如果$n$足够大，无论$p$是什么，分布是大致成正态分布的。但我们在第6章中说过，如果$n$很大，$p$很小，那么二项分布大致是泊松分布。</p>
<p>那么它到底是正态分布还是泊松分布？这是两个二项式直方图，两者都有大的$n$但有不同的形状。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">k1 = np.arange(<span class="number">25</span>, <span class="number">76</span>)</span><br><span class="line">probs1 = stats.binom.pmf(k1, <span class="number">100</span>, <span class="number">0.5</span>)</span><br><span class="line">binom_fair = Table().values(k1).probability(probs1)</span><br><span class="line">Plot(binom_fair)</span><br><span class="line">plt.title(<span class="string">&#x27;Binomial (100, 0.5)&#x27;</span>);</span><br></pre></td></tr></table></figure>
<p><img src="/image/prob140/14_03_output_27_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">k2 = np.arange(<span class="number">0</span>, <span class="number">11</span>)</span><br><span class="line">probs2 = stats.binom.pmf(k2, <span class="number">100</span>, <span class="number">0.01</span>)</span><br><span class="line">binom_biased = Table().values(k2).probability(probs2)</span><br><span class="line">Plot(binom_biased)</span><br><span class="line">plt.title(<span class="string">&#x27;Binomial (100, 0.1)&#x27;</span>);</span><br></pre></td></tr></table></figure>
<p><img src="/image/prob140/14_03_output_28_0.png" alt="png"></p>
<p>差异是由于分布的扩散。当分布在0附近时，泊松近似适用。当扩展较大时，在均值的任一侧存在大量可能值，则可以尝试正态分布。</p>
<p>为了量化这一点，许多文本根据给出了粗略的阈值$n$和$p$，使得，如果$n$大于阈值，那么二项式$(n, p)$大致是正态分布。如果$n$很大，二项分布类似于泊松，意味着$n$没有超过正态分布的阈值。</p>
<p>阈值通常以“标准差$\sqrt{npq}$大于3” 或“$np$和$nq$都大于10”来表示，这些不完全一致，但非常相近。</p>
<p>您可以通过比较二项式与相应泊松之间的总变化距离以及二项式与相应法线之间的总变化距离来了解您对这些阈值的看法。然而，在这个过程中，对二项式的法线与泊松近似的选择很少会成为一个问题，因为当$n$和$p$的值都给出时， 如果您对使用哪个有疑问，那么您可以使用确切的二项式概率。</p>
<p>这是二项式（100,0.5）分布和近似正态曲线。曲线的参数是$np = 50$和$\sqrt{npq} = 5$。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Plot(binom_fair)</span><br><span class="line">Plot_norm((<span class="number">25</span>, <span class="number">75</span>), <span class="number">50</span>, <span class="number">5</span>, color=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">plt.xticks(np.arange(<span class="number">25</span>, <span class="number">76</span>, <span class="number">5</span>))</span><br><span class="line">plt.title(<span class="string">&#x27;Binomial (100, 0.5) and its Normal Approximation&#x27;</span>);</span><br></pre></td></tr></table></figure>
<p><img src="/image/prob140/14_03_output_31_0.png" alt="png"></p>
<p>注意点“$\mbox{mean } \pm \mbox{ SD}$“$= 50 \pm 5$是曲线的拐点。</p>
<h2 id="样本均值"><a href="#样本均值" class="headerlink" title="样本均值"></a>样本均值</h2><p>中心极限定理的核心是什么？一个答案是它允许我们基于随机样本做出推论，即使我们对总体的分布知之甚少。</p>
<p>在数据8中，您看到如果我们想要估计总体的平均值，我们可以基于大随机样本的平均值来构建参数的置信区间。在该过程中，您使用引导程序生成样本均值的经验分布，然后使用经验分布来创建置信区间。你会记得那些经验分布总是钟形的。</p>
<p>在本节中，我们将研究样本均值的概率分布，并表明您可以使用它来构建总体均值的置信区间，而无需进行任何重新采样。</p>
<p>让我们从样本总和开始，我们现在很清楚。回想一下我们的假设和符号：</p>
<p>设$X<em>1, X_2, \ldots, X_n$ 是一个i.i.d采样, 设每一个$X_i$的均值为$\mu$标准差为$\sigma$。设$S_n$是样本总和，即$S_n = \sum</em>{i=1}^n X_i$。可以得到</p>
<script type="math/tex; mode=display">
E(S_n) = n\mu ~~~~~~~~~~  SD(S_n) = \sqrt{n}\sigma</script><p>这些结果意味着随着样本量的增加，样本总和的分布向右移动并变得更加分散。</p>
<p>您可以在下图中看到这一点。该图显示了5个筛子的总和和20个筛子的总和的分布。分布是精确的，使用本章前面定义的<code>dist_sum</code>方法计算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">die = np.append(<span class="number">0</span>, (<span class="number">1</span>/<span class="number">6</span>)*np.ones(<span class="number">6</span>))</span><br><span class="line">dist_sum_5 = dist_sum(<span class="number">5</span>, die)</span><br><span class="line">dist_sum_20 = dist_sum(<span class="number">20</span>, die)</span><br><span class="line">Plots(<span class="string">&#x27;Sum of 5 dice&#x27;</span>, dist_sum_5, <span class="string">&#x27;Sum of 20 dice&#x27;</span>, dist_sum_20)</span><br></pre></td></tr></table></figure>
<p><img src="/image/prob140/14_04_output_4_0.png" alt="png"></p>
<p>您可以看到正态分布已经显示为5和20个样本的总和。</p>
<p>您还可以看到黄金区域不是蓝色区域的四倍，尽管黄金区域中的样本大小是蓝色的四倍。黄金区域高只有蓝色一半，分布是蓝色的两倍。那是因为总和的标准与$\sqrt{n}$成正比。它增长比$n$慢。由于样本量大4倍，因此黄金分布的标准差为蓝色的 $\sqrt{4} = 2$倍。</p>
<p>样本的<em>平均值</em>表现不同</p>
<h3 id="IID-样本的均值"><a href="#IID-样本的均值" class="headerlink" title="IID 样本的均值"></a>IID 样本的均值</h3><p>设$\bar{X}_n$是样本年均值，即</p>
<script type="math/tex; mode=display">
\bar{X}_n = \frac{S_n}{n}</script><p>然后$\bar{X}_n$只是$S_n$的线性变换，所以</p>
<script type="math/tex; mode=display">
E(\bar{X}_n) = \frac{E(S_n)}{n} = \frac{n\mu}{n} = \mu ~~~~ \text{for all }n</script><p>样本均值的期望总是总体的均值$\mu$，无论样本大小。因此，无论样本大小如何，样本均值都是总体均值的无偏估计。</p>
<p>样本均值标准差是</p>
<script type="math/tex; mode=display">
SD(\bar{X}_n) = \frac{SD(S_n)}{n} = \frac{\sqrt{n}\sigma}{n} = \frac{\sigma}{\sqrt{n}}</script><p>随着样本量的增加，样本均值的变化性降低。因此，随着样本量的增加，样本均值对总体均值的估计更准确。</p>
<p>下图显示了5个筛子和20个筛子的平均值的分布。两者都以3.5为中心，但较大样本的均值分布较窄。您在数据8中经常看到这一点：随着样本量的增加，样本均值的分布更集中在总体均值周围。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># HIDDEN</span></span><br><span class="line">dist_mean_5 = Table().with_columns(</span><br><span class="line">    <span class="string">&#x27;Value&#x27;</span>, dist_sum_5.column(<span class="number">0</span>)/<span class="number">5</span>,</span><br><span class="line">    <span class="string">&#x27;Probability&#x27;</span>, <span class="number">5</span>*dist_sum_5.column(<span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line">dist_mean_20 = Table().with_columns(</span><br><span class="line">    <span class="string">&#x27;Value&#x27;</span>, dist_sum_20.column(<span class="number">0</span>)/<span class="number">20</span>,</span><br><span class="line">    <span class="string">&#x27;Probability&#x27;</span>, <span class="number">20</span>*dist_sum_20.column(<span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line">Plots(<span class="string">&#x27;Mean of 5 dice&#x27;</span>, dist_mean_5, <span class="string">&#x27;Mean of 20 dice&#x27;</span>, dist_mean_20, width=<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/image/prob140/14_04_output_7_0.png" alt="png"></p>
<p>精确是有代价的。样本平均值的标准差随着样本大小的平方根减小。因此，如果要将样本均值的标准差降低3倍，则必须将样本量增加$3^2 = 9$倍。</p>
<p>一般结果通常可作为反例进行证明。</p>
<h4 id="平方根法则"><a href="#平方根法则" class="headerlink" title="平方根法则"></a>平方根法则</h4><p>如果将样本扩大n倍，则样本均值的均值会减少$\sqrt{n}$倍。</p>
<h3 id="弱大数定律"><a href="#弱大数定律" class="headerlink" title="弱大数定律"></a>弱大数定律</h3><p>样本均值是总体均值的无偏估计，并且当样本较大时，其标准差比较小。因此，大样本的平均值接近总体平均值的概率很高。</p>
<p>这一结论称为<em>弱大数定律</em>。</p>
<p>令$X_1, X_2, \ldots, X_n$是i.i.d.，每一个有均值$\mu$和标准差$\sigma$，令$\bar{X}_n$是样本均值。取一个极小数$\epsilon &gt; 0$，有</p>
<script type="math/tex; mode=display">
P(|\bar{X}_n - \mu| < \epsilon) \to 1 ~~~ \text{as } n \to \infty</script><p>也就是说，对于$n$取值很大时，几乎可以确定均值在$\mu \pm \epsilon$的范围内，</p>
<p>为了证明该定律，我命将证明$P(|\bar{X}_n - \mu| \ge \epsilon) \to 0$，这是用切比雪夫不等式很容易求解。</p>
<script type="math/tex; mode=display">
P(|\bar{X}_n - \mu| \ge \epsilon)~ \le ~ \frac{\sigma_{\bar{X}_n}^2}{\epsilon^2} 
~ = ~ \frac{\sigma^2}{n\epsilon^2} ~ \to ~ 0 ~~~ \text{as } n \to \infty</script><h3 id="相关定律"><a href="#相关定律" class="headerlink" title="相关定律"></a>相关定律</h3><ul>
<li><strong>强大数定律。</strong> 这表示在概率为1时，样本平均值收敛到极限，并且该极限是常数$\mu$。请参阅<a href="https://terrytao.wordpress.com/2008/06/18/the-strong-law-of-large-numbers/">Fields Medalist Terence Tao撰写的这篇博客文章</a>。他陈述了基础标准差可能不存在的情况下的定律情况。请注意，我们的弱大数定律证明方法在这种情况下无效; 结果仍然是正确的，但证据需要更多的探索。</li>
<li><strong>小数定律。</strong> 这是<a href="https://en.wikipedia.org/wiki/Ladislaus_Bortkiewicz">Ladislaus Bortkiewicz</a> (1868-1931)一本书的标题。其中他描述了低概率事件分布的泊松近似。这就是为什么这些注释的第6.4节被称为小数定律。</li>
<li><strong>平均定律。</strong> 在总体是二元的情况下，这是弱法则的通用名称，样本均值只是样本中成功的比例。在通常的使用中，人们有时会忘记定律是一种限制性陈述。如果你正在抛硬币并连续看到10个正面，那么下一个抛正面的机会仍然是1/2。平均律并没有说你“应该是反面”。它不适用于有限的投掷集。</li>
</ul>
<h3 id="分布的形状"><a href="#分布的形状" class="headerlink" title="分布的形状"></a>分布的形状</h3><p>中心极限定理告诉我们，对于大样本，样本均值的分布大致是正态的。样本均值是样本和的线性变换。因此，如果样本总和的分布大致是正态的，则样本均值的分布也大致是正态的，但具有不同的参数。具体来说，对于$n$很大的情况下</p>
<script type="math/tex; mode=display">
P(\bar{X}_n \le x) ~ \approx ~ \Phi \big{(} \frac{x - \mu}{\sigma/\sqrt{n}} \big{)} ~~~~ \text{for all } x</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># HIDDEN</span></span><br><span class="line">Plot_norm(x_limits=(-<span class="number">4</span>, <span class="number">4</span>), mu=<span class="number">0</span>, sigma=<span class="number">1</span>)</span><br><span class="line">plt.yticks(np.arange(<span class="number">0</span>, <span class="number">0.401</span>, <span class="number">0.05</span>), np.array(<span class="number">7</span>*[<span class="string">&#x27;&#x27;</span>]))</span><br><span class="line">plt.xticks(np.arange(-<span class="number">4</span>, <span class="number">4.1</span>),[<span class="string">&#x27;&#x27;</span>,<span class="string">&#x27;&#x27;</span>,<span class="string">&#x27;&#x27;</span>,<span class="string">&#x27;$\mu - \sigma/\sqrt&#123;n&#125;$&#x27;</span>, <span class="string">&#x27;$\mu$&#x27;</span>, <span class="string">&#x27;$\mu+\sigma/\sqrt&#123;n&#125;$&#x27;</span>,<span class="string">&#x27;&#x27;</span>,<span class="string">&#x27;&#x27;</span>])</span><br><span class="line">plt.title(<span class="string">&#x27;Approximate Distribution of Sample Mean&#x27;</span>);</span><br></pre></td></tr></table></figure>
<p><img src="/image/prob140/14_04_output_12_0.png" alt="png"></p>
<h2 id="置信区间"><a href="#置信区间" class="headerlink" title="置信区间"></a>置信区间</h2><p>假设你有一个大的iid样本。CLT意味着有约95％的概率，样本均值在总体平均值的2个标准差距离内。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># HIDDEN</span></span><br><span class="line">Plot_norm(x_limits=(-<span class="number">4</span>, <span class="number">4</span>), mu=<span class="number">0</span>, sigma=<span class="number">1</span>, left_end=-<span class="number">2</span>, right_end=<span class="number">2</span>)</span><br><span class="line">plt.yticks(np.arange(<span class="number">0</span>, <span class="number">0.401</span>, <span class="number">0.05</span>), np.array(<span class="number">7</span>*[<span class="string">&#x27;&#x27;</span>]))</span><br><span class="line">plt.xticks(np.arange(-<span class="number">4</span>, <span class="number">4.1</span>),[<span class="string">&#x27;&#x27;</span>,<span class="string">&#x27;&#x27;</span>,<span class="string">&#x27;$\mu - 2\sigma/\sqrt&#123;n&#125;$&#x27;</span>, <span class="string">&#x27;&#x27;</span>, <span class="string">&#x27;$\mu$&#x27;</span>, <span class="string">&#x27;&#x27;</span>, <span class="string">&#x27;$\mu+2\sigma/\sqrt&#123;n&#125;$&#x27;</span>,<span class="string">&#x27;&#x27;</span>])</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Sample Mean&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Gold Area: Approximately 95%&#x27;</span>);</span><br></pre></td></tr></table></figure>
<p><img src="/image/prob140/14_05_output_3_0.png" alt="png"></p>
<p>这可以用不同的方式表达：</p>
<ul>
<li>在所有样本集合的约95％的样本中，样本平均值在<em>总体平均值$\pm ~ 2 \sigma/\sqrt{n}$</em>的范围内。</li>
</ul>
<p>换言之：</p>
<ul>
<li>在所有样本集合的约95％的样本中， 总体的平均值在<em>样本均值 $\pm ~ 2 \sigma/\sqrt{n}$</em>的范围内。</li>
</ul>
<p>这就是为什么用<em>样本均值$\pm ~ 2 \sigma/\sqrt{n}$</em>作为$\mu$的估计间隔。</p>
<h3 id="mu-的置信区间"><a href="#mu-的置信区间" class="headerlink" title="$\mu$的置信区间"></a>$\mu$的置信区间</h3><p><em>样本均值$\pm ~ 2 \sigma/\sqrt{n}$</em>的区间称之为<em>参数$\mu$的95％置信区间</em>这个区间，拥有一个95%的<em>置信水平</em>。</p>
<p>你可以选择不同的置信水平，比如说80％。在这个选择下，你的期望区间会更窄。要确切了解中心两侧需要多少标准差的距离，来获得大约80％的中心区域，您必须找到在标准正态曲线上相应的$z$，如下图所示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># HIDDEN</span></span><br><span class="line">Plot_norm(x_limits=(-<span class="number">4</span>, <span class="number">4</span>), mu=<span class="number">0</span>, sigma=<span class="number">1</span>, left_end=-<span class="number">1.28</span>, right_end=<span class="number">1.28</span>)</span><br><span class="line">plt.yticks(np.arange(<span class="number">0</span>, <span class="number">0.401</span>, <span class="number">0.05</span>), np.array(<span class="number">7</span>*[<span class="string">&#x27;&#x27;</span>]))</span><br><span class="line">plt.xticks(make_array(-<span class="number">1.28</span>, <span class="number">0</span>, <span class="number">1.28</span>),[<span class="string">&#x27;$-z$&#x27;</span>, <span class="string">&#x27;0&#x27;</span>, <span class="string">&#x27;$z$&#x27;</span>])</span><br><span class="line">plt.title(<span class="string">&#x27;Gold Area: Approximately 80%&#x27;</span>);</span><br></pre></td></tr></table></figure>
<p><img src="/image/prob140/14_05_output_6_0.png" alt="png"></p>
<p>正如您从数据8中所知，并且可以在图中看到，间隔从分布的第10百分位到第90百分位。所以$z$是标准正态曲线的第90个百分点，也称为曲线的“90％点”。<code>scipy</code>方法会调用<code>ppf</code>并将f分位数的十进制值作为其参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stats.norm.ppf(<span class="number">.9</span>)</span><br></pre></td></tr></table></figure>
<pre><code>1.2815515655446004
</code></pre><p>因此，总体的大约80％置信区间意味着总体均值$\mu$在”样本均值 $\pm ~ 1.28\sigma/\sqrt{n}$”范围内。</p>
<p>让我们仔细校验，2是$z$的一个很好的取值，这意味着95％的置信区间。该$z$我们需要的是97.5％的点数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stats.norm.ppf(<span class="number">.975</span>)</span><br></pre></td></tr></table></figure>
<pre><code>1.959963984540054
</code></pre><p>那是$z = 1.96$，这就是我们一直使用的2。这个值已经足够了，但是$z = 1.96$也常用于构建95％置信区间。</p>
<h3 id="一般定义"><a href="#一般定义" class="headerlink" title="一般定义"></a>一般定义</h3><p>设$\lambda$是一个置信水平，令$z<em>\lambda$代表了这样一个分位数，使得正态曲线中，$(-z</em>\lambda, ~ z<em>\lambda)$包含了$\lambda$%区域。在上面的例子中，$\lambda$的值是80， $z</em>\lambda$的值是1.28。</p>
<p>当$n$足够大时，有</p>
<script type="math/tex; mode=display">
\frac{\lambda}{100} ~ \approx ~ 
P(\bar{X}_n \in \mu ~ \pm ~ z_\lambda \sigma/\sqrt{n}) ~ = ~
P(\mu \in \bar{X}_n ~ \pm ~ z_\lambda \sigma/\sqrt{n})</script><p>随机区间$\bar{X}<em>n ~ \pm ~ z</em>\lambda \sigma/\sqrt{n}$被称为<em>总体均值$\mu$的$\lambda$%置信区间</em>。这意味着，大约有$\lambda$%的概率，该随机区间包含$\mu$。</p>
<p>不同级别的置信区间之间的唯一区别是$z$的选择，这取决于置信水平。另外两个组成是样本均值和标准差。</p>
<h3 id="复习数据8中的示例"><a href="#复习数据8中的示例" class="headerlink" title="复习数据8中的示例"></a>复习数据8中的示例</h3><p>让我们回到数据8中非常熟悉的一个例子：1,174对母亲及其新生儿的随机样本。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">baby = Table.read_table(<span class="string">&#x27;baby.csv&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">baby</span><br></pre></td></tr></table></figure>
<p><table border="1" class="dataframe">
    <thead>
        <tr>
            <th>Birth Weight</th> <th>Gestational Days</th> <th>Maternal Age</th> <th>Maternal Height</th> <th>Maternal Pregnancy Weight</th> <th>Maternal Smoker</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>120         </td> <td>284             </td> <td>27          </td> <td>62             </td> <td>100                      </td> <td>False          </td>
        </tr>
    </tbody>
        <tr>
            <td>113         </td> <td>282             </td> <td>33          </td> <td>64             </td> <td>135                      </td> <td>False          </td>
        </tr>
    </tbody>
        <tr>
            <td>128         </td> <td>279             </td> <td>28          </td> <td>64             </td> <td>115                      </td> <td>True           </td>
        </tr>
    </tbody>
        <tr>
            <td>108         </td> <td>282             </td> <td>23          </td> <td>67             </td> <td>125                      </td> <td>True           </td>
        </tr>
    </tbody>
        <tr>
            <td>136         </td> <td>286             </td> <td>25          </td> <td>62             </td> <td>93                       </td> <td>False          </td>
        </tr>
    </tbody>
        <tr>
            <td>138         </td> <td>244             </td> <td>33          </td> <td>62             </td> <td>178                      </td> <td>False          </td>
        </tr>
    </tbody>
        <tr>
            <td>132         </td> <td>245             </td> <td>23          </td> <td>65             </td> <td>140                      </td> <td>False          </td>
        </tr>
    </tbody>
        <tr>
            <td>120         </td> <td>289             </td> <td>25          </td> <td>62             </td> <td>125                      </td> <td>False          </td>
        </tr>
    </tbody>
        <tr>
            <td>143         </td> <td>299             </td> <td>30          </td> <td>66             </td> <td>136                      </td> <td>True           </td>
        </tr>
    </tbody>
        <tr>
            <td>140         </td> <td>351             </td> <td>27          </td> <td>68             </td> <td>120                      </td> <td>False          </td>
        </tr>
    </tbody>
</table></p>
<p>... (1164 rows omitted)</p>



<p>第三栏包括母亲的年龄。让我们为总体中母亲的平均年龄构建大约95％的置信区间。我们在Data 8中使用bootstrap完成了这项工作，因此我们有了能够进行比较的结果。</p>
<p>因为我们的数据来自大型随机样本，我们可以应用本节的方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">n = <span class="number">1174</span></span><br><span class="line">ages = baby.column(<span class="string">&#x27;Maternal Age&#x27;</span>)</span><br><span class="line"></span><br><span class="line">samp_mean = np.mean(ages)</span><br><span class="line">samp_mean</span><br></pre></td></tr></table></figure>
<pre><code>27.228279386712096
</code></pre><p>可以发现样本的$\bar{X}_n$值是27.23。我们知道$n = 1174$，所以，我们需要总体的标准差$\sigma$然后就可以完成我们的计算。</p>
<p>但是，我们当然不知道总体的标准差$\sigma$。</p>
<p>所以，我们使用数据来估计$\sigma$，当然，这个估计存在一些误差，但它除以 $\sqrt{n}$后，误差会被缩小。请记住，我们的方法依赖于CLT，仅在$n$很大时有效。</p>
<p>$\sigma$的估计大约是5。82年。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sigma_estimate = np.std(ages)</span><br><span class="line">sigma_estimate</span><br></pre></td></tr></table></figure>
<pre><code>5.8153604041908968
</code></pre><p>一个总体的95%置信区间是$(26.89, 27.57)$。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">samp_mean - <span class="number">2</span>*sigma_estimate/(n**<span class="number">0.5</span>), samp_mean + <span class="number">2</span>*sigma_estimate/(n**<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure>
<pre><code>(26.888831911866099, 27.567726861558093)
</code></pre><p>不需要bootstrapping了! </p>
<p>现在让我们比较两种方法的结果。调用Data 8的<code>bootstrap_mean</code>方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">bootstrap_mean</span>(<span class="params">original_sample, label, replications</span>):</span><br><span class="line">    </span><br><span class="line">    <span class="string">&quot;&quot;&quot;Displays approximate 95% confidence interval for population mean.</span></span><br><span class="line"><span class="string">    original_sample: table containing the original sample</span></span><br><span class="line"><span class="string">    label: label of column containing the variable</span></span><br><span class="line"><span class="string">    replications: number of bootstrap samples</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    just_one_column = original_sample.column(label)</span><br><span class="line">    n = <span class="built_in">len</span>(just_one_column)</span><br><span class="line">    means = make_array()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> np.arange(replications):</span><br><span class="line">        bootstrap_sample = np.random.choice(just_one_column, size=n)</span><br><span class="line">        resampled_mean = np.mean(bootstrap_sample)</span><br><span class="line">        means = np.append(means, resampled_mean)</span><br><span class="line">        </span><br><span class="line">    left = percentile(<span class="number">2.5</span>, means)</span><br><span class="line">    right = percentile(<span class="number">97.5</span>, means)</span><br><span class="line">    </span><br><span class="line">    resampled_means = Table().with_column(</span><br><span class="line">    <span class="string">&#x27;Bootstrap Sample Mean&#x27;</span>, means</span><br><span class="line">    )</span><br><span class="line">    resampled_means.hist(bins=<span class="number">15</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Approximate 95% confidence interval for population mean:&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(np.<span class="built_in">round</span>(left, <span class="number">2</span>), <span class="string">&#x27;to&#x27;</span>, np.<span class="built_in">round</span>(right, <span class="number">2</span>))</span><br><span class="line">    plt.plot(make_array(left, right), make_array(<span class="number">0</span>, <span class="number">0</span>), color=<span class="string">&#x27;yellow&#x27;</span>, lw=<span class="number">8</span>);</span><br></pre></td></tr></table></figure>
<p>让我们为总体平均值构建95％置信区间的bootstrap。我们将使用5000引导样本，就像我们在Data 8中所做的那样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bootstrap_mean(baby, <span class="string">&#x27;Maternal Age&#x27;</span>, <span class="number">5000</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Approximate 95% confidence interval for population mean:
26.9 to 27.57
</code></pre><p><img src="/image/prob140/14_05_output_25_1.png" alt="png"></p>
<p>bootstrap置信区间与我们使用正态近似得到的区间（26.89,27.57）基本相同。</p>
<p>正如我们在数据8中所做的那样，我们观察到样本中母亲年龄的分布远非正态分布：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">baby.select(<span class="string">&#x27;Maternal Age&#x27;</span>).hist()</span><br></pre></td></tr></table></figure>
<p><img src="/image/prob140/14_05_output_28_0.png" alt="png"></p>
<p>但是，样本均值的经验分布，显示为前一个单元格的输出，大致为钟形。这是因为由中心极限定理可得，大样本的平均值的概率分布是近似正态的。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">喵十八</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">54</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">77</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/Yao544303" title="GitHub → https://github.com/Yao544303" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:yao544303963@gmail.com" title="E-Mail → mailto:yao544303963@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">喵十八</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  


</body>
</html>
